{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_223.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# RNN \n",
        "Use non-consecutive predictors. Same hour of each day: 7 daily predictors to predict next 3. Smoothing window size 3. Predict steam given steam and day of year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "825019e5-a5e3-496a-9f8d-6b8fa15eea52"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Eagle'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "PREDICTORS = ['doy','meter'] # short list for testing\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_SKIP = 24\n",
        "STEPS_FORWARD = 7 \n",
        "STEPS_FUTURE =  3 \n",
        "METER_FILE='steam.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 2 ['doy', 'meter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "09bbc3ea-7a36-4703-b122-d8dd3ed14b31"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "3dea27b3-0d3c-4325-906d-db64bcd17394"
      },
      "source": [
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    site_df.insert(0,'hour',0)\n",
        "    site_df.insert(1,'month',0)\n",
        "    site_df.insert(2,'doy',0)\n",
        "    L=len(site_df)\n",
        "    for i in range(0,L):\n",
        "        dt=site_df.index[i]\n",
        "        hour=dt.hour\n",
        "        month=dt.month\n",
        "        doy=dt.dayofyear\n",
        "        site_df.iat[i,0] = hour\n",
        "        site_df.iat[i,1] = month\n",
        "        site_df.iat[i,2] = doy\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 18:00:00</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1026.2</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.2</td>\n",
              "      <td>310.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>330.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>320.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 18:00:00    18     12  365  ...          1026.2          330.0        2.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1027.0          320.0        1.5\n",
              "2017-12-31 21:00:00    21     12  365  ...          1027.2          310.0        2.6\n",
              "2017-12-31 22:00:00    22     12  365  ...          1027.4          330.0        3.1\n",
              "2017-12-31 23:00:00    23     12  365  ...          1027.4          320.0        4.6\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "9b19fdd3-7d9e-45c1-b814-3b6688e8d22a"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>92.2957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>277.5584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>280.5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>289.3302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>164.3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        meter\n",
              "timestamp                    \n",
              "2017-12-31 19:00:00   92.2957\n",
              "2017-12-31 20:00:00  277.5584\n",
              "2017-12-31 21:00:00  280.5331\n",
              "2017-12-31 22:00:00  289.3302\n",
              "2017-12-31 23:00:00  164.3474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "46efc96c-9e9d-4363-bcd8-3dfa8dbe8d8a"
      },
      "source": [
        "# This is the first version that uses STEPS_SKIP.\n",
        "# To retrofit this to older notebooks, set STEPS_SKIP=0.\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    num_samples = len(df) - STEPS_FORWARD*STEPS_SKIP - STEPS_FUTURE*STEPS_SKIP\n",
        "    X_shape = (num_samples,STEPS_FORWARD,NUM_PREDICTORS)\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = df[PREDICTED_VARIABLE].values  # meter\n",
        "    # TO DO: can we take predicted from mdf instead?\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = predictor_series[sam+time*STEPS_SKIP]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat] = val\n",
        "        for time1 in range (STEPS_FORWARD,STEPS_FORWARD+STEPS_FUTURE): \n",
        "            time0 = time1 - STEPS_FORWARD\n",
        "            y[sam,time0]=predicted_series[sam+time1*STEPS_SKIP]\n",
        "    return X,y \n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (17304, 7, 2)\n",
            "y shape: (17304, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObWmpMDVuNQ",
        "outputId": "4e0c5c45-49a8-4772-b86a-88d1edbb0429"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['doy', 'meter']\n",
            "X example:\n",
            " [[  5 232]\n",
            " [  6  43]\n",
            " [  7 166]\n",
            " [  8 173]\n",
            " [  9 133]\n",
            " [ 10 125]\n",
            " [ 11 154]]\n",
            "y example:\n",
            " [225 139 182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_RNN():\n",
        "    # The GRU in Keras is optimized for speed on CoLab GPU.\n",
        "    rnn = Sequential([\n",
        "        GRU(16,return_sequences=True, \n",
        "                  input_shape=(STEPS_FORWARD,NUM_PREDICTORS)), \n",
        "        GRU(16,return_sequences=True),\n",
        "        GRU(16,return_sequences=False),\n",
        "        Dense(STEPS_FUTURE)\n",
        "    ])\n",
        "    rnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    return rnn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "90d00bab-28cc-4371-d3f7-bb64a1c58085"
      },
      "source": [
        "cors = []\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",BLDG)\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # \n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        y_test = np.asarray(y[split:])\n",
        "        model = make_RNN()\n",
        "        print(model.summary())\n",
        "        #print(\"Example X train:\\n\",X_train[example].astype(int))\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        # Keep a table for reporting later.\n",
        "        y_pred = model.predict(X_test)\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        print(\"mean,rmse,rmse/mean,bldg:\",mean,rmse,rmse/mean,BLDG)\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n",
        "print()\n",
        "print(\"History\",STEPS_FORWARD,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Eagle_office_Lamont\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [57 59 48]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 20s 4ms/step - loss: 1170.6874\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 838.3461\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 646.2140\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 505.6089\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 405.3040\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 341.2502\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 292.2790\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 257.9878\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 239.6275\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 237.0791\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 229.2289\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 230.1619\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 228.3590\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 224.5295\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 226.7137\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 231.0367\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 225.1771\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 228.1565\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 235.1984\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 233.1712\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 227.9484\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 230.2551\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 169.3135\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 111.7818\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 86.7147\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 78.7311\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 73.6496\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 71.3528\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 68.1579\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 65.2239\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 62.3163\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 64.4818\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 60.5497\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 59.9462\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 57.5819\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 57.9455\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 59.5102\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 56.0848\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 58.3568\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 59.1882\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 56.5908\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 56.0086\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 55.6550\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 53.8127\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 54.5300\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 53.5260\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 58.0571\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 54.1870\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 55.4061\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 56.5419\n",
            "mean,rmse,rmse/mean,bldg: 36.93460755405991 12.147188826350744 0.32888365765282124 Eagle_office_Lamont\n",
            "Example prediction:\n",
            " 0 [50 50 50]\n",
            "Example prediction:\n",
            " 2 [43 43 42]\n",
            "Example prediction:\n",
            " 4 [44 43 43]\n",
            "Example prediction:\n",
            " 6 [45 44 44]\n",
            "Example prediction:\n",
            " 8 [46 46 46]\n",
            "Example prediction:\n",
            " 10 [45 45 44]\n",
            "Example prediction:\n",
            " 12 [45 45 44]\n",
            "Example prediction:\n",
            " 14 [44 44 44]\n",
            "Example prediction:\n",
            " 16 [52 52 52]\n",
            "Example prediction:\n",
            " 18 [52 52 52]\n",
            "Example prediction:\n",
            " 20 [52 52 52]\n",
            "Example prediction:\n",
            " 22 [39 39 39]\n",
            "Building Eagle_health_Athena\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_3 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1297 1401 1045]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 4ms/step - loss: 336544.7108\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 334124.9660\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 334393.7642\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 325928.9907\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 324651.9292\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 318933.9221\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 313376.9042\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 309551.0744\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 298600.0270\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 301400.2532\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 294681.3930\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 298595.4891\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 287739.2165\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 285833.3691\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 276955.9019\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 273452.1534\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 271559.3517\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 261888.2321\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 269201.0648\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 253444.0011\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 260375.3201\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 249346.8060\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 250690.2266\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 248498.0673\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 240230.8759\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 233924.5839\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 239153.5545\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 227648.6777\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 225791.6940\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 219450.5129\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 217030.9632\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 218392.7887\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 207807.1945\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 207430.2762\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 199727.4059\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 203143.3042\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 196738.1537\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 192204.3865\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 192260.8535\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 195713.5649\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 185150.7213\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 180965.2834\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 178354.9958\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 174819.0234\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 176283.1950\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 173241.0515\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 165443.4507\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 160957.9190\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 166568.7398\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 161724.1780\n",
            "mean,rmse,rmse/mean,bldg: 477.6710835157538 313.0452679590395 0.6553573761571756 Eagle_health_Athena\n",
            "Example prediction:\n",
            " 0 [226 223 223]\n",
            "Example prediction:\n",
            " 2 [226 223 223]\n",
            "Example prediction:\n",
            " 4 [226 223 223]\n",
            "Example prediction:\n",
            " 6 [226 223 223]\n",
            "Example prediction:\n",
            " 8 [226 223 223]\n",
            "Example prediction:\n",
            " 10 [226 223 223]\n",
            "Example prediction:\n",
            " 12 [226 223 223]\n",
            "Example prediction:\n",
            " 14 [226 223 223]\n",
            "Example prediction:\n",
            " 16 [226 223 223]\n",
            "Example prediction:\n",
            " 18 [226 223 223]\n",
            "Example prediction:\n",
            " 20 [226 223 223]\n",
            "Example prediction:\n",
            " 22 [226 223 223]\n",
            "Building Eagle_assembly_Herbert\n",
            "Building Eagle_public_Alvin\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_6 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [543 574 312]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 4ms/step - loss: 53407.0362\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 51895.1676\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49052.7011\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 48237.3572\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 47400.7282\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 44134.8230\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 43757.4127\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 42118.9322\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 40168.8752\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 38104.1005\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 37600.1562\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 36583.9574\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 35490.8793\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 33148.8901\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 32254.7637\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 31497.0092\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 30883.8532\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28554.9494\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28052.9054\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28383.7485\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26264.0550\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25665.7891\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 24331.7105\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23013.9778\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23183.4671\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 21475.8274\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 21432.9850\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20510.8331\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19665.8703\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19006.3988\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18191.1760\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17802.4739\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17172.5417\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17072.8797\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16418.4755\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15433.6479\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15317.5572\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15372.5446\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14710.0214\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14011.8909\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14132.7521\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14051.2904\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13852.9175\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13327.1457\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13360.5027\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13610.1310\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12979.6991\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13299.6413\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12858.7641\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12930.3578\n",
            "mean,rmse,rmse/mean,bldg: 182.0635582126762 75.88929144212759 0.416828563536466 Eagle_public_Alvin\n",
            "Example prediction:\n",
            " 0 [191 191 191]\n",
            "Example prediction:\n",
            " 2 [191 191 191]\n",
            "Example prediction:\n",
            " 4 [191 191 191]\n",
            "Example prediction:\n",
            " 6 [191 191 191]\n",
            "Example prediction:\n",
            " 8 [191 191 191]\n",
            "Example prediction:\n",
            " 10 [191 191 191]\n",
            "Example prediction:\n",
            " 12 [191 191 191]\n",
            "Example prediction:\n",
            " 14 [191 191 191]\n",
            "Example prediction:\n",
            " 16 [191 191 191]\n",
            "Example prediction:\n",
            " 18 [191 191 191]\n",
            "Example prediction:\n",
            " 20 [191 191 191]\n",
            "Example prediction:\n",
            " 22 [191 191 191]\n",
            "Building Eagle_education_Raul\n",
            "Building Eagle_education_Roman\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_9 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [2492 2202 1264]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 1670026.0625\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1673481.8203\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1616884.6429\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1620522.2086\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1606592.3166\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1619621.3869\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1605985.1599\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1568724.6089\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1567645.3580\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1551403.2155\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1549464.7178\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1551495.6232\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1532914.0110\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1526565.4053\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1513427.6287\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1517216.4995\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1521487.7275\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1480803.4324\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1496143.4724\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1500027.9389\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1453958.6608\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1445890.9816\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1430731.5469\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1429763.0584\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1405409.7286\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1405805.5777\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1397128.5896\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1362116.9989\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1366642.4733\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1372507.1268\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1354420.9954\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1349608.0841\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1331914.9522\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1321333.4347\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1322821.2647\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1326169.5395\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1300907.8897\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1298975.3428\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1263863.5041\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1275421.4416\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1254877.1746\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1271203.3828\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1279333.5694\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1212894.6613\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1246292.4490\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1234852.0460\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1209332.2220\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1207774.0726\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1194001.0464\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1178590.4315\n",
            "mean,rmse,rmse/mean,bldg: 1199.3775815637814 1061.9480444162843 0.8854159530243073 Eagle_education_Roman\n",
            "Example prediction:\n",
            " 0 [230 229 230]\n",
            "Example prediction:\n",
            " 2 [230 229 230]\n",
            "Example prediction:\n",
            " 4 [230 229 230]\n",
            "Example prediction:\n",
            " 6 [230 229 230]\n",
            "Example prediction:\n",
            " 8 [230 229 230]\n",
            "Example prediction:\n",
            " 10 [230 229 230]\n",
            "Example prediction:\n",
            " 12 [230 229 230]\n",
            "Example prediction:\n",
            " 14 [230 229 230]\n",
            "Example prediction:\n",
            " 16 [230 229 230]\n",
            "Example prediction:\n",
            " 18 [230 229 230]\n",
            "Example prediction:\n",
            " 20 [230 229 230]\n",
            "Example prediction:\n",
            " 22 [230 229 230]\n",
            "Building Eagle_office_Mandi\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_12 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [65 65 45]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 1409.9470\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1030.7449\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 825.4044\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 652.6366\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 520.4309\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 444.3541\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 368.9608\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 297.5004\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 237.1350\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 212.2291\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 191.4072\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 170.4277\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 148.2457\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 118.6548\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 101.4080\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 92.6813\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 81.3735\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 77.3848\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 67.4853\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 69.4696\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 64.1230\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 61.7367\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 60.4312\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 60.1070\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 59.1313\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 57.5519\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54.6858\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 56.0781\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 57.0996\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 55.4192\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54.6706\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 56.8454\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54.0171\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.8607\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.8004\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.7583\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54.2066\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50.0797\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50.2308\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 51.4370\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.7341\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.2881\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49.7863\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49.3078\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50.8579\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52.0096\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50.4848\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49.8124\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50.0490\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49.8731\n",
            "mean,rmse,rmse/mean,bldg: 35.89274527609939 10.312526796752291 0.2873150748828147 Eagle_office_Mandi\n",
            "Example prediction:\n",
            " 0 [52 52 52]\n",
            "Example prediction:\n",
            " 2 [52 52 52]\n",
            "Example prediction:\n",
            " 4 [50 50 49]\n",
            "Example prediction:\n",
            " 6 [40 40 39]\n",
            "Example prediction:\n",
            " 8 [40 39 39]\n",
            "Example prediction:\n",
            " 10 [43 42 42]\n",
            "Example prediction:\n",
            " 12 [44 44 43]\n",
            "Example prediction:\n",
            " 14 [46 46 46]\n",
            "Example prediction:\n",
            " 16 [48 48 47]\n",
            "Example prediction:\n",
            " 18 [45 45 44]\n",
            "Example prediction:\n",
            " 20 [49 49 49]\n",
            "Example prediction:\n",
            " 22 [46 45 45]\n",
            "Building Eagle_education_Jewell\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_15 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_17 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [155 129   0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 3894.0011\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3547.2729\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3527.0461\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3219.1447\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3049.6800\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2893.5263\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2851.1807\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2672.4582\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2580.2597\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2372.0422\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2316.5739\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2258.5625\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2165.7536\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2064.2749\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1980.5568\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1977.9649\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1866.2136\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1831.5080\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1811.9850\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1724.2521\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1758.0533\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1610.7681\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1532.8587\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1548.2774\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1557.0678\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1522.0308\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1458.6714\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1413.7557\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1398.3937\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1396.5752\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1368.7072\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1326.8788\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1271.2898\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1264.6027\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1279.8562\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1241.3715\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1246.9286\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1194.8182\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1234.0077\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1196.3508\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1209.2381\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1151.3438\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1198.8857\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1118.9708\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1140.9351\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1113.7258\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1116.2762\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1143.2981\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1071.1915\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1098.9998\n",
            "mean,rmse,rmse/mean,bldg: 15.763918737885179 44.98780547062137 2.853846573219315 Eagle_education_Jewell\n",
            "Example prediction:\n",
            " 0 [128 129 124]\n",
            "Example prediction:\n",
            " 2 [128 129 124]\n",
            "Example prediction:\n",
            " 4 [128 129 124]\n",
            "Example prediction:\n",
            " 6 [128 129 124]\n",
            "Example prediction:\n",
            " 8 [128 129 124]\n",
            "Example prediction:\n",
            " 10 [128 129 124]\n",
            "Example prediction:\n",
            " 12 [128 129 124]\n",
            "Example prediction:\n",
            " 14 [128 129 124]\n",
            "Example prediction:\n",
            " 16 [128 129 124]\n",
            "Example prediction:\n",
            " 18 [128 129 124]\n",
            "Example prediction:\n",
            " 20 [128 129 124]\n",
            "Example prediction:\n",
            " 22 [128 129 124]\n",
            "Building Eagle_office_Henriette\n",
            " Count bad values before pseudofill: 162\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_18 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_19 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_20 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 0.0062\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.9756e-06\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4.7832e-07\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.3725e-07\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.9091e-07\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.3919e-07\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.4193e-07\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7.3258e-08\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9.7932e-08\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9.0569e-08\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.8672e-07\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1240e-07\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4.9592e-07\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.7237e-07\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.7313e-06\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7.8065e-07\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4.1664e-07\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6.9813e-07\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.0795e-06\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.1423e-06\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0024e-06\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.1380e-06\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8.1761e-07\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.6888e-06\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3.0333e-06\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.5669e-06\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3.8347e-06\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.2560e-06\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3.5334e-06\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.3407e-06\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0499e-06\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8.1302e-07\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8.8402e-07\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4.2028e-06\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8.2185e-06\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3076e-08\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1082e-06\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9.6498e-07\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.8382e-06\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0497e-06\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2.1148e-06\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.9848e-07\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0380e-06\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.2413e-06\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0219e-06\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1503e-06\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1715e-06\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5616e-07\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.1267e-06\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1.0121e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean,rmse,rmse/mean,bldg: 0.0 0.001584130407754609 inf Eagle_office_Henriette\n",
            "Example prediction:\n",
            " 0 [0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0]\n",
            "Building Eagle_health_Reba\n",
            " Count bad values before pseudofill: 36\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_21 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_22 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_23 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [2748 2678 1436]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 4ms/step - loss: 1451496.6857\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1473198.5607\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1434579.2652\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1427972.4733\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1431784.0211\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1398650.8249\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1409417.8566\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1388559.2964\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1364155.5106\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1387348.2063\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1345879.0873\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1321576.4435\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1336145.7197\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1326570.3309\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1332456.1916\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1316903.5391\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1297086.2243\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1301382.0848\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1274093.0119\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1252646.7904\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1267650.8125\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1257831.2633\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1252659.7638\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1248740.3203\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1239165.5680\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1212017.9669\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1203461.3162\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1196280.5083\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1209814.2362\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1169732.7431\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1168072.3240\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1182332.1363\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1122734.9825\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1163166.8952\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1131321.1199\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1127212.7178\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1123529.1383\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1118372.8690\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1115054.0340\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1086048.5726\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1098946.3058\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1049044.3056\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1095031.3309\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1064696.0531\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1032044.7321\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1032222.4380\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1036787.9437\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1018110.8024\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1016668.8511\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 1014858.2716\n",
            "mean,rmse,rmse/mean,bldg: 1084.4328846508208 910.7884535691945 0.8398753546306017 Eagle_health_Reba\n",
            "Example prediction:\n",
            " 0 [228 229 232]\n",
            "Example prediction:\n",
            " 2 [228 229 232]\n",
            "Example prediction:\n",
            " 4 [228 229 232]\n",
            "Example prediction:\n",
            " 6 [228 229 232]\n",
            "Example prediction:\n",
            " 8 [228 229 232]\n",
            "Example prediction:\n",
            " 10 [228 229 232]\n",
            "Example prediction:\n",
            " 12 [228 229 232]\n",
            "Example prediction:\n",
            " 14 [228 229 232]\n",
            "Example prediction:\n",
            " 16 [228 229 232]\n",
            "Example prediction:\n",
            " 18 [228 229 232]\n",
            "Example prediction:\n",
            " 20 [228 229 232]\n",
            "Example prediction:\n",
            " 22 [228 229 232]\n",
            "Building Eagle_lodging_Edgardo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_24 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_25 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_26 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [152 213  95]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 4ms/step - loss: 9487.2781\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 8607.8955\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 8114.7514\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7362.5268\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 6712.9141\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 6345.1659\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 5953.5439\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5623.6207\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5160.4927\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5074.7380\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4818.8233\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4385.2335\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 4271.6737\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4001.5380\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3921.2816\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3596.0704\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3615.6074\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3474.3161\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3344.0812\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3428.4203\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3309.7611\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3324.1415\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 3351.5248\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3219.5914\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3265.8489\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3156.3025\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2839.8244\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2758.4297\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2711.5857\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2418.6308\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2457.2098\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2340.2575\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2356.8479\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2231.0614\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2224.8932\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2168.3644\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2064.6476\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2068.6296\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2034.4951\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 2022.0724\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1926.1406\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1957.3089\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1930.2737\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1918.0935\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1885.1201\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1866.5420\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1871.7139\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1901.6417\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1845.8706\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1859.9092\n",
            "mean,rmse,rmse/mean,bldg: 81.9636656538589 45.6757143621696 0.5572678332257966 Eagle_lodging_Edgardo\n",
            "Example prediction:\n",
            " 0 [140 139 138]\n",
            "Example prediction:\n",
            " 2 [140 139 138]\n",
            "Example prediction:\n",
            " 4 [138 137 136]\n",
            "Example prediction:\n",
            " 6 [139 138 137]\n",
            "Example prediction:\n",
            " 8 [140 138 138]\n",
            "Example prediction:\n",
            " 10 [140 139 138]\n",
            "Example prediction:\n",
            " 12 [140 139 138]\n",
            "Example prediction:\n",
            " 14 [124 123 122]\n",
            "Example prediction:\n",
            " 16 [134 133 133]\n",
            "Example prediction:\n",
            " 18 [131 130 129]\n",
            "Example prediction:\n",
            " 20 [126 126 124]\n",
            "Example prediction:\n",
            " 22 [139 138 137]\n",
            "Building Eagle_education_Cassie\n",
            "Building Eagle_education_Peter\n",
            " Count bad values before pseudofill: 34\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_27 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_28 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_29 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [6385 5795 3466]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 4ms/step - loss: 12589556.1838\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12723975.5221\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12558171.3529\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12610464.5625\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12412524.6728\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12803910.5257\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12461906.2574\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12501328.8750\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12595894.3309\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12488190.9412\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12422280.6029\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12208692.2059\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12523465.1801\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12448212.8493\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12437877.3493\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12245913.5662\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12162556.6618\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12203600.5184\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11946797.0110\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12037881.3382\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12093005.1618\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11946996.8860\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11997695.9890\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12156091.7610\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12228381.0846\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11991557.3750\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11722287.2868\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 12036141.0441\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11858321.8493\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11855231.2463\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11984152.9118\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11935058.2941\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11801725.9191\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11682918.7757\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11707733.7941\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11664940.9375\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11668684.0515\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11817169.2279\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11491288.8015\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11366474.2500\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11506263.9963\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11643978.6360\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11452599.6838\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11443765.9926\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11095181.9559\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11299504.8346\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11362735.9559\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11476491.7243\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11441218.0110\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 11152176.9963\n",
            "mean,rmse,rmse/mean,bldg: 3154.8298985018228 3007.108607435327 0.9531761471080751 Eagle_education_Peter\n",
            "Example prediction:\n",
            " 0 [231 233 229]\n",
            "Example prediction:\n",
            " 2 [231 233 229]\n",
            "Example prediction:\n",
            " 4 [231 233 229]\n",
            "Example prediction:\n",
            " 6 [231 233 229]\n",
            "Example prediction:\n",
            " 8 [231 233 229]\n",
            "Example prediction:\n",
            " 10 [231 233 229]\n",
            "Example prediction:\n",
            " 12 [231 233 229]\n",
            "Example prediction:\n",
            " 14 [231 233 229]\n",
            "Example prediction:\n",
            " 16 [231 233 229]\n",
            "Example prediction:\n",
            " 18 [231 233 229]\n",
            "Example prediction:\n",
            " 20 [231 233 229]\n",
            "Example prediction:\n",
            " 22 [231 233 229]\n",
            "Building Eagle_health_Gregoria\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_30 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_31 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_32 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 540031.2284\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 504103.5443\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 513722.5668\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 519222.4782\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 496051.4666\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 502199.8549\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 500112.4267\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 493318.9588\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 489698.8425\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 478819.5146\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 4ms/step - loss: 490395.0786\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 481850.5631\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 485283.5298\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 469202.4545\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 447805.7880\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 468359.5890\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 464303.5186\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 463489.4755\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 484119.5145\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 469457.2281\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 427533.1280\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 454736.7356\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 449728.3185\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 431980.1989\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 438078.3187\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 419057.0665\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 449414.9345\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 425630.5873\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 426592.8845\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 422534.2016\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 403937.3234\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 407648.1448\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 422127.2252\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 406658.0991\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 392881.3559\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 406938.8871\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 398297.7501\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 380388.9559\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 387298.1097\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 379523.0124\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 383446.8169\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 391790.7772\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 383606.5935\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 356403.2879\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 368815.1321\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 377656.5840\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 381880.2061\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 380654.0172\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 363948.3149\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 342898.8787\n",
            "mean,rmse,rmse/mean,bldg: 659.5054129061737 756.3530065870026 1.146848823050687 Eagle_health_Gregoria\n",
            "Example prediction:\n",
            " 0 [224 222 221]\n",
            "Example prediction:\n",
            " 2 [224 222 221]\n",
            "Example prediction:\n",
            " 4 [224 222 221]\n",
            "Example prediction:\n",
            " 6 [224 222 221]\n",
            "Example prediction:\n",
            " 8 [224 222 221]\n",
            "Example prediction:\n",
            " 10 [224 222 221]\n",
            "Example prediction:\n",
            " 12 [224 222 221]\n",
            "Example prediction:\n",
            " 14 [224 222 221]\n",
            "Example prediction:\n",
            " 16 [224 221 221]\n",
            "Example prediction:\n",
            " 18 [224 221 221]\n",
            "Example prediction:\n",
            " 20 [224 221 221]\n",
            "Example prediction:\n",
            " 22 [224 222 221]\n",
            "Building Eagle_lodging_Dawn\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_33 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_34 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_35 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [203 177 130]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 12938.9207\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12176.2050\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10873.5295\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10197.7648\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9683.1790\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9163.4206\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8458.8851\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8163.1795\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7539.6560\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6917.4547\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6548.7063\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6104.8169\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5694.3521\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5210.5122\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5116.1266\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4799.3809\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4688.7001\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4312.4866\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4198.2585\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4016.0211\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3718.2214\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3637.4097\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3719.0138\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3475.5403\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3497.6651\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3462.4960\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3245.8649\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3375.1567\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3331.2408\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3429.6592\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3417.2991\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3373.4808\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3330.7160\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3211.2673\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3385.3011\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3313.6081\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3343.6536\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3212.0611\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3330.1838\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3308.4499\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3254.2766\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3298.3423\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3359.8026\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3305.7906\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3212.1231\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2796.6212\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2647.1805\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2443.7594\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2468.2260\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2298.2190\n",
            "mean,rmse,rmse/mean,bldg: 92.8091523125402 46.74454694902982 0.503663117098784 Eagle_lodging_Dawn\n",
            "Example prediction:\n",
            " 0 [125 125 125]\n",
            "Example prediction:\n",
            " 2 [125 125 125]\n",
            "Example prediction:\n",
            " 4 [125 125 125]\n",
            "Example prediction:\n",
            " 6 [125 125 125]\n",
            "Example prediction:\n",
            " 8 [125 125 125]\n",
            "Example prediction:\n",
            " 10 [125 125 125]\n",
            "Example prediction:\n",
            " 12 [125 125 125]\n",
            "Example prediction:\n",
            " 14 [125 125 125]\n",
            "Example prediction:\n",
            " 16 [125 125 125]\n",
            "Example prediction:\n",
            " 18 [125 125 125]\n",
            "Example prediction:\n",
            " 20 [125 125 125]\n",
            "Example prediction:\n",
            " 22 [125 125 125]\n",
            "Building Eagle_office_Nereida\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_36 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_37 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_38 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [425 437 361]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 72381.8386\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 68463.8026\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 66103.6569\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 64523.8865\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 61523.2195\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 60615.6929\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 59445.3132\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 57308.1135\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 53951.3443\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 53074.5069\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 51713.1128\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49402.4761\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 48838.3113\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 46420.4287\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 45172.0630\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 43170.8107\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 42353.9824\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 40855.4718\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 39011.4490\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 37824.9969\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 36175.7570\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 35018.0285\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 34353.5676\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 32368.6344\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 31511.5180\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 31179.4679\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 29419.0259\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28140.2192\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27767.3571\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26301.5657\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25316.2140\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 24700.9037\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23563.8862\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23048.4710\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 21976.0517\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20775.9410\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20452.2775\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20368.9404\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18886.8194\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18265.3143\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18184.5529\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17848.0552\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17259.7992\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16795.7246\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15793.1480\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16002.2475\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15346.4460\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14981.8904\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14684.3703\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14135.4453\n",
            "mean,rmse,rmse/mean,bldg: 273.01729748007574 132.886448260309 0.48673270700002735 Eagle_office_Nereida\n",
            "Example prediction:\n",
            " 0 [204 206 203]\n",
            "Example prediction:\n",
            " 2 [204 206 203]\n",
            "Example prediction:\n",
            " 4 [204 206 203]\n",
            "Example prediction:\n",
            " 6 [204 206 203]\n",
            "Example prediction:\n",
            " 8 [204 206 203]\n",
            "Example prediction:\n",
            " 10 [204 206 203]\n",
            "Example prediction:\n",
            " 12 [204 206 203]\n",
            "Example prediction:\n",
            " 14 [204 206 203]\n",
            "Example prediction:\n",
            " 16 [204 206 203]\n",
            "Example prediction:\n",
            " 18 [204 206 203]\n",
            "Example prediction:\n",
            " 20 [204 206 203]\n",
            "Example prediction:\n",
            " 22 [204 206 203]\n",
            "Building Eagle_lodging_Tressa\n",
            "Building Eagle_education_Eileen\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_39 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_40 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_41 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [39 59 29]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 2364.1620\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1864.2599\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1620.9548\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1416.8676\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1185.7730\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1034.0914\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 956.2223\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 771.7577\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 723.2730\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 621.0767\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 583.2242\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 530.0435\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 504.9539\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 461.1846\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 443.4860\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 422.1563\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 377.9474\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 355.2102\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 348.7136\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 333.9409\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 327.7107\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 317.0976\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 314.6446\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 302.0909\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 305.9197\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 287.5231\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 275.8062\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 273.9863\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 275.5916\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 263.5028\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 258.0005\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 259.8915\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 260.3344\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 247.4434\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 259.7947\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 249.3681\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 245.1045\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 237.3497\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 244.3597\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 235.0436\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 231.6615\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 238.5102\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 236.0954\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 245.7880\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 236.1074\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 223.2086\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 227.8292\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 223.3669\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 240.2911\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 224.3282\n",
            "mean,rmse,rmse/mean,bldg: 46.462781809371094 17.39741383968319 0.3744376286177144 Eagle_education_Eileen\n",
            "Example prediction:\n",
            " 0 [35 36 38]\n",
            "Example prediction:\n",
            " 2 [34 35 37]\n",
            "Example prediction:\n",
            " 4 [32 33 35]\n",
            "Example prediction:\n",
            " 6 [31 32 34]\n",
            "Example prediction:\n",
            " 8 [32 33 35]\n",
            "Example prediction:\n",
            " 10 [33 34 36]\n",
            "Example prediction:\n",
            " 12 [36 37 39]\n",
            "Example prediction:\n",
            " 14 [32 34 35]\n",
            "Example prediction:\n",
            " 16 [43 44 44]\n",
            "Example prediction:\n",
            " 18 [40 41 42]\n",
            "Example prediction:\n",
            " 20 [23 24 25]\n",
            "Example prediction:\n",
            " 22 [40 40 41]\n",
            "Building Eagle_education_Wesley\n",
            " Count bad values before pseudofill: 112\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_42 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_43 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_44 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 0.0112\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4989e-04\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.8034e-04\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5722e-04\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5452e-04\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.6097e-04\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.7506e-04\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4525e-04\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5079e-04\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4275e-04\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.8153e-04\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.6484e-04\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5078e-04\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.6870e-04\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.6215e-04\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.7525e-04\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3908e-04\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4236e-04\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4831e-04\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4828e-04\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5000e-04\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4430e-04\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4941e-04\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4709e-04\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3477e-04\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4209e-04\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.6565e-04\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4828e-04\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4527e-04\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3690e-04\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4989e-04\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4077e-04\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3919e-04\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3646e-04\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3958e-04\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4212e-04\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4546e-04\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3579e-04\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4731e-04\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5630e-04\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3237e-04\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3348e-04\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4494e-04\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3051e-04\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3483e-04\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3246e-04\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.5280e-04\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3254e-04\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.3812e-04\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5.4615e-04\n",
            "mean,rmse,rmse/mean,bldg: 0.10558919611442527 0.03097821131404654 0.2933842898138552 Eagle_education_Wesley\n",
            "Example prediction:\n",
            " 0 [0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0]\n",
            "Building Eagle_health_Vincenza\n",
            " Count bad values before pseudofill: 75\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_45 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_46 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_47 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [248 235 213]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 16437.0977\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15204.8810\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14440.8060\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13619.4118\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12648.0093\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11815.2511\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10943.2336\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10214.4763\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9896.6283\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9068.5471\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8523.3734\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7968.2711\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7424.1010\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7074.9335\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6512.3206\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6107.2203\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5950.3127\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5483.7675\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5289.3383\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4811.5506\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4632.0453\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4351.7998\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4219.2649\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4111.5291\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3913.8782\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3848.5670\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3664.5952\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3581.8131\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3508.8705\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3431.6017\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3467.6788\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3389.4545\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3424.3158\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3287.7189\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3317.7875\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3395.3335\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3392.5278\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3342.1395\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3456.1687\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3342.9412\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3370.5041\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3346.9924\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3372.3819\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3349.5946\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3370.9309\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2963.8300\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2148.3995\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1943.2047\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1806.8512\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1743.3044\n",
            "mean,rmse,rmse/mean,bldg: 122.36347174423398 40.00389756041577 0.3269267943298678 Eagle_health_Vincenza\n",
            "Example prediction:\n",
            " 0 [139 139 139]\n",
            "Example prediction:\n",
            " 2 [139 139 139]\n",
            "Example prediction:\n",
            " 4 [139 139 139]\n",
            "Example prediction:\n",
            " 6 [139 139 139]\n",
            "Example prediction:\n",
            " 8 [139 139 139]\n",
            "Example prediction:\n",
            " 10 [139 139 139]\n",
            "Example prediction:\n",
            " 12 [139 139 139]\n",
            "Example prediction:\n",
            " 14 [139 139 139]\n",
            "Example prediction:\n",
            " 16 [66 64 65]\n",
            "Example prediction:\n",
            " 18 [65 63 64]\n",
            "Example prediction:\n",
            " 20 [123 122 123]\n",
            "Example prediction:\n",
            " 22 [139 139 139]\n",
            "Building Eagle_office_Dallas\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_48 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_49 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_50 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [117  44   0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 4340.8571\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3713.5544\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3551.7670\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3418.4223\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3266.6659\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3140.4565\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3065.6290\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2975.2730\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2878.5481\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3006.6045\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3081.4080\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2930.5784\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2761.4096\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2750.0765\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3108.3805\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3010.4131\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3016.5942\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2847.1095\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2833.7008\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2862.9869\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2891.8953\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2855.2333\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2826.9238\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2891.1243\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2400.8679\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2482.6939\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2420.1657\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2345.1497\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2172.0516\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2165.8715\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2532.1780\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2173.5130\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2299.5233\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2170.3082\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2105.5931\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2080.7159\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2235.1147\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2244.7402\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2026.6355\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1887.0873\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1900.4185\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1828.8472\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2094.4264\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2205.4886\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1930.5950\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1877.3139\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2049.4500\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1851.0398\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1917.8500\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1841.7443\n",
            "mean,rmse,rmse/mean,bldg: 56.5030097936379 54.07694609753777 0.9570631068157134 Eagle_office_Dallas\n",
            "Example prediction:\n",
            " 0 [86 86 83]\n",
            "Example prediction:\n",
            " 2 [74 73 72]\n",
            "Example prediction:\n",
            " 4 [86 86 84]\n",
            "Example prediction:\n",
            " 6 [86 86 84]\n",
            "Example prediction:\n",
            " 8 [82 82 80]\n",
            "Example prediction:\n",
            " 10 [86 86 83]\n",
            "Example prediction:\n",
            " 12 [79 78 76]\n",
            "Example prediction:\n",
            " 14 [84 84 82]\n",
            "Example prediction:\n",
            " 16 [84 84 82]\n",
            "Example prediction:\n",
            " 18 [56 54 54]\n",
            "Example prediction:\n",
            " 20 [83 83 81]\n",
            "Example prediction:\n",
            " 22 [84 84 82]\n",
            "Building Eagle_education_Shante\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_51 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_52 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_53 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 210695.3664\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 213120.9824\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 232469.9579\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 203460.7830\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 205744.5418\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 207726.2155\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 208426.6889\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 214600.8230\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 208850.9127\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 210713.6190\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 183377.3325\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 196060.5922\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 215473.5989\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 199016.5168\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 193043.6469\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 192308.2981\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 213812.3554\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 218695.3720\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 196848.9056\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 211982.4400\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 212014.4999\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 228801.7379\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 211530.7000\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 212076.7253\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 200187.2199\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 207450.6679\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 215054.4103\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 205705.8244\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 220483.6277\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 204233.0244\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 189917.5629\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 208340.8369\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 205217.4729\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 196770.6493\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 198104.3031\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 184558.6870\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 204519.5124\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 206521.6425\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 172743.2337\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 212793.4994\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 196597.9642\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 204040.0401\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 201012.9518\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 185954.7124\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 189496.5927\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 196730.5791\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 204158.1579\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 186863.5875\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 193866.0526\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 178948.9942\n",
            "mean,rmse,rmse/mean,bldg: 1003.9941709339338 1847.5049215016759 1.8401550277758016 Eagle_education_Shante\n",
            "Example prediction:\n",
            " 0 [158 162 165]\n",
            "Example prediction:\n",
            " 2 [158 162 165]\n",
            "Example prediction:\n",
            " 4 [158 162 165]\n",
            "Example prediction:\n",
            " 6 [158 162 165]\n",
            "Example prediction:\n",
            " 8 [158 162 165]\n",
            "Example prediction:\n",
            " 10 [158 162 165]\n",
            "Example prediction:\n",
            " 12 [158 162 165]\n",
            "Example prediction:\n",
            " 14 [158 162 165]\n",
            "Example prediction:\n",
            " 16 [158 162 165]\n",
            "Example prediction:\n",
            " 18 [158 162 165]\n",
            "Example prediction:\n",
            " 20 [158 162 165]\n",
            "Example prediction:\n",
            " 22 [158 162 165]\n",
            "Building Eagle_office_Chauncey\n",
            " Count bad values before pseudofill: 116\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_54 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_55 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_56 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1450 1449 1232]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 1086650.7750\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1081906.7403\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1069800.0680\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1053185.6868\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1053270.2847\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1040877.7463\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1033926.0423\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1030689.6147\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1025032.4637\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1010711.8759\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 988247.6983\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 993905.3575\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 965649.7397\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 976824.2470\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 962373.2792\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 949500.6298\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 952047.4465\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 946087.2213\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 934904.1845\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 923043.8649\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 919600.2976\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 913711.7948\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 885983.9609\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 903790.6147\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 882709.4170\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 883418.0473\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 864343.0129\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 863569.1167\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 843473.1969\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 840413.0993\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 846711.8104\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 842769.0322\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 824274.8681\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 822679.8431\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 803137.1772\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 804797.3996\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 797503.6778\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 791724.4731\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 793745.1468\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 780264.3362\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 763047.3863\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 757784.4060\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 750972.7355\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 750166.5898\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 737686.9710\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 722629.5246\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 732259.5122\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 711014.2725\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 718859.2537\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 698770.7647\n",
            "mean,rmse,rmse/mean,bldg: 1037.6368908901366 935.4922416422072 0.9015603144561442 Eagle_office_Chauncey\n",
            "Example prediction:\n",
            " 0 [229 229 231]\n",
            "Example prediction:\n",
            " 2 [229 229 231]\n",
            "Example prediction:\n",
            " 4 [229 229 231]\n",
            "Example prediction:\n",
            " 6 [229 229 231]\n",
            "Example prediction:\n",
            " 8 [229 229 231]\n",
            "Example prediction:\n",
            " 10 [229 229 231]\n",
            "Example prediction:\n",
            " 12 [229 229 231]\n",
            "Example prediction:\n",
            " 14 [229 229 231]\n",
            "Example prediction:\n",
            " 16 [229 229 231]\n",
            "Example prediction:\n",
            " 18 [229 229 231]\n",
            "Example prediction:\n",
            " 20 [229 229 231]\n",
            "Example prediction:\n",
            " 22 [229 229 231]\n",
            "Building Eagle_office_Phyllis\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_57 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_58 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_59 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [158 159 111]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 9114.6858\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8146.0566\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7252.2414\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6702.2207\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5939.2930\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5512.7610\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5012.9668\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4583.1859\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4135.9491\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3741.7411\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3509.5927\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3133.8999\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2831.5770\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2657.6349\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2432.4717\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2250.4133\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2112.1311\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1988.3181\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1874.4397\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1739.9358\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1693.1471\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1617.1472\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1611.3609\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1543.2899\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1534.7035\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1521.2404\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1549.4653\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1482.3308\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1544.0147\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1518.0696\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1520.8135\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1478.5504\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1533.7392\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1461.6867\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1483.0217\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1505.5688\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1503.1890\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1365.1843\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 972.7262\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 884.7708\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 805.7988\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 599.9593\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 526.7136\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 492.9378\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 472.4658\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 425.1358\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 414.3233\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 418.0424\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 409.6522\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 391.7192\n",
            "mean,rmse,rmse/mean,bldg: 87.20950227834119 20.891204065518824 0.2395519240419657 Eagle_office_Phyllis\n",
            "Example prediction:\n",
            " 0 [125 125 125]\n",
            "Example prediction:\n",
            " 2 [125 125 125]\n",
            "Example prediction:\n",
            " 4 [125 125 125]\n",
            "Example prediction:\n",
            " 6 [125 125 125]\n",
            "Example prediction:\n",
            " 8 [125 125 125]\n",
            "Example prediction:\n",
            " 10 [125 125 125]\n",
            "Example prediction:\n",
            " 12 [125 125 125]\n",
            "Example prediction:\n",
            " 14 [125 125 125]\n",
            "Example prediction:\n",
            " 16 [125 125 125]\n",
            "Example prediction:\n",
            " 18 [125 125 125]\n",
            "Example prediction:\n",
            " 20 [125 125 125]\n",
            "Example prediction:\n",
            " 22 [125 125 125]\n",
            "Building Eagle_office_Freida\n",
            " Count bad values before pseudofill: 63\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_60 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_61 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_62 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [205 162 221]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 20152.8773\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18291.4824\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 17320.5632\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16525.1543\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15338.9898\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14439.8856\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13824.2506\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13282.3801\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12502.1803\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11828.1090\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11071.5077\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10728.2678\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9956.7065\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9552.8249\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9218.4549\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8665.7875\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8140.9706\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8090.7084\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7599.5968\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7015.8883\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7138.6700\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6573.3308\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6447.4968\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6081.8249\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5615.8156\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5240.5686\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5253.9426\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5032.2701\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4568.4418\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4324.7580\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4355.5872\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4453.6939\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4032.9845\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3992.2387\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3857.5702\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3665.0803\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3703.9029\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3603.5957\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3548.6945\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3514.8246\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3347.1743\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3349.7982\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3254.1241\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3115.1841\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3181.7669\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3072.0297\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3110.5549\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3028.5611\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3027.6881\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2954.0268\n",
            "mean,rmse,rmse/mean,bldg: 101.557715031556 48.56638470098512 0.4782146258991213 Eagle_office_Freida\n",
            "Example prediction:\n",
            " 0 [168 167 167]\n",
            "Example prediction:\n",
            " 2 [168 167 167]\n",
            "Example prediction:\n",
            " 4 [168 167 166]\n",
            "Example prediction:\n",
            " 6 [168 167 167]\n",
            "Example prediction:\n",
            " 8 [168 167 167]\n",
            "Example prediction:\n",
            " 10 [168 167 167]\n",
            "Example prediction:\n",
            " 12 [168 167 167]\n",
            "Example prediction:\n",
            " 14 [166 165 165]\n",
            "Example prediction:\n",
            " 16 [167 165 165]\n",
            "Example prediction:\n",
            " 18 [168 166 166]\n",
            "Example prediction:\n",
            " 20 [167 166 166]\n",
            "Example prediction:\n",
            " 22 [168 167 166]\n",
            "Building Eagle_office_Francis\n",
            " Count bad values before pseudofill: 20\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_63 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_64 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_65 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [408 390 280]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 87230.5065\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 84457.6610\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 82001.1007\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 79702.6863\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 77689.9615\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 74903.1711\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 73079.6701\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 70412.3763\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 68684.8928\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 65603.6826\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 63302.6940\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 61740.2836\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 60060.5504\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 58041.6520\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 55566.2245\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 53714.9209\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52943.2021\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 49652.8144\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 49652.4083\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 47116.8613\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 45244.8837\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 43924.1829\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 42854.9708\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 40290.2777\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 39321.3983\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 38089.2558\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 36727.4602\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 34771.3239\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 33950.9205\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 32679.7809\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 31131.6500\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 30349.1975\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28840.2207\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27433.2832\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26746.9195\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25143.2507\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 24661.7531\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23914.9726\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 22737.8136\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20934.5956\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20473.3458\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 19791.1098\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18989.9600\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17852.1156\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17541.3509\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16807.5857\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16099.4028\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15101.0800\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14425.1539\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14193.2286\n",
            "mean,rmse,rmse/mean,bldg: 336.48649201094844 202.53720683673723 0.6019177935682101 Eagle_office_Francis\n",
            "Example prediction:\n",
            " 0 [210 212 213]\n",
            "Example prediction:\n",
            " 2 [210 212 213]\n",
            "Example prediction:\n",
            " 4 [210 212 213]\n",
            "Example prediction:\n",
            " 6 [210 212 213]\n",
            "Example prediction:\n",
            " 8 [210 212 213]\n",
            "Example prediction:\n",
            " 10 [210 212 213]\n",
            "Example prediction:\n",
            " 12 [210 212 213]\n",
            "Example prediction:\n",
            " 14 [210 212 213]\n",
            "Example prediction:\n",
            " 16 [210 212 213]\n",
            "Example prediction:\n",
            " 18 [210 212 213]\n",
            "Example prediction:\n",
            " 20 [210 212 213]\n",
            "Example prediction:\n",
            " 22 [210 212 213]\n",
            "Building Eagle_office_Sheree\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_66 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_67 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_68 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [122 111  68]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 4493.9596\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 3924.3537\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3539.1530\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3242.7166\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2995.7137\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2670.4470\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2526.9989\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2369.5363\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2218.8247\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2092.9587\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2060.1004\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2097.3747\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2007.4821\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1923.9608\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1888.1421\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1918.7132\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1823.4884\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1876.3127\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1902.9147\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1869.8459\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1870.9633\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1628.1502\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1303.4142\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1273.1201\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1194.2608\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1186.5622\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1142.9037\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1127.0898\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1053.6776\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1030.2100\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1045.9796\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 988.3623\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 981.3446\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 974.6361\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 964.4013\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 969.7193\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 946.9217\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 926.5671\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 914.0599\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 908.2957\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 881.3681\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 887.2239\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 891.2217\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 871.7433\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 859.7940\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 855.3328\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 858.7785\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 880.8624\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 873.8779\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 853.8886\n",
            "mean,rmse,rmse/mean,bldg: 62.02257521757287 39.18762851223849 0.6318284652768731 Eagle_office_Sheree\n",
            "Example prediction:\n",
            " 0 [108 109 107]\n",
            "Example prediction:\n",
            " 2 [108 109 107]\n",
            "Example prediction:\n",
            " 4 [108 109 107]\n",
            "Example prediction:\n",
            " 6 [108 109 107]\n",
            "Example prediction:\n",
            " 8 [108 109 107]\n",
            "Example prediction:\n",
            " 10 [108 109 107]\n",
            "Example prediction:\n",
            " 12 [108 109 107]\n",
            "Example prediction:\n",
            " 14 [108 109 107]\n",
            "Example prediction:\n",
            " 16 [19 19 19]\n",
            "Example prediction:\n",
            " 18 [105 105 103]\n",
            "Example prediction:\n",
            " 20 [108 109 107]\n",
            "Example prediction:\n",
            " 22 [108 109 107]\n",
            "Building Eagle_education_Sherrill\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_69 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_70 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_71 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4088 3945 3076]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 5446202.8180\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5330645.8235\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5338057.1011\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5364850.3419\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5363061.5772\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5381857.4559\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5273939.8456\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5344284.1507\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5183395.6774\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5391594.5000\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5297299.3051\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5306732.6599\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5196293.1029\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5105279.0496\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5127216.1544\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5166055.7206\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5128849.0386\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5184785.2298\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5088073.1820\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5094632.6296\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5140419.5018\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5020357.4678\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 4984428.7169\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4957892.3070\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5035664.4963\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4998001.7004\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4985944.1746\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5011863.2279\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4908308.0092\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4867515.7050\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4891638.7721\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4924148.9835\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4822835.2757\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4843524.3971\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4838719.1783\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4809097.1452\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4707765.1461\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4814566.7151\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4714121.4706\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4753713.8658\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4792943.8199\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4666150.9338\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4650473.7592\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4704547.3419\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4689422.8419\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4593124.0441\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4572220.2904\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4590355.3162\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4667668.2408\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4695673.8713\n",
            "mean,rmse,rmse/mean,bldg: 2032.483907505765 2049.415664937544 1.0083305739195532 Eagle_education_Sherrill\n",
            "Example prediction:\n",
            " 0 [230 229 231]\n",
            "Example prediction:\n",
            " 2 [230 229 231]\n",
            "Example prediction:\n",
            " 4 [230 229 231]\n",
            "Example prediction:\n",
            " 6 [230 229 231]\n",
            "Example prediction:\n",
            " 8 [230 229 231]\n",
            "Example prediction:\n",
            " 10 [230 229 231]\n",
            "Example prediction:\n",
            " 12 [230 229 231]\n",
            "Example prediction:\n",
            " 14 [230 229 231]\n",
            "Example prediction:\n",
            " 16 [230 229 231]\n",
            "Example prediction:\n",
            " 18 [230 229 231]\n",
            "Example prediction:\n",
            " 20 [230 229 231]\n",
            "Example prediction:\n",
            " 22 [230 229 231]\n",
            "Building Eagle_education_Brooke\n",
            " Count bad values before pseudofill: 56\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_72 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_73 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_74 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [3929 4001 3115]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 4651518.8162\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4613085.3070\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4489152.7059\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4513726.5211\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4514054.1176\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4493332.7619\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4423217.5726\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4508541.7776\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4456761.8925\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 4417706.8171\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4387340.4982\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4429673.6636\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4344271.0607\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4401090.9292\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4365468.0147\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4323496.0699\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4213637.3401\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4326040.7335\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4283520.2914\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4250153.5322\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4319334.2904\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4226474.9972\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4280770.5119\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4223023.6232\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4190411.6985\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4169288.9504\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4191644.0432\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4244868.0496\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4172110.4632\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4158588.0230\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4074685.5322\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4077947.5699\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4018750.7767\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4066281.4688\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4067052.2463\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4067389.1388\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3940862.5138\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4027439.1388\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3965385.9366\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4011143.9881\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3909068.1287\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3913904.0377\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3900171.9982\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3845647.1829\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3863673.2142\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3848468.6820\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3864619.0147\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3856048.6241\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3823305.5928\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3824595.0542\n",
            "mean,rmse,rmse/mean,bldg: 1639.356375181079 1618.2012304416526 0.9870954570588168 Eagle_education_Brooke\n",
            "Example prediction:\n",
            " 0 [230 229 229]\n",
            "Example prediction:\n",
            " 2 [230 229 229]\n",
            "Example prediction:\n",
            " 4 [230 229 229]\n",
            "Example prediction:\n",
            " 6 [230 229 229]\n",
            "Example prediction:\n",
            " 8 [230 229 229]\n",
            "Example prediction:\n",
            " 10 [230 229 229]\n",
            "Example prediction:\n",
            " 12 [230 229 229]\n",
            "Example prediction:\n",
            " 14 [230 229 229]\n",
            "Example prediction:\n",
            " 16 [230 229 229]\n",
            "Example prediction:\n",
            " 18 [230 229 229]\n",
            "Example prediction:\n",
            " 20 [230 229 229]\n",
            "Example prediction:\n",
            " 22 [230 229 229]\n",
            "Building Eagle_education_Alberto\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_75 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_76 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_77 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1261 1269  886]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 591707.0345\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 587917.3008\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 583166.1966\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 578317.4745\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 570770.6769\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 564678.3709\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 557957.8357\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 553001.9681\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 541717.5068\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 538910.5591\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 531321.0596\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 523352.4908\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 523780.1813\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 522063.3173\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 506261.4276\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 504507.9828\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 494357.2273\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 486540.2214\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 485343.1557\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 484735.8361\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 477155.4709\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 471413.8566\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 469694.5071\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 457073.7181\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 459849.8718\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 449795.1998\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 442663.0497\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 437086.3308\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 434399.9824\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 424587.1136\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 421706.1168\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 418551.8177\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 401767.5433\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 406166.6595\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 401579.0793\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 404593.8019\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 394411.5925\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 392416.6955\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 383678.0133\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 379625.1594\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 369461.8847\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 370615.7120\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 365738.7040\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 365960.5393\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 350671.3092\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 353917.1410\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 345697.4847\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 346525.3719\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 340431.0446\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 328891.0294\n",
            "mean,rmse,rmse/mean,bldg: 694.962522886029 523.7382914606978 0.7536209136655687 Eagle_education_Alberto\n",
            "Example prediction:\n",
            " 0 [224 229 225]\n",
            "Example prediction:\n",
            " 2 [224 229 225]\n",
            "Example prediction:\n",
            " 4 [224 229 225]\n",
            "Example prediction:\n",
            " 6 [224 229 225]\n",
            "Example prediction:\n",
            " 8 [224 229 225]\n",
            "Example prediction:\n",
            " 10 [224 229 225]\n",
            "Example prediction:\n",
            " 12 [224 229 225]\n",
            "Example prediction:\n",
            " 14 [224 229 225]\n",
            "Example prediction:\n",
            " 16 [224 229 225]\n",
            "Example prediction:\n",
            " 18 [224 229 225]\n",
            "Example prediction:\n",
            " 20 [224 229 225]\n",
            "Example prediction:\n",
            " 22 [224 229 225]\n",
            "Building Eagle_food_Kay\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_78 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_79 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_80 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [678 665 621]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 126980.4852\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 125162.1233\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 122161.7452\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 117687.3041\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 111222.4793\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 110382.1648\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 109284.8450\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 105971.4186\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 106115.2161\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 103353.2561\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 99105.5217\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 101011.2703\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 96288.0552\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 95402.2516\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 94093.5437\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 92030.6165\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 87982.2663\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 87838.5316\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 86461.3627\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 84858.9880\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 81363.6147\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 79783.2110\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 77971.0453\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 74094.0516\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 76946.9084\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 74003.5790\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 73049.1764\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 71945.2128\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 71171.8281\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 67674.8584\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 64135.9237\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 65191.7486\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 63407.4055\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 61631.3660\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 59002.1196\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 57976.3003\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 58604.8485\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 56605.3455\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 55540.9052\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 54599.2616\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54537.3027\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 55674.7117\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 52019.0370\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 51186.7250\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50321.5808\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49798.5913\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 48941.5856\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 47393.3480\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 47052.6658\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 46712.8159\n",
            "mean,rmse,rmse/mean,bldg: 276.2706673678431 176.58034013235635 0.6391570332627707 Eagle_food_Kay\n",
            "Example prediction:\n",
            " 0 [214 212 211]\n",
            "Example prediction:\n",
            " 2 [214 212 211]\n",
            "Example prediction:\n",
            " 4 [214 212 211]\n",
            "Example prediction:\n",
            " 6 [214 212 211]\n",
            "Example prediction:\n",
            " 8 [214 212 211]\n",
            "Example prediction:\n",
            " 10 [214 212 211]\n",
            "Example prediction:\n",
            " 12 [214 212 211]\n",
            "Example prediction:\n",
            " 14 [214 212 211]\n",
            "Example prediction:\n",
            " 16 [214 212 211]\n",
            "Example prediction:\n",
            " 18 [214 212 211]\n",
            "Example prediction:\n",
            " 20 [214 212 211]\n",
            "Example prediction:\n",
            " 22 [214 212 211]\n",
            "Building Eagle_health_Jodi\n",
            " Count bad values before pseudofill: 41\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_81 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_82 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_83 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [321 353 225]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 34659.0650\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 33058.7985\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 31514.3764\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 30282.8676\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28605.9790\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27419.6319\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26802.1604\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25120.2155\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 23930.9988\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 22733.3562\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 22368.2320\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 20738.1043\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20191.6879\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19127.9586\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18059.8103\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17163.6187\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16339.2918\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15672.0503\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14917.9628\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14111.6036\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13669.7762\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12835.0742\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12378.9843\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12028.3083\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11412.9951\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10881.2215\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10422.0884\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9923.1994\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9603.0332\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9099.3019\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8950.2089\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8578.5066\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8171.9113\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8198.7883\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7769.7486\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7608.4000\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7393.6079\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7374.3270\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6856.0591\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7005.7175\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6857.8252\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6782.4480\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6799.5170\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6658.8225\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6784.5574\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6575.8851\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6534.4846\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6567.8287\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6610.8062\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6661.6546\n",
            "mean,rmse,rmse/mean,bldg: 192.4223112628316 85.94674768357824 0.44665687216584093 Eagle_health_Jodi\n",
            "Example prediction:\n",
            " 0 [168 168 169]\n",
            "Example prediction:\n",
            " 2 [168 168 169]\n",
            "Example prediction:\n",
            " 4 [168 168 169]\n",
            "Example prediction:\n",
            " 6 [168 168 169]\n",
            "Example prediction:\n",
            " 8 [168 168 169]\n",
            "Example prediction:\n",
            " 10 [168 168 169]\n",
            "Example prediction:\n",
            " 12 [168 168 169]\n",
            "Example prediction:\n",
            " 14 [168 168 169]\n",
            "Example prediction:\n",
            " 16 [168 168 169]\n",
            "Example prediction:\n",
            " 18 [168 168 169]\n",
            "Example prediction:\n",
            " 20 [168 168 169]\n",
            "Example prediction:\n",
            " 22 [168 168 169]\n",
            "Building Eagle_education_Norah\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_84 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_85 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_86 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1682 1520  923]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 602985.0733\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 596501.2327\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 572689.6651\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 591635.6425\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 588020.9195\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 570699.0896\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 567344.7715\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 554164.5001\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 570773.8316\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 552199.7972\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 549936.5105\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 539199.9498\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 535258.1358\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 518906.1051\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 531503.6773\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 531331.8069\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 510494.3332\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 512182.2241\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 502562.6560\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 500081.4315\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 498843.8925\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 486541.3486\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 481766.8249\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 484618.7886\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 473609.3855\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 487515.4761\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 457489.5535\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 465023.1999\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 453340.4897\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 455063.2440\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 437167.7994\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 421684.5802\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 433923.2800\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 428223.1930\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 428361.9298\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 420034.0362\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 437800.0712\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 401265.6441\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 415715.0412\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 414884.2999\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 397724.1047\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 395375.1810\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 405054.0484\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 395711.3526\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 374830.2726\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 377435.7725\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 381932.0363\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 373334.9616\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 372058.9970\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 355052.1721\n",
            "mean,rmse,rmse/mean,bldg: 712.0113639930826 653.8815182753242 0.9183582613179708 Eagle_education_Norah\n",
            "Example prediction:\n",
            " 0 [227 223 225]\n",
            "Example prediction:\n",
            " 2 [227 223 225]\n",
            "Example prediction:\n",
            " 4 [227 223 225]\n",
            "Example prediction:\n",
            " 6 [227 223 225]\n",
            "Example prediction:\n",
            " 8 [227 223 225]\n",
            "Example prediction:\n",
            " 10 [227 223 225]\n",
            "Example prediction:\n",
            " 12 [227 223 225]\n",
            "Example prediction:\n",
            " 14 [227 223 225]\n",
            "Example prediction:\n",
            " 16 [227 223 225]\n",
            "Example prediction:\n",
            " 18 [227 223 225]\n",
            "Example prediction:\n",
            " 20 [227 223 225]\n",
            "Example prediction:\n",
            " 22 [227 223 225]\n",
            "Building Eagle_education_Will\n",
            " Count bad values before pseudofill: 15\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_87 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_88 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_89 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [336 320 289]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 76797.4187\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 74678.1815\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 70574.8220\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 69665.9562\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 66379.7031\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 64954.1054\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 63542.2996\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 61017.0619\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 58925.0588\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 55933.5847\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 53627.2519\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 51755.2611\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 50535.4923\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 49491.6716\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 47429.2547\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 45862.1595\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 45045.1973\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 42927.2481\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 42200.9413\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 39824.9432\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 38589.5304\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 37284.2236\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 35451.7423\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 33763.4305\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 32505.5523\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 31170.8913\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 30008.4410\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28774.1956\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28118.8653\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26408.9355\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25786.4338\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 24719.4408\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23350.8579\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 22682.4881\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 22040.4738\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20599.4627\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20103.9653\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19364.7599\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18428.6149\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18010.9316\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17430.7801\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16594.5281\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15410.2636\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14919.9915\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14962.7597\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13995.3099\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12956.5098\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13255.2749\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12148.9679\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12069.6541\n",
            "mean,rmse,rmse/mean,bldg: 226.17039013254305 51.34969319898955 0.22703985773246885 Eagle_education_Will\n",
            "Example prediction:\n",
            " 0 [209 209 209]\n",
            "Example prediction:\n",
            " 2 [209 209 209]\n",
            "Example prediction:\n",
            " 4 [209 209 209]\n",
            "Example prediction:\n",
            " 6 [209 209 209]\n",
            "Example prediction:\n",
            " 8 [209 209 209]\n",
            "Example prediction:\n",
            " 10 [209 209 209]\n",
            "Example prediction:\n",
            " 12 [209 209 209]\n",
            "Example prediction:\n",
            " 14 [209 209 209]\n",
            "Example prediction:\n",
            " 16 [209 209 209]\n",
            "Example prediction:\n",
            " 18 [209 209 209]\n",
            "Example prediction:\n",
            " 20 [209 209 209]\n",
            "Example prediction:\n",
            " 22 [209 209 209]\n",
            "Building Eagle_lodging_Blake\n",
            " Count bad values before pseudofill: 8\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_90 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_91 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_92 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [  4 426 415]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 29348.6739\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 30597.4073\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 29335.5846\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 29438.5595\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28608.7814\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28566.4586\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28803.0296\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28224.2976\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28149.1326\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28698.4690\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26856.1937\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 27560.7508\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27895.6000\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26915.5963\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26365.2773\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28501.1052\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25769.6097\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 27176.4389\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26735.7038\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26275.1090\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25275.0991\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25873.8517\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26306.7102\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27203.7134\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26851.8892\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26015.1574\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25475.5421\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26351.6815\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25572.0672\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25971.9425\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26975.4354\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25900.2611\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26471.8620\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27969.9972\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26191.8235\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26418.2101\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 28109.7591\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26016.3505\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25018.9841\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26529.3039\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26874.6629\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 24673.3858\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 25101.5557\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 26180.2211\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26322.2198\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 25449.0977\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26854.8386\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26366.3853\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25210.1334\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25555.3295\n",
            "mean,rmse,rmse/mean,bldg: 43.43471997795112 42.52498417772956 0.9790551015251537 Eagle_lodging_Blake\n",
            "Example prediction:\n",
            " 0 [89 89 90]\n",
            "Example prediction:\n",
            " 2 [89 89 90]\n",
            "Example prediction:\n",
            " 4 [89 89 90]\n",
            "Example prediction:\n",
            " 6 [89 89 90]\n",
            "Example prediction:\n",
            " 8 [89 89 90]\n",
            "Example prediction:\n",
            " 10 [89 89 90]\n",
            "Example prediction:\n",
            " 12 [89 89 90]\n",
            "Example prediction:\n",
            " 14 [89 89 90]\n",
            "Example prediction:\n",
            " 16 [89 89 90]\n",
            "Example prediction:\n",
            " 18 [89 89 90]\n",
            "Example prediction:\n",
            " 20 [89 89 90]\n",
            "Example prediction:\n",
            " 22 [89 89 90]\n",
            "Building Eagle_education_Petra\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_93 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_94 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_95 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [133 126  77]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 4174.7401\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3502.5449\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3103.1303\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2854.0986\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2499.1948\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2347.2776\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2101.4623\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1933.4508\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1764.4491\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 1584.4166\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1585.4844\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1491.2148\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1334.6628\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1248.1913\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1080.8323\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 984.2192\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 900.8524\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 825.4103\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 768.1285\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 726.8134\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 645.7181\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 627.4516\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 558.6503\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 538.8652\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 486.2526\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 469.3392\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 444.2164\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 434.4512\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 405.8434\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 404.2479\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 378.5982\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 357.5597\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 352.1950\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 343.2359\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 337.3857\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 336.6220\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 326.1651\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 325.0990\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 316.5149\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 311.5466\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 310.6619\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 303.6833\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 305.4373\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 298.7139\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 300.9825\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 296.7667\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 295.9675\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 293.6672\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 296.3136\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 300.5723\n",
            "mean,rmse,rmse/mean,bldg: 57.04381605290169 27.761716394647934 0.4866735487840098 Eagle_education_Petra\n",
            "Example prediction:\n",
            " 0 [102 101 101]\n",
            "Example prediction:\n",
            " 2 [84 85 85]\n",
            "Example prediction:\n",
            " 4 [70 70 69]\n",
            "Example prediction:\n",
            " 6 [63 62 62]\n",
            "Example prediction:\n",
            " 8 [74 74 74]\n",
            "Example prediction:\n",
            " 10 [62 62 62]\n",
            "Example prediction:\n",
            " 12 [58 57 58]\n",
            "Example prediction:\n",
            " 14 [56 57 58]\n",
            "Example prediction:\n",
            " 16 [57 57 58]\n",
            "Example prediction:\n",
            " 18 [54 55 56]\n",
            "Example prediction:\n",
            " 20 [63 64 65]\n",
            "Example prediction:\n",
            " 22 [86 87 86]\n",
            "Building Eagle_lodging_Trina\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_96 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_97 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_98 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [314 208 166]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 12357.7471\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10840.4720\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10464.5546\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9719.0676\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9442.3690\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8755.2202\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8170.2957\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7729.2126\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 7732.7750\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7149.2337\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6888.9064\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 6731.7884\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 6439.6067\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5981.0504\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5934.0802\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5683.5025\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5524.2442\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5269.1531\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5308.3354\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5193.5269\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5420.3009\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5303.4960\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5056.0790\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4795.9179\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 5095.9942\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5045.5656\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5219.0152\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4978.6039\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4690.7956\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4395.8357\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 4130.3728\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4102.0343\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4022.0198\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3907.0846\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3674.5080\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3704.2328\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 3623.5992\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3600.2790\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3586.0189\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3569.7913\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3198.1157\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3189.7462\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3274.4793\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 3164.0447\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3079.0943\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3023.6612\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3023.9366\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2947.6578\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2969.0010\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3009.7032\n",
            "mean,rmse,rmse/mean,bldg: 91.27234855584753 54.96596316674614 0.6022192267038432 Eagle_lodging_Trina\n",
            "Example prediction:\n",
            " 0 [153 151 151]\n",
            "Example prediction:\n",
            " 2 [152 150 150]\n",
            "Example prediction:\n",
            " 4 [153 151 151]\n",
            "Example prediction:\n",
            " 6 [153 151 151]\n",
            "Example prediction:\n",
            " 8 [153 151 151]\n",
            "Example prediction:\n",
            " 10 [153 151 151]\n",
            "Example prediction:\n",
            " 12 [153 151 151]\n",
            "Example prediction:\n",
            " 14 [153 151 151]\n",
            "Example prediction:\n",
            " 16 [153 151 150]\n",
            "Example prediction:\n",
            " 18 [153 151 150]\n",
            "Example prediction:\n",
            " 20 [153 151 150]\n",
            "Example prediction:\n",
            " 22 [153 151 151]\n",
            "Building Eagle_health_Reuben\n",
            "Building Eagle_education_Teresa\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_99 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_100 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_101 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [466 412 359]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 6ms/step - loss: 30378.3375\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 28546.5763\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 27770.3653\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 25763.1150\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 24342.7980\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23505.2152\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 22615.7728\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 21244.1705\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20488.6423\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19771.6068\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18859.7276\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17891.2867\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16855.5578\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16589.6771\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 15284.7243\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15047.9393\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14647.6091\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13423.8057\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13975.2822\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12711.7666\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12214.5331\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11538.2096\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 11314.5129\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10948.8696\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10538.2046\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 10521.5290\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 10043.4853\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9734.9896\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9401.7229\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9349.4519\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9136.2721\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8680.6896\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8620.9535\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8548.3051\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8245.1075\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8066.2789\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8238.8402\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7885.6300\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8220.3376\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7938.9415\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7847.0682\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8052.6334\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 7891.6758\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7807.8046\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7944.0743\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7872.9150\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8014.2332\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 7926.8742\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7921.0067\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7939.6070\n",
            "mean,rmse,rmse/mean,bldg: 148.73384346526427 57.41778626244826 0.3860438547455258 Eagle_education_Teresa\n",
            "Example prediction:\n",
            " 0 [151 151 152]\n",
            "Example prediction:\n",
            " 2 [151 151 152]\n",
            "Example prediction:\n",
            " 4 [151 151 152]\n",
            "Example prediction:\n",
            " 6 [151 151 152]\n",
            "Example prediction:\n",
            " 8 [151 151 152]\n",
            "Example prediction:\n",
            " 10 [151 151 152]\n",
            "Example prediction:\n",
            " 12 [151 151 152]\n",
            "Example prediction:\n",
            " 14 [151 151 152]\n",
            "Example prediction:\n",
            " 16 [151 151 152]\n",
            "Example prediction:\n",
            " 18 [151 151 152]\n",
            "Example prediction:\n",
            " 20 [151 151 152]\n",
            "Example prediction:\n",
            " 22 [151 151 152]\n",
            "Building Eagle_office_Norbert\n",
            " Count bad values before pseudofill: 52\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_102 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_103 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_104 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [569 575 483]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 6ms/step - loss: 150786.6135\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 148133.0422\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 142388.3782\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 138776.7975\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 136042.5972\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 133602.2541\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 131520.9958\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 128442.0860\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 123312.9668\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 120802.7414\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 119507.6816\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 117431.7663\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 114263.7078\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 110643.7963\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 109279.5458\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 105431.1376\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 102226.8031\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 101888.5779\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 98153.9083\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 96464.8521\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 93153.2372\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 91348.8655\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 88818.9353\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 87595.3438\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 84518.9318\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 83416.6560\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 81123.6466\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 79516.2627\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 75542.7599\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 74753.4533\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 72521.2087\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 71220.9700\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 68350.0848\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 67864.2372\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 65417.3468\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 63605.6061\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 62188.6219\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 58677.7236\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 58197.4567\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 56460.3476\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 54333.0763\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 51850.7802\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 51788.2106\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 49376.3817\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 49453.6408\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 47034.0170\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 45017.1210\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 43263.0400\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 43033.8052\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 42052.6379\n",
            "mean,rmse,rmse/mean,bldg: 390.8100632860704 232.06496912052847 0.5938049986974323 Eagle_office_Norbert\n",
            "Example prediction:\n",
            " 0 [219 221 217]\n",
            "Example prediction:\n",
            " 2 [219 221 217]\n",
            "Example prediction:\n",
            " 4 [219 221 217]\n",
            "Example prediction:\n",
            " 6 [219 221 217]\n",
            "Example prediction:\n",
            " 8 [219 221 217]\n",
            "Example prediction:\n",
            " 10 [219 221 217]\n",
            "Example prediction:\n",
            " 12 [219 221 217]\n",
            "Example prediction:\n",
            " 14 [219 221 217]\n",
            "Example prediction:\n",
            " 16 [219 221 217]\n",
            "Example prediction:\n",
            " 18 [219 221 217]\n",
            "Example prediction:\n",
            " 20 [219 221 217]\n",
            "Example prediction:\n",
            " 22 [219 221 217]\n",
            "Building Eagle_lodging_Casey\n",
            "Building Eagle_office_Tia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_105 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_106 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_107 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [171 260 127]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 48080.3489\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 46243.0383\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 44489.0212\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 42882.1730\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 41270.9486\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 40168.6929\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 39343.3211\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 37119.6924\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 35654.8623\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 33489.5062\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 33520.7309\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 33657.9228\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 31229.8606\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 29794.8822\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 29317.4530\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 27928.4573\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 26737.7207\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 25435.0537\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 25073.9505\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 23835.2415\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23342.6973\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 22614.9580\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 21911.7147\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 21362.5917\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20993.0109\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 20189.2213\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 19434.8898\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18669.4882\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 18069.6397\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 17508.7473\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 16414.1480\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 16761.8511\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 16458.2675\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 16224.9981\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 15285.5383\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 14895.9399\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14946.6133\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 14581.6314\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14007.8181\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 14294.7436\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13799.4123\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 13439.4030\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 13424.1771\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11632.5531\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 11106.0376\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10728.3975\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9630.5523\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9501.0635\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9609.2652\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9414.4342\n",
            "mean,rmse,rmse/mean,bldg: 174.6359145783468 82.10853144404747 0.4701697909178422 Eagle_office_Tia\n",
            "Example prediction:\n",
            " 0 [196 196 196]\n",
            "Example prediction:\n",
            " 2 [196 196 196]\n",
            "Example prediction:\n",
            " 4 [196 196 196]\n",
            "Example prediction:\n",
            " 6 [196 196 196]\n",
            "Example prediction:\n",
            " 8 [196 196 196]\n",
            "Example prediction:\n",
            " 10 [196 196 196]\n",
            "Example prediction:\n",
            " 12 [196 196 196]\n",
            "Example prediction:\n",
            " 14 [196 196 196]\n",
            "Example prediction:\n",
            " 16 [195 196 196]\n",
            "Example prediction:\n",
            " 18 [196 196 196]\n",
            "Example prediction:\n",
            " 20 [196 196 196]\n",
            "Example prediction:\n",
            " 22 [196 196 196]\n",
            "Building Eagle_office_Remedios\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_108 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_109 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_110 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [189 194 161]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 6ms/step - loss: 13955.1444\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 12657.5743\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 11827.4064\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10794.1679\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10059.4664\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9514.8919\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8675.9513\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8288.8979\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7479.3012\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 6919.6073\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 6583.0560\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5908.1667\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5718.4041\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5107.5352\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4875.0112\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4510.7499\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 4154.7192\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3911.2165\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3653.4497\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3544.4699\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 3339.6309\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 3225.2293\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2969.5332\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2871.0636\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2811.0331\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2673.2945\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2598.9740\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2518.1996\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2524.1348\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 2483.0931\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 2505.8658\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 2529.1982\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1841.8236\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1620.5489\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1481.0169\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1358.3975\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1299.5147\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1276.8725\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1159.9759\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 1074.7102\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 1024.4885\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 977.7802\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 918.0130\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 843.3355\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 843.8772\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 788.2186\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 722.7183\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 733.9599\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 710.2624\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 720.8479\n",
            "mean,rmse,rmse/mean,bldg: 121.48132139556034 26.154087079788102 0.21529307369506379 Eagle_office_Remedios\n",
            "Example prediction:\n",
            " 0 [153 152 151]\n",
            "Example prediction:\n",
            " 2 [153 152 150]\n",
            "Example prediction:\n",
            " 4 [152 151 150]\n",
            "Example prediction:\n",
            " 6 [152 152 150]\n",
            "Example prediction:\n",
            " 8 [153 152 151]\n",
            "Example prediction:\n",
            " 10 [153 152 151]\n",
            "Example prediction:\n",
            " 12 [153 152 151]\n",
            "Example prediction:\n",
            " 14 [153 152 151]\n",
            "Example prediction:\n",
            " 16 [153 152 151]\n",
            "Example prediction:\n",
            " 18 [153 152 151]\n",
            "Example prediction:\n",
            " 20 [153 152 151]\n",
            "Example prediction:\n",
            " 22 [153 152 151]\n",
            "Building Eagle_office_Patrice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_111 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_112 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_113 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 6ms/step - loss: 33233.3871\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 31370.8970\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 30671.1786\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 29809.1674\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 27807.8582\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 26929.6922\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 25546.0332\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 25249.7394\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 23530.9219\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 23272.7076\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 23006.2229\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 21095.6415\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 20704.4375\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 20001.9871\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 20329.8766\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 18677.6913\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 18330.1055\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 17818.6249\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 17481.6177\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 16245.0257\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 15818.0504\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 15046.8379\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 15862.6767\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 14322.8551\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 13927.0973\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 13135.3791\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 12304.6667\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 12758.6675\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 12383.6197\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 12038.9368\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 11190.3991\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 11141.4429\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10444.4298\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10474.0740\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10418.1851\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 9823.7438\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9653.7166\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10102.1717\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9321.3143\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 8983.7108\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 9414.0322\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8370.0252\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9125.8342\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8231.1071\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8618.4247\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7770.5935\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7535.5951\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7221.9368\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 6970.8941\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 7029.8682\n",
            "mean,rmse,rmse/mean,bldg: 165.87984746703071 94.25234315157864 0.5681964662423004 Eagle_office_Patrice\n",
            "Example prediction:\n",
            " 0 [194 192 194]\n",
            "Example prediction:\n",
            " 2 [194 192 194]\n",
            "Example prediction:\n",
            " 4 [194 192 194]\n",
            "Example prediction:\n",
            " 6 [194 192 194]\n",
            "Example prediction:\n",
            " 8 [194 192 194]\n",
            "Example prediction:\n",
            " 10 [194 192 194]\n",
            "Example prediction:\n",
            " 12 [194 192 194]\n",
            "Example prediction:\n",
            " 14 [194 192 194]\n",
            "Example prediction:\n",
            " 16 [172 170 171]\n",
            "Example prediction:\n",
            " 18 [171 170 171]\n",
            "Example prediction:\n",
            " 20 [169 167 168]\n",
            "Example prediction:\n",
            " 22 [194 192 194]\n",
            "Building Eagle_education_Shana\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_114 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_115 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_116 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [187 188 131]\n",
            "Epoch 1/50\n",
            "271/271 [==============================] - 5s 5ms/step - loss: 12797.6162\n",
            "Epoch 2/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 11568.2476\n",
            "Epoch 3/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 10669.2616\n",
            "Epoch 4/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 9940.9469\n",
            "Epoch 5/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 9088.6522\n",
            "Epoch 6/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 8403.2735\n",
            "Epoch 7/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7728.8815\n",
            "Epoch 8/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 7152.6035\n",
            "Epoch 9/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 6645.5070\n",
            "Epoch 10/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 6101.1601\n",
            "Epoch 11/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 5663.2882\n",
            "Epoch 12/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 5132.7717\n",
            "Epoch 13/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4842.9460\n",
            "Epoch 14/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4456.4140\n",
            "Epoch 15/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 4125.8535\n",
            "Epoch 16/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 3760.4193\n",
            "Epoch 17/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 3563.5230\n",
            "Epoch 18/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 3279.3902\n",
            "Epoch 19/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 3108.7892\n",
            "Epoch 20/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 2808.9548\n",
            "Epoch 21/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2517.2227\n",
            "Epoch 22/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2219.9386\n",
            "Epoch 23/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 2065.4747\n",
            "Epoch 24/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1959.0142\n",
            "Epoch 25/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1854.4961\n",
            "Epoch 26/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1728.5880\n",
            "Epoch 27/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1685.2868\n",
            "Epoch 28/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 1583.2070\n",
            "Epoch 29/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1467.7040\n",
            "Epoch 30/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1362.5854\n",
            "Epoch 31/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1220.9196\n",
            "Epoch 32/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 1119.6525\n",
            "Epoch 33/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 1009.1055\n",
            "Epoch 34/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 924.0743\n",
            "Epoch 35/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 881.7134\n",
            "Epoch 36/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 790.6519\n",
            "Epoch 37/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 775.3705\n",
            "Epoch 38/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 718.6817\n",
            "Epoch 39/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 679.1865\n",
            "Epoch 40/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 623.8998\n",
            "Epoch 41/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 637.6195\n",
            "Epoch 42/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 604.5089\n",
            "Epoch 43/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 598.3012\n",
            "Epoch 44/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 574.6931\n",
            "Epoch 45/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 543.3595\n",
            "Epoch 46/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 553.1813\n",
            "Epoch 47/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 537.9434\n",
            "Epoch 48/50\n",
            "271/271 [==============================] - 1s 5ms/step - loss: 518.9400\n",
            "Epoch 49/50\n",
            "271/271 [==============================] - 2s 6ms/step - loss: 537.9118\n",
            "Epoch 50/50\n",
            "271/271 [==============================] - 1s 6ms/step - loss: 511.7088\n",
            "mean,rmse,rmse/mean,bldg: 103.18172944172048 25.1182750880496 0.24343723664989553 Eagle_education_Shana\n",
            "Example prediction:\n",
            " 0 [154 154 152]\n",
            "Example prediction:\n",
            " 2 [154 154 152]\n",
            "Example prediction:\n",
            " 4 [154 154 152]\n",
            "Example prediction:\n",
            " 6 [154 154 152]\n",
            "Example prediction:\n",
            " 8 [154 154 152]\n",
            "Example prediction:\n",
            " 10 [154 154 152]\n",
            "Example prediction:\n",
            " 12 [154 154 152]\n",
            "Example prediction:\n",
            " 14 [154 154 152]\n",
            "Example prediction:\n",
            " 16 [154 154 152]\n",
            "Example prediction:\n",
            " 18 [154 154 152]\n",
            "Example prediction:\n",
            " 20 [154 154 152]\n",
            "Example prediction:\n",
            " 22 [154 154 152]\n",
            "\n",
            "History 7 Future 3\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      0.00       0.00   inf   Eagle_office_Henriette\n",
            "      0.11       0.03  0.29   Eagle_education_Wesley\n",
            "     15.76      44.99  2.85   Eagle_education_Jewell\n",
            "     35.89      10.31  0.29   Eagle_office_Mandi\n",
            "     36.93      12.15  0.33   Eagle_office_Lamont\n",
            "     43.43      42.52  0.98   Eagle_lodging_Blake\n",
            "     46.46      17.40  0.37   Eagle_education_Eileen\n",
            "     56.50      54.08  0.96   Eagle_office_Dallas\n",
            "     57.04      27.76  0.49   Eagle_education_Petra\n",
            "     62.02      39.19  0.63   Eagle_office_Sheree\n",
            "     81.96      45.68  0.56   Eagle_lodging_Edgardo\n",
            "     87.21      20.89  0.24   Eagle_office_Phyllis\n",
            "     91.27      54.97  0.60   Eagle_lodging_Trina\n",
            "     92.81      46.74  0.50   Eagle_lodging_Dawn\n",
            "    101.56      48.57  0.48   Eagle_office_Freida\n",
            "    103.18      25.12  0.24   Eagle_education_Shana\n",
            "    121.48      26.15  0.22   Eagle_office_Remedios\n",
            "    122.36      40.00  0.33   Eagle_health_Vincenza\n",
            "    148.73      57.42  0.39   Eagle_education_Teresa\n",
            "    165.88      94.25  0.57   Eagle_office_Patrice\n",
            "    174.64      82.11  0.47   Eagle_office_Tia\n",
            "    182.06      75.89  0.42   Eagle_public_Alvin\n",
            "    192.42      85.95  0.45   Eagle_health_Jodi\n",
            "    226.17      51.35  0.23   Eagle_education_Will\n",
            "    273.02     132.89  0.49   Eagle_office_Nereida\n",
            "    276.27     176.58  0.64   Eagle_food_Kay\n",
            "    336.49     202.54  0.60   Eagle_office_Francis\n",
            "    390.81     232.06  0.59   Eagle_office_Norbert\n",
            "    477.67     313.05  0.66   Eagle_health_Athena\n",
            "    659.51     756.35  1.15   Eagle_health_Gregoria\n",
            "    694.96     523.74  0.75   Eagle_education_Alberto\n",
            "    712.01     653.88  0.92   Eagle_education_Norah\n",
            "   1003.99    1847.50  1.84   Eagle_education_Shante\n",
            "   1037.64     935.49  0.90   Eagle_office_Chauncey\n",
            "   1084.43     910.79  0.84   Eagle_health_Reba\n",
            "   1199.38    1061.95  0.89   Eagle_education_Roman\n",
            "   1639.36    1618.20  0.99   Eagle_education_Brooke\n",
            "   2032.48    2049.42  1.01   Eagle_education_Sherrill\n",
            "   3154.83    3007.11  0.95   Eagle_education_Peter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}