{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_224.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# RNN \n",
        "Use non-consecutive predictors. Skip = 0. Use 7 predictors (past 7 hr) to predict next 3 (3 hr). Smoothing window size 3. Predict steam given steam and day of year. Compare to RNN 223, 225."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "b6e034fe-970b-438d-fbf8-b251f6ad8b2f"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Eagle'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "PREDICTORS = ['doy','meter'] # short list for testing\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_SKIP = 0\n",
        "STEPS_FORWARD = 7 \n",
        "STEPS_FUTURE =  3 \n",
        "METER_FILE='steam.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 2 ['doy', 'meter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "1f3481ec-63a9-4722-fadc-b0015d4f5783"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "7035ac78-cf83-4196-d03e-341f6834dce9"
      },
      "source": [
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    site_df.insert(0,'hour',0)\n",
        "    site_df.insert(1,'month',0)\n",
        "    site_df.insert(2,'doy',0)\n",
        "    L=len(site_df)\n",
        "    for i in range(0,L):\n",
        "        dt=site_df.index[i]\n",
        "        hour=dt.hour\n",
        "        month=dt.month\n",
        "        doy=dt.dayofyear\n",
        "        site_df.iat[i,0] = hour\n",
        "        site_df.iat[i,1] = month\n",
        "        site_df.iat[i,2] = doy\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 18:00:00</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1026.2</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.2</td>\n",
              "      <td>310.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>330.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>320.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 18:00:00    18     12  365  ...          1026.2          330.0        2.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1027.0          320.0        1.5\n",
              "2017-12-31 21:00:00    21     12  365  ...          1027.2          310.0        2.6\n",
              "2017-12-31 22:00:00    22     12  365  ...          1027.4          330.0        3.1\n",
              "2017-12-31 23:00:00    23     12  365  ...          1027.4          320.0        4.6\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "6b328e51-c064-4bcb-faee-994771f9904b"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>92.2957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>277.5584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>280.5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>289.3302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>164.3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        meter\n",
              "timestamp                    \n",
              "2017-12-31 19:00:00   92.2957\n",
              "2017-12-31 20:00:00  277.5584\n",
              "2017-12-31 21:00:00  280.5331\n",
              "2017-12-31 22:00:00  289.3302\n",
              "2017-12-31 23:00:00  164.3474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "4318397a-0a8a-43f8-a29f-f73b3b939128"
      },
      "source": [
        "# This is the first version that uses STEPS_SKIP.\n",
        "# To retrofit this to older notebooks, set STEPS_SKIP=0.\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    num_samples = len(df) - STEPS_FORWARD*STEPS_SKIP - STEPS_FUTURE*STEPS_SKIP\n",
        "    X_shape = (num_samples,STEPS_FORWARD,NUM_PREDICTORS)\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = df[PREDICTED_VARIABLE].values  # meter\n",
        "    # TO DO: can we take predicted from mdf instead?\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = predictor_series[sam+time*STEPS_SKIP]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat] = val\n",
        "        for time1 in range (STEPS_FORWARD,STEPS_FORWARD+STEPS_FUTURE): \n",
        "            time0 = time1 - STEPS_FORWARD\n",
        "            y[sam,time0]=predicted_series[sam+time1*STEPS_SKIP]\n",
        "    return X,y \n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (17544, 7, 2)\n",
            "y shape: (17544, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObWmpMDVuNQ",
        "outputId": "18b5af3d-756d-4ac0-ba82-7674f6860407"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['doy', 'meter']\n",
            "X example:\n",
            " [[  5 232]\n",
            " [  5 232]\n",
            " [  5 232]\n",
            " [  5 232]\n",
            " [  5 232]\n",
            " [  5 232]\n",
            " [  5 232]]\n",
            "y example:\n",
            " [232 232 232]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_RNN():\n",
        "    # The GRU in Keras is optimized for speed on CoLab GPU.\n",
        "    rnn = Sequential([\n",
        "        GRU(16,return_sequences=True, \n",
        "                  input_shape=(STEPS_FORWARD,NUM_PREDICTORS)), \n",
        "        GRU(16,return_sequences=True),\n",
        "        GRU(16,return_sequences=False),\n",
        "        Dense(STEPS_FUTURE)\n",
        "    ])\n",
        "    rnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    return rnn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "dcddf093-b254-4f3c-ec09-47097f989dc4"
      },
      "source": [
        "cors = []\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",BLDG)\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # \n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        y_test = np.asarray(y[split:])\n",
        "        model = make_RNN()\n",
        "        print(model.summary())\n",
        "        #print(\"Example X train:\\n\",X_train[example].astype(int))\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        # Keep a table for reporting later.\n",
        "        y_pred = model.predict(X_test)\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        print(\"mean,rmse,rmse/mean,bldg:\",mean,rmse,rmse/mean,BLDG)\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n",
        "print()\n",
        "print(\"History\",STEPS_FORWARD,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Eagle_office_Lamont\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [42 42 42]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 21s 5ms/step - loss: 1169.6221\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 830.8757\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 628.9882\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 480.7976\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 368.8626\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 294.3310\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 227.3455\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 179.7112\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 132.7912\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 101.9088\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 80.7689\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 61.0452\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 47.4043\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 39.4929\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 30.6963\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 24.2741\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 21.1642\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 17.8217\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 15.3657\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11.0201\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8.9207\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6.9466\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.9554\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.9087\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3.8535\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3.1639\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.4502\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.9775\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.5925\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.3034\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.0915\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.9822\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.7384\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.6593\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.5363\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.3678\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.4046\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.3615\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.3165\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.2216\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1552\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1411\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1504\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1105\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0776\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0860\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0652\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0880\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0389\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0391\n",
            "mean,rmse,rmse/mean,bldg: 36.93460755405991 0.07848015418604323 0.0021248406138111675 Eagle_office_Lamont\n",
            "Example prediction:\n",
            " 0 [45 45 45]\n",
            "Example prediction:\n",
            " 2 [41 41 41]\n",
            "Example prediction:\n",
            " 4 [41 41 41]\n",
            "Example prediction:\n",
            " 6 [41 41 41]\n",
            "Example prediction:\n",
            " 8 [43 43 43]\n",
            "Example prediction:\n",
            " 10 [42 42 42]\n",
            "Example prediction:\n",
            " 12 [40 40 40]\n",
            "Example prediction:\n",
            " 14 [42 42 42]\n",
            "Example prediction:\n",
            " 16 [39 39 39]\n",
            "Example prediction:\n",
            " 18 [47 47 47]\n",
            "Example prediction:\n",
            " 20 [46 46 46]\n",
            "Example prediction:\n",
            " 22 [43 43 43]\n",
            "Building Eagle_health_Athena\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_3 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1287 1287 1287]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 349866.3339\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 337270.0667\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 340439.5399\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 333204.5768\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 332276.3109\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 333695.1164\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 324687.9899\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 316102.7671\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 310271.3721\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 303359.9561\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 300939.6758\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 303980.0069\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 293380.7101\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 286004.6432\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 283407.6760\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 276609.8144\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 275230.7405\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 276696.5855\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 271272.3153\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 263506.2066\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 257966.1153\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 256368.9651\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 248879.0858\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 252539.7769\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 248506.1997\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 245615.1596\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 242248.1750\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 232925.0696\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 230993.8345\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 224730.0823\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 228429.0221\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 222701.6275\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 213198.2465\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 212247.8252\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 213830.3041\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 205575.8982\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 204280.7231\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 203153.4293\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 197595.2749\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 191912.0692\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 189768.9171\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 186723.6510\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 185764.0348\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 185620.3195\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 175750.8834\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 177893.4165\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 169695.7968\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 170580.8306\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 167824.2141\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 173947.5336\n",
            "mean,rmse,rmse/mean,bldg: 477.6710835157538 311.48576830908235 0.6520925780486551 Eagle_health_Athena\n",
            "Example prediction:\n",
            " 0 [227 227 228]\n",
            "Example prediction:\n",
            " 2 [227 227 228]\n",
            "Example prediction:\n",
            " 4 [227 227 228]\n",
            "Example prediction:\n",
            " 6 [227 227 228]\n",
            "Example prediction:\n",
            " 8 [227 227 228]\n",
            "Example prediction:\n",
            " 10 [227 227 228]\n",
            "Example prediction:\n",
            " 12 [227 227 228]\n",
            "Example prediction:\n",
            " 14 [227 227 228]\n",
            "Example prediction:\n",
            " 16 [227 227 228]\n",
            "Example prediction:\n",
            " 18 [227 227 228]\n",
            "Example prediction:\n",
            " 20 [227 227 228]\n",
            "Example prediction:\n",
            " 22 [227 227 228]\n",
            "Building Eagle_assembly_Herbert\n",
            "Building Eagle_public_Alvin\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_6 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [460 460 460]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 56807.4727\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 52491.0130\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 51675.3634\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 50361.2871\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 46759.1438\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 47405.4905\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 42913.1631\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 42170.0583\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 41803.6057\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 40643.4339\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 38598.1069\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 37104.3839\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 35556.7616\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 34798.4763\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 33141.1977\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 31792.6881\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 30419.8939\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 30566.2606\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 29296.6192\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 27777.7928\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 27149.5241\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 25636.7646\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 24876.2336\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24057.0195\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 23775.1564\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 21763.9686\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 21265.1533\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 20354.7964\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 19951.8423\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 19847.3585\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 19117.7878\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 18628.0934\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 17437.1913\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 16577.5011\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 16175.7218\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 15644.1359\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 14641.5719\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 14461.4213\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13682.3377\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13393.2247\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13122.7520\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12667.9024\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11441.3867\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10910.5262\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10843.6150\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10519.8849\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10508.5578\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9736.3210\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9394.0931\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9246.4505\n",
            "mean,rmse,rmse/mean,bldg: 182.0635582126762 34.275615509588754 0.18826181277611825 Eagle_public_Alvin\n",
            "Example prediction:\n",
            " 0 [211 209 208]\n",
            "Example prediction:\n",
            " 2 [211 209 208]\n",
            "Example prediction:\n",
            " 4 [211 209 208]\n",
            "Example prediction:\n",
            " 6 [146 145 145]\n",
            "Example prediction:\n",
            " 8 [156 155 155]\n",
            "Example prediction:\n",
            " 10 [166 165 165]\n",
            "Example prediction:\n",
            " 12 [172 171 171]\n",
            "Example prediction:\n",
            " 14 [148 148 148]\n",
            "Example prediction:\n",
            " 16 [137 136 136]\n",
            "Example prediction:\n",
            " 18 [130 130 129]\n",
            "Example prediction:\n",
            " 20 [104 104 103]\n",
            "Example prediction:\n",
            " 22 [139 138 138]\n",
            "Building Eagle_education_Raul\n",
            "Building Eagle_education_Roman\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_9 (GRU)                  (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1612 1612 1612]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 1702897.5983\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1678145.7572\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1681670.2043\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1706573.6436\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1645001.4443\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1649590.2183\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1642605.4891\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1600227.2563\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1599680.3596\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1590278.4325\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1553012.1907\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1594112.9044\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1576324.7726\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1569672.6735\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1557307.3003\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1552265.6843\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1535263.3397\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1515471.0439\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1546838.7047\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1492526.6920\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1465839.5897\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1471235.1744\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1471320.9882\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1454094.3302\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1457605.1748\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1450561.5557\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1423859.5045\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1418249.5611\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1426859.9244\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1429162.2038\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1393867.2237\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1362857.6273\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1387254.3813\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1349817.0747\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1354069.6091\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1354047.6069\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1351105.2763\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1342243.7595\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1315608.4178\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1297742.7525\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1295592.1626\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1263401.8220\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1269478.0154\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1256144.1658\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1268470.7024\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1247269.7790\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1235189.1923\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1231050.7880\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1183557.5213\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1225943.0190\n",
            "mean,rmse,rmse/mean,bldg: 1199.3775815637814 1069.739732825571 0.8919123962870934 Eagle_education_Roman\n",
            "Example prediction:\n",
            " 0 [233 233 233]\n",
            "Example prediction:\n",
            " 2 [233 233 233]\n",
            "Example prediction:\n",
            " 4 [233 233 233]\n",
            "Example prediction:\n",
            " 6 [233 233 233]\n",
            "Example prediction:\n",
            " 8 [233 233 233]\n",
            "Example prediction:\n",
            " 10 [233 233 233]\n",
            "Example prediction:\n",
            " 12 [233 233 233]\n",
            "Example prediction:\n",
            " 14 [233 233 233]\n",
            "Example prediction:\n",
            " 16 [233 233 233]\n",
            "Example prediction:\n",
            " 18 [233 233 233]\n",
            "Example prediction:\n",
            " 20 [233 233 233]\n",
            "Example prediction:\n",
            " 22 [233 233 233]\n",
            "Building Eagle_office_Mandi\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_12 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [64 64 64]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 1479.9860\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1120.8375\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 888.9054\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 687.1899\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 543.9295\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 439.1323\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 335.7398\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 274.8284\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 220.6295\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 186.9385\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 146.0608\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 123.1083\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 98.3886\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 78.9982\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 62.5461\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 50.9898\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 40.8214\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 29.7990\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 21.2197\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 15.8483\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11.7024\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9.0598\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5.7822\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3.8307\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.6958\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.6563\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.1320\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.6640\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.4462\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.5105\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.2715\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.3632\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1558\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.3176\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1241\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1373\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1092\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0941\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0825\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0991\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1582\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0856\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1135\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0706\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0899\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0854\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0837\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.1112\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0747\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 0.0748\n",
            "mean,rmse,rmse/mean,bldg: 35.89274527609939 0.30782167413328326 0.008576152973683474 Eagle_office_Mandi\n",
            "Example prediction:\n",
            " 0 [60 59 59]\n",
            "Example prediction:\n",
            " 2 [59 58 58]\n",
            "Example prediction:\n",
            " 4 [53 53 53]\n",
            "Example prediction:\n",
            " 6 [39 39 39]\n",
            "Example prediction:\n",
            " 8 [34 34 34]\n",
            "Example prediction:\n",
            " 10 [37 37 37]\n",
            "Example prediction:\n",
            " 12 [37 37 37]\n",
            "Example prediction:\n",
            " 14 [36 36 36]\n",
            "Example prediction:\n",
            " 16 [38 38 38]\n",
            "Example prediction:\n",
            " 18 [34 33 34]\n",
            "Example prediction:\n",
            " 20 [38 38 38]\n",
            "Example prediction:\n",
            " 22 [39 39 39]\n",
            "Building Eagle_education_Jewell\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_15 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_17 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [97 97 97]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 4280.5882\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3677.4355\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3611.8051\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3344.5682\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3107.3168\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2999.2950\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2808.2019\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2726.6909\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2526.8000\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2384.5995\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2164.3211\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2072.8715\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1968.8821\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1829.8914\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1644.1538\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1578.6230\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1538.4105\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1401.1376\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1264.0687\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1228.1460\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1117.8428\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1019.4570\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 932.8548\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 880.0678\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 817.5367\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 761.7118\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 704.1228\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 654.0311\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 624.7547\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 523.7671\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 449.7273\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 433.5493\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 380.8486\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 376.4180\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 307.7284\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 288.3673\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 272.3533\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 235.6122\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 227.4124\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 203.5119\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 180.3512\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 173.9377\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 137.9291\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 135.6423\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 110.4769\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 106.8657\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 91.4814\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 83.1202\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 75.8320\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 61.8692\n",
            "mean,rmse,rmse/mean,bldg: 15.763918737885179 0.2956381243721011 0.018754101013068444 Eagle_education_Jewell\n",
            "Example prediction:\n",
            " 0 [0 0 0]\n",
            "Example prediction:\n",
            " 2 [21 21 21]\n",
            "Example prediction:\n",
            " 4 [0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0]\n",
            "Building Eagle_office_Henriette\n",
            " Count bad values before pseudofill: 162\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_18 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_19 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_20 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 0.0552\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.3769e-05\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.7651e-06\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.2811e-06\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.1102e-06\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6.9692e-07\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.0386e-07\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3.0728e-07\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.2305e-07\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.0383e-07\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.3597e-07\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.4246e-07\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.0487e-07\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8.1352e-08\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8.3619e-08\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8.0257e-08\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8.2644e-08\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7.6718e-07\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.4196e-07\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.7416e-06\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9.7739e-07\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.5534e-06\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.2899e-06\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 4.1831e-06\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 5.4539e-07\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.1866e-06\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4.3487e-08\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.7551e-06\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.4585e-06\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7.9747e-07\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.7874e-06\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7.3256e-07\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.7158e-06\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.4423e-06\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.3949e-06\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7.5380e-06\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7.0595e-08\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9.0407e-07\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.0611e-06\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.5042e-06\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.2146e-06\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 2.5171e-06\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.6406e-06\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3.2669e-07\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4.2484e-07\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.4804e-06\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9.4988e-07\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1.1656e-06\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.0309e-06\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.4245e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean,rmse,rmse/mean,bldg: 0.0 0.00011003479710501037 inf Eagle_office_Henriette\n",
            "Example prediction:\n",
            " 0 [0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0]\n",
            "Building Eagle_health_Reba\n",
            " Count bad values before pseudofill: 36\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_21 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_22 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_23 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1668 1668 1668]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 1481009.7079\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1446714.4909\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1436445.6264\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1429980.9588\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1395945.7070\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1396414.1875\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1377835.9325\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1342223.0548\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1384758.9352\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1363216.9443\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1350155.6522\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1324036.7301\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1309886.9914\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1313284.1531\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1301598.8596\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1290435.4751\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1306031.8872\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1283762.7609\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1236325.6404\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1264714.6282\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1250661.6957\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1255029.6997\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1224624.6544\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1210916.8084\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1220666.5584\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1181635.0283\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1203004.8483\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1194852.1771\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1185584.9683\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1173921.1744\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1133617.8019\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1144174.2926\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1130538.3619\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1146468.0430\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1120312.0906\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1122751.5512\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1100497.3456\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1080603.9615\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1082854.2769\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1070503.0052\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1055295.5125\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1062866.5324\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1037397.6762\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1055992.0824\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1019208.0505\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1018547.2264\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 995821.9144\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1013152.5288\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 988138.3942\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1012183.8028\n",
            "mean,rmse,rmse/mean,bldg: 1084.4328846508208 924.3705376625381 0.8523999509293546 Eagle_health_Reba\n",
            "Example prediction:\n",
            " 0 [229 234 235]\n",
            "Example prediction:\n",
            " 2 [229 234 235]\n",
            "Example prediction:\n",
            " 4 [229 234 235]\n",
            "Example prediction:\n",
            " 6 [229 234 235]\n",
            "Example prediction:\n",
            " 8 [229 234 235]\n",
            "Example prediction:\n",
            " 10 [229 234 235]\n",
            "Example prediction:\n",
            " 12 [229 234 235]\n",
            "Example prediction:\n",
            " 14 [229 234 235]\n",
            "Example prediction:\n",
            " 16 [229 234 235]\n",
            "Example prediction:\n",
            " 18 [229 234 235]\n",
            "Example prediction:\n",
            " 20 [229 234 235]\n",
            "Example prediction:\n",
            " 22 [229 234 235]\n",
            "Building Eagle_lodging_Edgardo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_24 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_25 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_26 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [143 143 143]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 9554.5220\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8936.0365\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8397.3605\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7729.7540\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7195.4982\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6430.6215\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6173.8407\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5745.0332\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5392.1092\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4806.5978\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4788.8399\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4467.1605\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4457.0959\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3805.2376\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3574.1330\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3407.0155\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3218.7691\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3003.6304\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2829.1328\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2660.8592\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2298.4445\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2369.2067\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2178.1331\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2018.3933\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1838.9361\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1777.2191\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1651.8821\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1529.7558\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1362.1269\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1299.8531\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1234.4462\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1102.6928\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1092.1649\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1038.1376\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 969.9772\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 877.2541\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 817.8303\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 696.4695\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 699.7094\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 668.6373\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 634.3533\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 571.2340\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 500.4851\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 465.5712\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 428.8415\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 407.4533\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 345.0196\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 360.4160\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 387.9593\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 318.0443\n",
            "mean,rmse,rmse/mean,bldg: 81.9636656538589 19.787695813704257 0.2414203373635064 Eagle_lodging_Edgardo\n",
            "Example prediction:\n",
            " 0 [118 118 118]\n",
            "Example prediction:\n",
            " 2 [92 93 93]\n",
            "Example prediction:\n",
            " 4 [87 87 87]\n",
            "Example prediction:\n",
            " 6 [87 87 87]\n",
            "Example prediction:\n",
            " 8 [95 95 95]\n",
            "Example prediction:\n",
            " 10 [160 159 159]\n",
            "Example prediction:\n",
            " 12 [146 145 145]\n",
            "Example prediction:\n",
            " 14 [124 124 124]\n",
            "Example prediction:\n",
            " 16 [77 77 78]\n",
            "Example prediction:\n",
            " 18 [57 57 57]\n",
            "Example prediction:\n",
            " 20 [76 76 76]\n",
            "Example prediction:\n",
            " 22 [69 69 69]\n",
            "Building Eagle_education_Cassie\n",
            "Building Eagle_education_Peter\n",
            " Count bad values before pseudofill: 34\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_27 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_28 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_29 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4886 4886 4886]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 13214136.9348\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12696729.8406\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13146160.2391\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12973949.2391\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12886048.0399\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12855143.0181\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12620406.8841\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12746752.1268\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12537006.1304\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12803491.9239\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12666059.3080\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12698692.2210\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12716404.9348\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12615894.2428\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12579632.7790\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12402925.9058\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12646684.1304\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12673873.5797\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12572692.4565\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12308353.6341\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12430532.6449\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12374417.9312\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12280748.8007\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12482252.9384\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12244418.3659\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12056580.6051\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11998655.6757\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12272663.9565\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12170158.0507\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12200462.2935\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12195336.0435\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12053268.3043\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12106917.1341\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11904047.8116\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11800997.0797\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11983733.6341\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12027106.7609\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11703594.3804\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11988960.8116\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11889744.2899\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11882469.7645\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11734526.2790\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11736762.6848\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11781622.8297\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11813185.5725\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11548860.0254\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11713530.1196\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11475558.4094\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11610309.0942\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11470970.7572\n",
            "mean,rmse,rmse/mean,bldg: 3154.8298985018228 3038.8623106435693 0.9632412549680335 Eagle_education_Peter\n",
            "Example prediction:\n",
            " 0 [233 236 235]\n",
            "Example prediction:\n",
            " 2 [233 236 235]\n",
            "Example prediction:\n",
            " 4 [233 236 235]\n",
            "Example prediction:\n",
            " 6 [233 236 235]\n",
            "Example prediction:\n",
            " 8 [233 236 235]\n",
            "Example prediction:\n",
            " 10 [233 236 235]\n",
            "Example prediction:\n",
            " 12 [233 236 235]\n",
            "Example prediction:\n",
            " 14 [233 236 235]\n",
            "Example prediction:\n",
            " 16 [233 236 235]\n",
            "Example prediction:\n",
            " 18 [233 236 235]\n",
            "Example prediction:\n",
            " 20 [233 236 235]\n",
            "Example prediction:\n",
            " 22 [233 236 235]\n",
            "Building Eagle_health_Gregoria\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_30 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_31 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_32 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 673550.5233\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 627905.2405\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 669882.7036\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 665691.7844\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 662514.4257\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 670153.7640\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 657822.3793\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 665081.9962\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 668326.6372\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 641918.0258\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 613301.7035\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 608691.9429\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 640091.0102\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 605921.0682\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 624607.0833\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 591816.6182\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 622681.3675\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 614897.1846\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 578965.6175\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 579665.9596\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 611561.1517\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 601608.9766\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 556160.1281\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 562368.0896\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 565589.5299\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 576452.4695\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 570478.0348\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 560213.4135\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 557209.7270\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 531556.3599\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 549827.0489\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 550897.8859\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 512937.9906\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 536625.7421\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 511809.3593\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 553473.0673\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 492119.3860\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 500805.2116\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 539532.1692\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 517064.2920\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 492084.9305\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 532957.6942\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 499165.1935\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 526083.1518\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 490326.3341\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 499665.1231\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 473836.0876\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 470698.2540\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 480052.0964\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 457091.3536\n",
            "mean,rmse,rmse/mean,bldg: 659.5054129061737 757.3400494619063 1.148345464102584 Eagle_health_Gregoria\n",
            "Example prediction:\n",
            " 0 [225 224 225]\n",
            "Example prediction:\n",
            " 2 [225 224 225]\n",
            "Example prediction:\n",
            " 4 [225 224 225]\n",
            "Example prediction:\n",
            " 6 [225 224 225]\n",
            "Example prediction:\n",
            " 8 [225 224 225]\n",
            "Example prediction:\n",
            " 10 [225 224 225]\n",
            "Example prediction:\n",
            " 12 [225 224 225]\n",
            "Example prediction:\n",
            " 14 [225 224 225]\n",
            "Example prediction:\n",
            " 16 [225 224 225]\n",
            "Example prediction:\n",
            " 18 [225 224 225]\n",
            "Example prediction:\n",
            " 20 [225 224 225]\n",
            "Example prediction:\n",
            " 22 [225 224 225]\n",
            "Building Eagle_lodging_Dawn\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_33 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_34 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_35 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [225 225 225]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 13634.3620\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12102.7941\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11711.7999\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10480.7437\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9772.1622\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 9155.3547\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8613.4552\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 8308.7954\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7797.8877\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 6940.2064\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6523.0666\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 6230.9006\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5776.2479\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5560.3141\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5192.0562\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 4874.3770\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4456.0647\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 4242.9341\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3865.7744\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3848.3906\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3675.1101\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3085.9086\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 3391.5521\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2962.4670\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2905.8885\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2574.9909\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2451.9205\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2287.5958\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2205.7263\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2042.8377\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1936.2452\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1765.6451\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1818.8607\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1556.7719\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1643.5927\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1497.4806\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1280.9574\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1268.6867\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1127.8875\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1086.9475\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1066.0012\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 981.1437\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 959.4211\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 896.1749\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 846.1936\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 697.8122\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 744.3985\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 667.9890\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 617.7239\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 568.7184\n",
            "mean,rmse,rmse/mean,bldg: 92.8091523125402 18.281408022360907 0.19697850445608214 Eagle_lodging_Dawn\n",
            "Example prediction:\n",
            " 0 [155 155 155]\n",
            "Example prediction:\n",
            " 2 [119 118 118]\n",
            "Example prediction:\n",
            " 4 [138 138 138]\n",
            "Example prediction:\n",
            " 6 [119 118 118]\n",
            "Example prediction:\n",
            " 8 [104 104 104]\n",
            "Example prediction:\n",
            " 10 [96 96 96]\n",
            "Example prediction:\n",
            " 12 [104 104 104]\n",
            "Example prediction:\n",
            " 14 [119 118 118]\n",
            "Example prediction:\n",
            " 16 [107 106 106]\n",
            "Example prediction:\n",
            " 18 [109 109 109]\n",
            "Example prediction:\n",
            " 20 [94 94 94]\n",
            "Example prediction:\n",
            " 22 [96 96 96]\n",
            "Building Eagle_office_Nereida\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_36 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_37 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_38 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [314 314 314]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 71292.2432\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 68944.8061\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 68039.2276\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 65499.7602\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 63254.5619\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 61026.8625\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 58970.2892\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 57984.2529\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 55465.8304\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 52763.6945\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 51739.0632\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 50054.2708\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 48254.3902\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 45781.7013\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 44827.2860\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 43294.5296\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 42401.9888\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 40821.5754\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 39449.1379\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 37747.2010\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 36371.2285\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 34256.7276\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 32759.2963\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 31536.1017\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 31009.6630\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 29558.7861\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 28418.8492\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 27280.2329\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 25902.8164\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 25092.5078\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 23513.7424\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 23035.6331\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 21570.9860\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 20814.2555\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 20234.2649\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 19373.8592\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 18282.3431\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17848.3977\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 16897.7115\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 16079.0805\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15563.6600\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14979.5412\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13784.4076\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 13373.8851\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12694.2555\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12257.8282\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11568.4628\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 11079.1848\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10759.0892\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9685.0081\n",
            "mean,rmse,rmse/mean,bldg: 273.01729748007574 116.40415052269331 0.4263618151563758 Eagle_office_Nereida\n",
            "Example prediction:\n",
            " 0 [215 213 214]\n",
            "Example prediction:\n",
            " 2 [215 213 214]\n",
            "Example prediction:\n",
            " 4 [215 213 214]\n",
            "Example prediction:\n",
            " 6 [215 213 214]\n",
            "Example prediction:\n",
            " 8 [215 213 214]\n",
            "Example prediction:\n",
            " 10 [215 213 214]\n",
            "Example prediction:\n",
            " 12 [215 213 214]\n",
            "Example prediction:\n",
            " 14 [215 213 214]\n",
            "Example prediction:\n",
            " 16 [215 213 214]\n",
            "Example prediction:\n",
            " 18 [215 213 214]\n",
            "Example prediction:\n",
            " 20 [215 213 214]\n",
            "Example prediction:\n",
            " 22 [215 213 214]\n",
            "Building Eagle_lodging_Tressa\n",
            "Building Eagle_education_Eileen\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_39 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_40 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_41 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [96 96 96]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 5ms/step - loss: 2399.1305\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1982.3349\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1655.4906\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1444.8683\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1200.5816\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 1019.4830\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 869.4446\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 769.4718\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 669.7210\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 570.8278\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 524.3925\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 448.1977\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 388.7219\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 381.0421\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 330.3048\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 274.9636\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 272.9855\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 263.3139\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 227.6515\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 200.8576\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 183.3054\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 168.0829\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 137.1693\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 124.8312\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 111.1910\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 92.7995\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 94.1570\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 77.4889\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 74.7608\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 63.6306\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 54.8449\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 46.4411\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 45.0427\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 34.6129\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 39.4895\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 31.0822\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 31.9427\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 28.2374\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 22.3324\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 19.4515\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 20.2618\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 14.1119\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19.8691\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11.4282\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 10.5561\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11.6077\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11.2591\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 8.5375\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 6.5131\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 7.7781\n",
            "mean,rmse,rmse/mean,bldg: 46.462781809371094 1.867106613580852 0.04018499411510214 Eagle_education_Eileen\n",
            "Example prediction:\n",
            " 0 [85 85 85]\n",
            "Example prediction:\n",
            " 2 [88 88 88]\n",
            "Example prediction:\n",
            " 4 [67 67 67]\n",
            "Example prediction:\n",
            " 6 [31 31 31]\n",
            "Example prediction:\n",
            " 8 [33 33 33]\n",
            "Example prediction:\n",
            " 10 [33 33 33]\n",
            "Example prediction:\n",
            " 12 [35 34 34]\n",
            "Example prediction:\n",
            " 14 [36 35 35]\n",
            "Example prediction:\n",
            " 16 [21 21 21]\n",
            "Example prediction:\n",
            " 18 [28 28 28]\n",
            "Example prediction:\n",
            " 20 [49 49 49]\n",
            "Example prediction:\n",
            " 22 [52 52 52]\n",
            "Building Eagle_education_Wesley\n",
            " Count bad values before pseudofill: 112\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_42 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_43 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_44 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 0.0301\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.0354e-04\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.0842e-04\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.0434e-04\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.0575e-04\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.1430e-04\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.0760e-04\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.1236e-04\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 5.1246e-04\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.1313e-04\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5.1593e-04\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4.4834e-04\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.9660e-04\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.7199e-04\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.6347e-04\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.0027e-04\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6.5976e-05\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.1395e-05\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.9977e-05\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.4790e-05\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.2086e-05\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.3737e-05\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 2.4096e-05\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.4748e-05\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.3507e-05\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.4524e-05\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.8138e-05\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.2924e-05\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.0617e-05\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.1255e-05\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4.2881e-05\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.8325e-05\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.5876e-05\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.3674e-05\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.5509e-05\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.9204e-05\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.7315e-05\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.8832e-05\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.9371e-05\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3.0849e-05\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.5451e-05\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6.5201e-05\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.2865e-05\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.6030e-05\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 6.4951e-06\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.3275e-05\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 1.0628e-05\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.9065e-05\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.2195e-05\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1.6567e-05\n",
            "mean,rmse,rmse/mean,bldg: 0.10558919611442527 0.008257689177004018 0.07820581537580129 Eagle_education_Wesley\n",
            "Example prediction:\n",
            " 0 [0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0]\n",
            "Building Eagle_health_Vincenza\n",
            " Count bad values before pseudofill: 75\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_45 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_46 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_47 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [227 227 227]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 17236.8040\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 15812.7915\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14651.8667\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13783.1604\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12782.3539\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 1s 5ms/step - loss: 12020.6786\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11220.2955\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10640.9034\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 9773.6466\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9233.3476\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 5ms/step - loss: 8514.2705\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7928.8680\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7453.5383\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6895.8898\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6427.6831\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6111.4835\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5633.9261\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5082.4802\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4803.4242\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4454.7545\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4156.0947\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3884.2421\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3562.9724\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3169.1783\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3090.8839\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2741.0375\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2560.3629\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2387.5411\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2174.6516\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2084.0273\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1811.0552\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1645.5294\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1638.9097\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1459.6673\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1269.0503\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1201.4464\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1120.2522\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1041.0367\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 890.6815\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 820.6091\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 764.7215\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 684.1710\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 605.3622\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 591.8085\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 498.0796\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 437.7585\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 415.7657\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 361.4862\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 324.6722\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 308.5169\n",
            "mean,rmse,rmse/mean,bldg: 122.36347174423398 15.52647069124723 0.1268881184059643 Eagle_health_Vincenza\n",
            "Example prediction:\n",
            " 0 [183 182 183]\n",
            "Example prediction:\n",
            " 2 [163 163 163]\n",
            "Example prediction:\n",
            " 4 [154 155 154]\n",
            "Example prediction:\n",
            " 6 [138 139 138]\n",
            "Example prediction:\n",
            " 8 [160 161 160]\n",
            "Example prediction:\n",
            " 10 [168 168 168]\n",
            "Example prediction:\n",
            " 12 [165 166 165]\n",
            "Example prediction:\n",
            " 14 [127 127 127]\n",
            "Example prediction:\n",
            " 16 [50 50 50]\n",
            "Example prediction:\n",
            " 18 [55 55 55]\n",
            "Example prediction:\n",
            " 20 [94 94 95]\n",
            "Example prediction:\n",
            " 22 [183 182 183]\n",
            "Building Eagle_office_Dallas\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_48 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_49 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_50 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [89 89 89]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 4617.8353\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3845.8631\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3293.9312\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3310.6278\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2761.7556\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2909.1714\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2277.6158\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2502.9234\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2104.3673\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1769.5730\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2015.0906\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2000.4508\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2009.3960\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1494.3521\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1324.2075\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1520.2019\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1443.5947\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1255.4415\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1272.8622\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1116.9871\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 959.2965\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1247.7699\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1317.4205\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1051.3986\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1055.2517\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1328.6657\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1058.2429\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 802.7536\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 601.0725\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 912.2978\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 741.2118\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1128.2162\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1019.3056\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 825.0908\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1110.1670\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 617.6974\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1149.6422\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 688.3111\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 704.8966\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 602.5789\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 837.1167\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 962.2057\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 562.7485\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 408.4185\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 625.1590\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 463.6568\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 640.9671\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 563.9566\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 902.2353\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 505.4942\n",
            "mean,rmse,rmse/mean,bldg: 56.5030097936379 25.58411196149228 0.4527920203707978 Eagle_office_Dallas\n",
            "Example prediction:\n",
            " 0 [76 76 76]\n",
            "Example prediction:\n",
            " 2 [57 57 57]\n",
            "Example prediction:\n",
            " 4 [72 72 72]\n",
            "Example prediction:\n",
            " 6 [40 40 40]\n",
            "Example prediction:\n",
            " 8 [68 68 68]\n",
            "Example prediction:\n",
            " 10 [138 138 138]\n",
            "Example prediction:\n",
            " 12 [76 76 76]\n",
            "Example prediction:\n",
            " 14 [4 4 4]\n",
            "Example prediction:\n",
            " 16 [56 57 56]\n",
            "Example prediction:\n",
            " 18 [56 57 56]\n",
            "Example prediction:\n",
            " 20 [36 36 36]\n",
            "Example prediction:\n",
            " 22 [76 76 76]\n",
            "Building Eagle_education_Shante\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_51 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_52 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_53 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 173230.1954\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 155360.0954\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 174942.4715\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 158453.8033\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 142766.0794\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 149403.3154\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 162707.3089\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 161865.1149\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 167273.1273\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 170171.2455\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 164892.6210\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 151851.8432\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 157732.2216\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 159276.3034\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 161497.8300\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 161131.1821\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 144005.0302\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 169497.5177\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 153581.1737\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 162849.5208\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 158103.2868\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 144646.6494\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 158184.4254\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 141533.6796\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 165730.5112\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 153928.8764\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 145871.0205\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 142289.8854\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 152894.7656\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 142513.9011\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 163876.2421\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 159070.8780\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 140025.1497\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 149310.9060\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 141244.6660\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 147130.5745\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 139265.6193\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 138396.3384\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 152223.9417\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 141209.6179\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 143564.6196\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 143329.1274\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 155342.2856\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 155913.3371\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 155944.7156\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 148281.1613\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 143525.8461\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 151131.2167\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 156955.0392\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 135564.6944\n",
            "mean,rmse,rmse/mean,bldg: 1003.9941709339338 1859.7580343554769 1.8523593943035503 Eagle_education_Shante\n",
            "Example prediction:\n",
            " 0 [161 161 159]\n",
            "Example prediction:\n",
            " 2 [161 161 159]\n",
            "Example prediction:\n",
            " 4 [161 161 159]\n",
            "Example prediction:\n",
            " 6 [161 161 159]\n",
            "Example prediction:\n",
            " 8 [161 161 159]\n",
            "Example prediction:\n",
            " 10 [161 161 159]\n",
            "Example prediction:\n",
            " 12 [161 161 159]\n",
            "Example prediction:\n",
            " 14 [161 161 159]\n",
            "Example prediction:\n",
            " 16 [161 161 159]\n",
            "Example prediction:\n",
            " 18 [161 161 159]\n",
            "Example prediction:\n",
            " 20 [161 161 159]\n",
            "Example prediction:\n",
            " 22 [161 161 159]\n",
            "Building Eagle_office_Chauncey\n",
            " Count bad values before pseudofill: 116\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_54 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_55 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_56 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1391 1391 1391]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 1082460.0883\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1078832.9241\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1078700.7269\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1060532.0265\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1064885.4896\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1043108.1073\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1032910.0222\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1026614.6028\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1026333.4475\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1003419.3861\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 989879.6803\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1004982.2174\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 996217.4230\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 973846.8447\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 974175.6055\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 966731.4167\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 950044.6320\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 939627.7708\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 937447.6144\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 930886.0183\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 919588.9712\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 923484.1173\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 901275.0383\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 901264.8478\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 876540.8308\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 889063.2015\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 867994.2620\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 869187.4368\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 857594.0797\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 834974.0281\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 839437.6721\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 826907.6064\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 819443.6316\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 808101.3306\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 805987.6470\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 799419.1456\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 800199.5353\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 791296.0919\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 781723.1137\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 774309.0104\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 774574.5315\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 756256.2602\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 748669.9164\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 747642.1454\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 735184.6293\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 738592.1350\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 725991.7649\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 708801.8152\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 702623.9280\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 694414.1780\n",
            "mean,rmse,rmse/mean,bldg: 1037.6368908901366 931.8298306789883 0.8980307454948121 Eagle_office_Chauncey\n",
            "Example prediction:\n",
            " 0 [232 232 233]\n",
            "Example prediction:\n",
            " 2 [232 232 233]\n",
            "Example prediction:\n",
            " 4 [232 232 233]\n",
            "Example prediction:\n",
            " 6 [232 232 233]\n",
            "Example prediction:\n",
            " 8 [232 232 233]\n",
            "Example prediction:\n",
            " 10 [232 232 233]\n",
            "Example prediction:\n",
            " 12 [232 232 233]\n",
            "Example prediction:\n",
            " 14 [232 232 233]\n",
            "Example prediction:\n",
            " 16 [232 232 233]\n",
            "Example prediction:\n",
            " 18 [232 232 233]\n",
            "Example prediction:\n",
            " 20 [232 232 233]\n",
            "Example prediction:\n",
            " 22 [232 232 233]\n",
            "Building Eagle_office_Phyllis\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_57 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_58 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_59 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [157 157 157]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 9389.8056\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8188.4179\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7456.9591\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6816.6258\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6197.1849\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5514.0902\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5003.9358\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4588.2557\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4063.7214\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3736.8405\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3369.1602\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3017.1408\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2681.9950\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2498.6298\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2177.8828\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1977.4234\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1807.7982\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1597.7097\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1474.8395\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1342.8536\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1234.8877\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1095.5163\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 995.1954\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 863.7122\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 780.6439\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 686.2050\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 601.3017\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 536.2694\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 475.1854\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 417.6405\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 360.4547\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 304.0529\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 267.2045\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 222.5261\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 199.2899\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 159.6978\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 138.6500\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 117.7824\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 87.1703\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73.8261\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 55.8111\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 45.1273\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 34.7100\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26.7486\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18.2087\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12.8888\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9.3239\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6.4904\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4.5808\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2.8879\n",
            "mean,rmse,rmse/mean,bldg: 87.20950227834119 0.5934212971617111 0.006804548606042091 Eagle_office_Phyllis\n",
            "Example prediction:\n",
            " 0 [144 144 144]\n",
            "Example prediction:\n",
            " 2 [142 141 142]\n",
            "Example prediction:\n",
            " 4 [128 128 128]\n",
            "Example prediction:\n",
            " 6 [91 92 91]\n",
            "Example prediction:\n",
            " 8 [81 81 81]\n",
            "Example prediction:\n",
            " 10 [88 88 88]\n",
            "Example prediction:\n",
            " 12 [87 87 87]\n",
            "Example prediction:\n",
            " 14 [85 85 85]\n",
            "Example prediction:\n",
            " 16 [89 89 89]\n",
            "Example prediction:\n",
            " 18 [81 81 81]\n",
            "Example prediction:\n",
            " 20 [90 91 90]\n",
            "Example prediction:\n",
            " 22 [92 92 92]\n",
            "Building Eagle_office_Freida\n",
            " Count bad values before pseudofill: 63\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_60 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_61 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_62 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [384 384 384]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 20610.6456\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19216.4012\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17504.1937\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16747.9026\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15660.9626\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15132.8554\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14062.6098\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13401.2963\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12686.8823\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12240.5897\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11172.9564\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10872.0977\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10318.7453\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9555.1720\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9190.2102\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8952.3915\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8313.6601\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8255.8336\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7784.4671\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7493.0858\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7252.0772\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6922.8309\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5777.3027\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5588.4522\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5268.2806\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4865.9573\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4548.7014\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4320.7528\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3926.1722\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3961.6157\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3858.5003\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3537.1386\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3372.9914\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3136.2275\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2965.8183\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2842.1730\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2676.1289\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2496.1353\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2495.9617\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2201.8534\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2105.7864\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1902.7990\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2042.9361\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1959.7295\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1581.6187\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1636.8677\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1500.9205\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1410.8538\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1457.3504\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1222.7522\n",
            "mean,rmse,rmse/mean,bldg: 101.557715031556 17.943018024797357 0.1766780398635604 Eagle_office_Freida\n",
            "Example prediction:\n",
            " 0 [11 11 11]\n",
            "Example prediction:\n",
            " 2 [11 11 11]\n",
            "Example prediction:\n",
            " 4 [11 11 11]\n",
            "Example prediction:\n",
            " 6 [11 11 11]\n",
            "Example prediction:\n",
            " 8 [11 11 11]\n",
            "Example prediction:\n",
            " 10 [11 11 11]\n",
            "Example prediction:\n",
            " 12 [11 11 11]\n",
            "Example prediction:\n",
            " 14 [11 11 11]\n",
            "Example prediction:\n",
            " 16 [11 11 11]\n",
            "Example prediction:\n",
            " 18 [11 11 11]\n",
            "Example prediction:\n",
            " 20 [11 11 11]\n",
            "Example prediction:\n",
            " 22 [11 11 11]\n",
            "Building Eagle_office_Francis\n",
            " Count bad values before pseudofill: 20\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_63 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_64 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_65 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [378 378 378]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 87431.1007\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 85357.6648\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 81786.6434\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 79877.5267\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 76502.2694\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73505.6337\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 71031.7456\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 70171.7216\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 67982.1328\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 66214.1393\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 62753.4919\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 61015.4877\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 59388.5611\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 57233.2840\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 55428.0383\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 53667.3910\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 50757.9916\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 50396.9593\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 48258.8177\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 46113.0114\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 45398.1649\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 42932.5520\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 41430.1939\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 39744.2511\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 38555.9444\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 37126.7536\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 35537.9213\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 34306.6653\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 32965.9970\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 31479.5863\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30944.4654\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28442.7592\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 27690.2302\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26765.7779\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 25340.3734\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24896.4845\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23286.0703\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 22579.0580\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21576.7547\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20808.4227\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19635.6258\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18964.9998\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17621.7144\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17438.2826\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16523.3592\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15704.1977\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14968.9713\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14816.1537\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13657.5579\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13198.3373\n",
            "mean,rmse,rmse/mean,bldg: 336.48649201094844 203.78686422262504 0.6056316347343724 Eagle_office_Francis\n",
            "Example prediction:\n",
            " 0 [216 215 215]\n",
            "Example prediction:\n",
            " 2 [216 215 215]\n",
            "Example prediction:\n",
            " 4 [216 215 215]\n",
            "Example prediction:\n",
            " 6 [216 215 215]\n",
            "Example prediction:\n",
            " 8 [216 215 215]\n",
            "Example prediction:\n",
            " 10 [216 215 215]\n",
            "Example prediction:\n",
            " 12 [216 215 215]\n",
            "Example prediction:\n",
            " 14 [216 215 215]\n",
            "Example prediction:\n",
            " 16 [216 215 215]\n",
            "Example prediction:\n",
            " 18 [216 215 215]\n",
            "Example prediction:\n",
            " 20 [216 215 215]\n",
            "Example prediction:\n",
            " 22 [216 215 215]\n",
            "Building Eagle_office_Sheree\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_66 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_67 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_68 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [143 143 143]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 4803.1950\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4199.1202\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3902.5418\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3387.1579\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3178.6828\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2819.2560\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2499.3073\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2303.8101\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2104.2506\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1976.4203\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1728.9178\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1600.8418\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1486.8028\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1289.6501\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1216.3986\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1090.1559\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1030.2291\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 922.8507\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 822.5856\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 726.9388\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 683.2941\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 670.1514\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 611.8630\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 547.9113\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 507.7194\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 486.3370\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 405.0817\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 349.1429\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 322.7071\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 261.5398\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 287.2856\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 244.1926\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 219.6419\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 208.7503\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 187.8507\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 183.1491\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 141.7834\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 145.5476\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 127.8137\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 121.3057\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 88.7551\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 97.7823\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 78.3060\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73.9739\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 67.0730\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 68.9527\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 52.2754\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 53.0860\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 48.1485\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 45.3149\n",
            "mean,rmse,rmse/mean,bldg: 62.02257521757287 10.925367788972622 0.17615146985830307 Eagle_office_Sheree\n",
            "Example prediction:\n",
            " 0 [153 153 153]\n",
            "Example prediction:\n",
            " 2 [153 153 153]\n",
            "Example prediction:\n",
            " 4 [133 133 133]\n",
            "Example prediction:\n",
            " 6 [131 131 132]\n",
            "Example prediction:\n",
            " 8 [153 153 153]\n",
            "Example prediction:\n",
            " 10 [138 138 139]\n",
            "Example prediction:\n",
            " 12 [48 49 49]\n",
            "Example prediction:\n",
            " 14 [55 55 55]\n",
            "Example prediction:\n",
            " 16 [3 3 3]\n",
            "Example prediction:\n",
            " 18 [49 49 49]\n",
            "Example prediction:\n",
            " 20 [142 142 143]\n",
            "Example prediction:\n",
            " 22 [153 153 153]\n",
            "Building Eagle_education_Sherrill\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_69 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_70 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_71 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [3461 3461 3461]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 5410917.2654\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5519765.6250\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5404150.3714\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5550621.0036\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5530851.3514\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5480690.2428\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5397535.8641\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5318281.7337\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5356907.3098\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5335471.3877\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5325403.3496\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5264389.7156\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5342357.8207\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5223485.8496\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5308399.2717\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5170796.8641\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5227490.1721\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5185226.9402\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5170737.1621\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5149347.7120\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5106640.5217\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5168257.1395\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5055415.1866\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5093096.2083\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5107301.8351\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5100761.1540\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5085097.3116\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5081161.0091\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5010202.9493\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4993618.3043\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5000341.2699\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4917025.7627\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4841167.0027\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4823877.5109\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4882206.3949\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4832522.9846\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4917705.1341\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4781484.9420\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4787718.2708\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4805305.2536\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4828083.5091\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4740479.9928\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4727292.0525\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4746833.5371\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4649589.8451\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4674568.3659\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4619792.5181\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4607007.9040\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 4698963.7591\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4614473.3605\n",
            "mean,rmse,rmse/mean,bldg: 2032.483907505765 2078.0052500397105 1.02239690182335 Eagle_education_Sherrill\n",
            "Example prediction:\n",
            " 0 [233 233 235]\n",
            "Example prediction:\n",
            " 2 [233 233 235]\n",
            "Example prediction:\n",
            " 4 [233 233 235]\n",
            "Example prediction:\n",
            " 6 [233 233 235]\n",
            "Example prediction:\n",
            " 8 [233 233 235]\n",
            "Example prediction:\n",
            " 10 [233 233 235]\n",
            "Example prediction:\n",
            " 12 [233 233 235]\n",
            "Example prediction:\n",
            " 14 [233 233 235]\n",
            "Example prediction:\n",
            " 16 [233 233 235]\n",
            "Example prediction:\n",
            " 18 [233 233 235]\n",
            "Example prediction:\n",
            " 20 [233 233 235]\n",
            "Example prediction:\n",
            " 22 [233 233 235]\n",
            "Building Eagle_education_Brooke\n",
            " Count bad values before pseudofill: 56\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_72 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_73 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_74 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4255 4255 4255]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 4710816.9710\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4650126.2908\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4592153.2274\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4609156.7029\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4640693.6893\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4511915.1630\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4565101.2899\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4590603.5027\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4572606.3533\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4578968.3841\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4491669.6232\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4440839.7654\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4485020.3197\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4466246.9909\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4297533.4031\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4409116.7717\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4332403.8487\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4497217.1486\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4438953.2174\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4493908.2455\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4464598.9746\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4293119.8261\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4298150.4004\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4236645.5118\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4322901.3687\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4142835.4819\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4285763.4538\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4242228.9139\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4161348.0806\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4277636.7355\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4178432.8623\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4169548.0707\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 4200040.6150\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4132096.7708\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4139937.7065\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4093092.2989\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4065751.3786\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4096584.3379\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4107330.1259\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4024773.6893\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4050724.3650\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4017052.4058\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4048827.7274\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3993715.1712\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3914068.3759\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3904508.4167\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3996101.0380\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3973195.0906\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3862659.0933\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3770861.4176\n",
            "mean,rmse,rmse/mean,bldg: 1639.356375181079 1633.2334640676315 0.9962650518178079 Eagle_education_Brooke\n",
            "Example prediction:\n",
            " 0 [236 232 233]\n",
            "Example prediction:\n",
            " 2 [236 232 233]\n",
            "Example prediction:\n",
            " 4 [236 232 233]\n",
            "Example prediction:\n",
            " 6 [236 232 233]\n",
            "Example prediction:\n",
            " 8 [236 232 233]\n",
            "Example prediction:\n",
            " 10 [236 232 233]\n",
            "Example prediction:\n",
            " 12 [236 232 233]\n",
            "Example prediction:\n",
            " 14 [236 232 233]\n",
            "Example prediction:\n",
            " 16 [236 232 233]\n",
            "Example prediction:\n",
            " 18 [236 232 233]\n",
            "Example prediction:\n",
            " 20 [236 232 233]\n",
            "Example prediction:\n",
            " 22 [236 232 233]\n",
            "Building Eagle_education_Alberto\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_75 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_76 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_77 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1254 1254 1254]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 615983.9792\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 612735.8281\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 598433.9355\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 588970.9447\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 580137.0188\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 572945.3036\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 568714.8128\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 569185.5909\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 552227.2637\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 558458.2878\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 547911.6264\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 532283.8187\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 535483.4119\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 535242.5350\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 521610.1067\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 518579.7665\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 513113.8362\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 492868.0185\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 496087.8372\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 488001.8045\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 476404.3938\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 467625.2732\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 483355.5602\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 468426.1359\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 464403.9566\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 463163.9885\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 449657.1248\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 449224.6775\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 447738.9984\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 445570.4072\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 430571.5545\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 422615.1583\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 417243.7834\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 417915.1018\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 420074.6844\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 402026.1037\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 401432.7014\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 394158.1379\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 386322.2365\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 387132.2604\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 386864.8695\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 372259.2909\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 371550.3742\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 372225.2553\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 363398.3150\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 359630.2508\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 354356.5418\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 347254.0883\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 341026.9889\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 337368.0720\n",
            "mean,rmse,rmse/mean,bldg: 694.962522886029 519.837533269893 0.7480080092824577 Eagle_education_Alberto\n",
            "Example prediction:\n",
            " 0 [229 231 231]\n",
            "Example prediction:\n",
            " 2 [229 231 231]\n",
            "Example prediction:\n",
            " 4 [229 231 231]\n",
            "Example prediction:\n",
            " 6 [229 231 231]\n",
            "Example prediction:\n",
            " 8 [229 231 231]\n",
            "Example prediction:\n",
            " 10 [229 231 231]\n",
            "Example prediction:\n",
            " 12 [229 231 231]\n",
            "Example prediction:\n",
            " 14 [229 231 231]\n",
            "Example prediction:\n",
            " 16 [229 231 231]\n",
            "Example prediction:\n",
            " 18 [229 231 231]\n",
            "Example prediction:\n",
            " 20 [229 231 231]\n",
            "Example prediction:\n",
            " 22 [229 231 231]\n",
            "Building Eagle_food_Kay\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_78 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_79 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_80 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [598 598 598]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 126524.5650\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 123512.6578\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 118872.3921\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 119415.1309\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 114065.4402\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 109381.4902\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 110548.8930\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 108220.6241\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 104207.9932\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 98636.3109\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 103636.1247\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 99496.8205\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 94304.2313\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 92934.6013\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 90116.9705\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 90862.0177\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 85402.8442\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 86527.1931\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 83175.2588\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 83478.9694\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 82853.8032\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 78029.5318\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 77667.5945\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 76109.9573\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 72040.5655\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 70900.7946\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73646.0551\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 67525.9960\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 65075.4138\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 64247.8352\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 65403.0163\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 63206.2519\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 59541.9121\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 59882.7189\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 56114.3229\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 58174.3643\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 54162.9192\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 52958.2615\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 52557.5761\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 51114.4264\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 49759.7246\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 47940.4070\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 47428.9622\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 47734.4246\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 46037.6169\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 46597.2876\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 43983.2268\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 42779.3965\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 41014.7160\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 42486.0079\n",
            "mean,rmse,rmse/mean,bldg: 276.2706673678431 155.9985943297739 0.5646585495885025 Eagle_food_Kay\n",
            "Example prediction:\n",
            " 0 [220 219 218]\n",
            "Example prediction:\n",
            " 2 [220 219 218]\n",
            "Example prediction:\n",
            " 4 [220 219 218]\n",
            "Example prediction:\n",
            " 6 [220 219 218]\n",
            "Example prediction:\n",
            " 8 [220 219 218]\n",
            "Example prediction:\n",
            " 10 [220 219 218]\n",
            "Example prediction:\n",
            " 12 [220 219 218]\n",
            "Example prediction:\n",
            " 14 [110 110 111]\n",
            "Example prediction:\n",
            " 16 [33 33 33]\n",
            "Example prediction:\n",
            " 18 [119 119 119]\n",
            "Example prediction:\n",
            " 20 [175 175 175]\n",
            "Example prediction:\n",
            " 22 [220 219 218]\n",
            "Building Eagle_health_Jodi\n",
            " Count bad values before pseudofill: 41\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_81 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_82 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_83 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [248 248 248]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 35427.4871\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 33184.2974\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 31843.9870\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30351.9719\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28915.8841\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 27586.3052\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26115.9539\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24721.5468\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24187.8836\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 22362.8357\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21421.5561\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20414.1328\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19632.6971\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18593.5723\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17666.6925\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16653.6079\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15509.2020\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14867.4707\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14457.0410\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13187.1582\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12775.1934\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11725.8968\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11657.2348\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10750.1810\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10352.2440\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9588.1094\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9251.4839\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8650.1856\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8236.2167\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7527.2140\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7121.7406\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6485.0803\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6266.6653\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5846.2461\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5413.9881\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5206.9243\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5044.5374\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4489.3673\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4345.5353\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3896.6846\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3673.2098\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3459.7789\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3276.1338\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 3065.5806\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2797.7599\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 2691.9564\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2482.1729\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2392.9467\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2113.8495\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2000.0660\n",
            "mean,rmse,rmse/mean,bldg: 192.4223112628316 63.819493206570286 0.33166368695882975 Eagle_health_Jodi\n",
            "Example prediction:\n",
            " 0 [194 195 194]\n",
            "Example prediction:\n",
            " 2 [199 201 199]\n",
            "Example prediction:\n",
            " 4 [184 185 184]\n",
            "Example prediction:\n",
            " 6 [155 155 155]\n",
            "Example prediction:\n",
            " 8 [151 151 151]\n",
            "Example prediction:\n",
            " 10 [145 145 145]\n",
            "Example prediction:\n",
            " 12 [135 135 135]\n",
            "Example prediction:\n",
            " 14 [145 146 145]\n",
            "Example prediction:\n",
            " 16 [160 160 160]\n",
            "Example prediction:\n",
            " 18 [170 171 170]\n",
            "Example prediction:\n",
            " 20 [161 161 161]\n",
            "Example prediction:\n",
            " 22 [177 178 177]\n",
            "Building Eagle_education_Norah\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_84 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_85 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_86 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1521 1521 1521]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 634567.1488\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 602787.4683\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 609135.8716\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 601950.2116\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 591941.4608\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 592045.5385\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 568112.7501\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 574020.9186\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 558187.6343\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 573780.9416\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 564345.9900\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 541416.7154\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 538050.7931\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 526553.8038\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 549021.0502\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 517940.8339\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 545847.7668\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 518789.3254\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 528420.2646\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 512305.1703\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 499209.1419\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 510408.8752\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 498277.8623\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 495692.7077\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 475682.2163\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 471338.6395\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 476927.5171\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 467253.8505\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 461920.8424\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 447695.3513\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 441913.6158\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 458823.1018\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 446991.6599\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 444641.7014\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 433497.8895\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 427273.9315\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 431311.9526\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 416552.5759\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 411428.0557\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 414170.6008\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 401109.6341\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 396231.8877\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 389561.5320\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 389639.6839\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 397012.2335\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 385520.4668\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 375896.1270\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 385505.4755\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 369454.5635\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 374157.1088\n",
            "mean,rmse,rmse/mean,bldg: 712.0113639930826 660.2034988340047 0.9272373057804436 Eagle_education_Norah\n",
            "Example prediction:\n",
            " 0 [230 230 227]\n",
            "Example prediction:\n",
            " 2 [230 230 227]\n",
            "Example prediction:\n",
            " 4 [230 230 227]\n",
            "Example prediction:\n",
            " 6 [230 230 227]\n",
            "Example prediction:\n",
            " 8 [230 230 227]\n",
            "Example prediction:\n",
            " 10 [230 230 227]\n",
            "Example prediction:\n",
            " 12 [230 230 227]\n",
            "Example prediction:\n",
            " 14 [230 230 227]\n",
            "Example prediction:\n",
            " 16 [230 230 227]\n",
            "Example prediction:\n",
            " 18 [230 230 227]\n",
            "Example prediction:\n",
            " 20 [230 230 227]\n",
            "Example prediction:\n",
            " 22 [230 230 227]\n",
            "Building Eagle_education_Will\n",
            " Count bad values before pseudofill: 15\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_87 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_88 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_89 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [337 337 337]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 76845.7783\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73458.4492\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 69609.0900\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 68266.9276\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 67143.0077\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 65656.1681\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 62913.1074\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 59948.2882\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 58947.8705\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 57474.3487\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 53852.3341\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 52545.4885\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 50456.6447\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 48554.0953\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 47532.6675\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 44967.8600\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 44382.8158\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 41366.2471\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 41479.4449\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 39252.4969\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 38102.7811\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 36690.5825\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 34564.9557\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 34032.8401\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 32510.4743\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30393.3978\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30574.8363\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28632.9215\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26784.5914\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26203.1113\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24793.2042\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24813.6375\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23165.0886\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21781.3733\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20816.7461\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20880.7828\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19549.2409\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 18304.8095\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17540.9610\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17239.8265\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16568.5071\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15468.8976\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15036.2664\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13912.7446\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 13601.8572\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13978.5527\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12214.0271\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12106.7643\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 12318.4160\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10901.9811\n",
            "mean,rmse,rmse/mean,bldg: 226.17039013254305 46.07117828350373 0.20370119296564218 Eagle_education_Will\n",
            "Example prediction:\n",
            " 0 [213 212 212]\n",
            "Example prediction:\n",
            " 2 [213 212 212]\n",
            "Example prediction:\n",
            " 4 [213 212 212]\n",
            "Example prediction:\n",
            " 6 [213 212 212]\n",
            "Example prediction:\n",
            " 8 [213 212 212]\n",
            "Example prediction:\n",
            " 10 [213 212 212]\n",
            "Example prediction:\n",
            " 12 [213 212 212]\n",
            "Example prediction:\n",
            " 14 [213 212 212]\n",
            "Example prediction:\n",
            " 16 [213 212 212]\n",
            "Example prediction:\n",
            " 18 [213 212 212]\n",
            "Example prediction:\n",
            " 20 [213 212 212]\n",
            "Example prediction:\n",
            " 22 [213 212 212]\n",
            "Building Eagle_lodging_Blake\n",
            " Count bad values before pseudofill: 8\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_90 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_91 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_92 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [463 463 463]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 33035.2572\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 31280.6053\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29052.0587\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28634.2163\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26478.4820\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 27973.3562\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26485.6408\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26864.3783\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29310.7444\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28035.9465\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 25398.8720\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 25756.9008\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26479.3490\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 25795.0647\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24775.0269\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23781.0942\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24245.3256\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 23662.7449\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21136.2111\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23814.6778\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21848.3124\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21976.3316\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21243.7651\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20932.3075\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21589.0826\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20331.3899\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20677.1066\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20068.8611\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19272.8837\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19323.5694\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19498.7087\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17922.9230\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17888.7457\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18438.2120\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17803.5439\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18561.0709\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16977.4010\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17055.8951\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16375.0710\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15029.5196\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15302.9064\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15834.1216\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16101.0168\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15734.3725\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14091.2891\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14551.6109\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14554.6016\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16128.3656\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14804.0590\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15351.1415\n",
            "mean,rmse,rmse/mean,bldg: 43.43471997795112 5.307902773487843 0.12220414397013052 Eagle_lodging_Blake\n",
            "Example prediction:\n",
            " 0 [80 80 80]\n",
            "Example prediction:\n",
            " 2 [124 124 125]\n",
            "Example prediction:\n",
            " 4 [106 106 106]\n",
            "Example prediction:\n",
            " 6 [88 88 88]\n",
            "Example prediction:\n",
            " 8 [88 88 88]\n",
            "Example prediction:\n",
            " 10 [96 96 96]\n",
            "Example prediction:\n",
            " 12 [105 105 105]\n",
            "Example prediction:\n",
            " 14 [124 124 125]\n",
            "Example prediction:\n",
            " 16 [105 105 105]\n",
            "Example prediction:\n",
            " 18 [89 89 89]\n",
            "Example prediction:\n",
            " 20 [89 89 89]\n",
            "Example prediction:\n",
            " 22 [88 88 88]\n",
            "Building Eagle_education_Petra\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_93 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_94 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_95 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [117 117 117]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 4335.0880\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3602.6296\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3207.1689\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2821.9638\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2584.1361\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2374.7470\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2055.0249\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1917.9758\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1720.0670\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1686.6049\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1569.8767\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1516.9871\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1446.9009\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1190.0879\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1044.0159\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 949.2850\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 821.0298\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 737.8256\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 680.3233\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 590.1234\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 540.6104\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 500.4536\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 435.2334\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 392.9606\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 339.4275\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 291.4205\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 273.4450\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 258.9085\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 225.3144\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 193.9509\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 181.1528\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 167.6954\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 136.1501\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 106.9643\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 107.6159\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 90.1959\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 82.5610\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 72.5299\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 62.8892\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 58.0348\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 51.7456\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 43.8505\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 38.6709\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 31.1796\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30.4780\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 27.0204\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23.8590\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20.3801\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18.9302\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15.7646\n",
            "mean,rmse,rmse/mean,bldg: 57.04381605290169 6.086219781588112 0.10669376985480479 Eagle_education_Petra\n",
            "Example prediction:\n",
            " 0 [88 88 88]\n",
            "Example prediction:\n",
            " 2 [79 80 79]\n",
            "Example prediction:\n",
            " 4 [79 79 78]\n",
            "Example prediction:\n",
            " 6 [100 100 100]\n",
            "Example prediction:\n",
            " 8 [137 137 137]\n",
            "Example prediction:\n",
            " 10 [134 135 135]\n",
            "Example prediction:\n",
            " 12 [55 55 55]\n",
            "Example prediction:\n",
            " 14 [25 25 25]\n",
            "Example prediction:\n",
            " 16 [18 18 18]\n",
            "Example prediction:\n",
            " 18 [45 45 45]\n",
            "Example prediction:\n",
            " 20 [128 128 128]\n",
            "Example prediction:\n",
            " 22 [145 146 146]\n",
            "Building Eagle_lodging_Trina\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_96 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_97 (GRU)                 (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_98 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [233 233 233]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 7ms/step - loss: 13223.9888\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 11916.7550\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11139.4405\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10237.1481\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9845.5290\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9281.5268\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8646.8285\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8717.7146\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7901.3453\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7365.9370\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7290.9174\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6989.5105\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6730.9921\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6183.7950\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5941.2653\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5605.1142\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5411.4806\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5338.9242\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5164.3945\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4691.1583\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4371.2292\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4442.0976\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4078.9062\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3946.1141\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3978.4485\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3472.3220\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3303.6171\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3163.8865\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 2909.3686\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2804.8058\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2736.2782\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2716.9616\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2438.4071\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 2224.6850\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2253.2047\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2021.5250\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2013.7656\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1964.9933\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1717.0965\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1656.1946\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1781.8556\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1549.7726\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1434.8832\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1436.8211\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1468.3976\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1422.1222\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1259.2781\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1229.4958\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1228.7717\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1010.0423\n",
            "mean,rmse,rmse/mean,bldg: 91.27234855584753 22.83379668279536 0.2501721172302679 Eagle_lodging_Trina\n",
            "Example prediction:\n",
            " 0 [185 188 186]\n",
            "Example prediction:\n",
            " 2 [185 188 186]\n",
            "Example prediction:\n",
            " 4 [104 104 104]\n",
            "Example prediction:\n",
            " 6 [168 169 169]\n",
            "Example prediction:\n",
            " 8 [185 188 186]\n",
            "Example prediction:\n",
            " 10 [157 159 158]\n",
            "Example prediction:\n",
            " 12 [139 140 139]\n",
            "Example prediction:\n",
            " 14 [73 72 73]\n",
            "Example prediction:\n",
            " 16 [99 99 99]\n",
            "Example prediction:\n",
            " 18 [80 80 80]\n",
            "Example prediction:\n",
            " 20 [75 75 75]\n",
            "Example prediction:\n",
            " 22 [68 68 69]\n",
            "Building Eagle_health_Reuben\n",
            "Building Eagle_education_Teresa\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_99 (GRU)                 (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_100 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_101 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [226 226 226]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 30029.9821\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28132.6074\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26751.0484\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 25548.3559\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24167.4693\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23970.6638\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 22983.2569\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21661.7270\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20681.7655\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19265.4077\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18559.5166\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 17131.4680\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17212.5923\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15724.9738\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15396.4852\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 15566.3200\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14668.3924\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13191.0965\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12848.8069\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11960.6770\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11804.4781\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11508.8797\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10704.3456\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10466.6300\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9835.4074\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9548.8833\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8785.8438\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8739.4333\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8000.7134\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7805.7621\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7152.1000\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7001.0125\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6860.2773\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6358.0482\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6268.4019\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5982.7603\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5785.0830\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5742.0609\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5143.7990\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4985.7681\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4808.6947\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4546.2361\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4313.8429\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4177.2992\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4208.4402\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3727.7367\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3804.6086\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3479.9251\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3424.4687\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3332.0352\n",
            "mean,rmse,rmse/mean,bldg: 148.73384346526427 28.152045162427726 0.18927800496867034 Eagle_education_Teresa\n",
            "Example prediction:\n",
            " 0 [180 179 179]\n",
            "Example prediction:\n",
            " 2 [171 170 170]\n",
            "Example prediction:\n",
            " 4 [180 179 180]\n",
            "Example prediction:\n",
            " 6 [164 163 163]\n",
            "Example prediction:\n",
            " 8 [167 167 167]\n",
            "Example prediction:\n",
            " 10 [184 183 183]\n",
            "Example prediction:\n",
            " 12 [181 180 181]\n",
            "Example prediction:\n",
            " 14 [148 147 147]\n",
            "Example prediction:\n",
            " 16 [74 74 74]\n",
            "Example prediction:\n",
            " 18 [87 86 86]\n",
            "Example prediction:\n",
            " 20 [155 154 154]\n",
            "Example prediction:\n",
            " 22 [198 198 198]\n",
            "Building Eagle_office_Norbert\n",
            " Count bad values before pseudofill: 52\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_102 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_103 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_104 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [499 499 499]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 149099.6274\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 145805.6784\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 144227.1911\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 140176.1659\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 136011.1247\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 133832.2395\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 130539.2448\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 126086.7848\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 123674.6666\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 121215.4037\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 118473.1305\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 116575.4712\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 111547.2676\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 109759.3971\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 108345.6608\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 105208.6690\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 102946.0603\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 100633.0752\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 97366.1883\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 94663.6166\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 92910.6737\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 91081.9107\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 87970.8405\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 85706.4859\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 83801.1277\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 81248.8351\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 79396.5781\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 76752.9597\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 75789.1052\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 73305.9410\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 69497.2118\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 68874.1012\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 65783.7340\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 64918.0731\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 62812.2353\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 60312.0467\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 58763.5684\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 57400.9066\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 55365.4648\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 54440.9030\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 51796.8287\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 51092.4695\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 49059.5088\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 48313.2112\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 45324.8695\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 43774.5664\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 41785.3805\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 41632.6588\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 40142.4898\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 38959.4347\n",
            "mean,rmse,rmse/mean,bldg: 390.8100632860704 225.59457974163442 0.5772486456585962 Eagle_office_Norbert\n",
            "Example prediction:\n",
            " 0 [222 222 225]\n",
            "Example prediction:\n",
            " 2 [222 222 225]\n",
            "Example prediction:\n",
            " 4 [222 222 225]\n",
            "Example prediction:\n",
            " 6 [222 222 225]\n",
            "Example prediction:\n",
            " 8 [222 222 225]\n",
            "Example prediction:\n",
            " 10 [222 222 225]\n",
            "Example prediction:\n",
            " 12 [222 222 225]\n",
            "Example prediction:\n",
            " 14 [222 222 225]\n",
            "Example prediction:\n",
            " 16 [222 222 225]\n",
            "Example prediction:\n",
            " 18 [222 222 225]\n",
            "Example prediction:\n",
            " 20 [222 222 225]\n",
            "Example prediction:\n",
            " 22 [222 222 225]\n",
            "Building Eagle_lodging_Casey\n",
            "Building Eagle_office_Tia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_105 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_106 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_107 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [425 425 425]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 7ms/step - loss: 49603.2592\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 47547.1019\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 45801.5300\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 43196.3041\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 42288.3891\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 40664.8315\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 39360.4692\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 37405.5080\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 35749.3138\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 35511.6806\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 33572.4250\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 32076.2873\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 31152.0862\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29845.8056\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29595.2470\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 28843.2875\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 27199.1163\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26206.7532\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24439.2538\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23861.8403\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 22736.6842\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21781.7164\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21378.2523\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19547.5262\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18673.2828\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18534.5509\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17727.4084\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17306.1310\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 17548.2594\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16259.3155\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15727.1907\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 14849.1172\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13919.3396\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13861.7957\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13200.6090\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12218.2121\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 11986.8950\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10999.9074\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10989.5118\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10513.8639\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11189.0748\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9483.2930\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9985.2425\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9189.7552\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9710.6366\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8808.0857\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7967.4304\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7242.5708\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7569.9052\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7177.8412\n",
            "mean,rmse,rmse/mean,bldg: 174.6359145783468 69.31446017625832 0.3969083927759992 Eagle_office_Tia\n",
            "Example prediction:\n",
            " 0 [207 209 207]\n",
            "Example prediction:\n",
            " 2 [207 209 207]\n",
            "Example prediction:\n",
            " 4 [207 209 207]\n",
            "Example prediction:\n",
            " 6 [136 137 136]\n",
            "Example prediction:\n",
            " 8 [144 146 145]\n",
            "Example prediction:\n",
            " 10 [143 144 143]\n",
            "Example prediction:\n",
            " 12 [151 152 151]\n",
            "Example prediction:\n",
            " 14 [155 157 156]\n",
            "Example prediction:\n",
            " 16 [94 94 94]\n",
            "Example prediction:\n",
            " 18 [123 124 123]\n",
            "Example prediction:\n",
            " 20 [207 209 207]\n",
            "Example prediction:\n",
            " 22 [207 209 207]\n",
            "Building Eagle_office_Remedios\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_108 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_109 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_110 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [139 139 139]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 6ms/step - loss: 13963.5908\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12852.1645\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11644.2436\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11166.5098\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10173.8568\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9242.9063\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8602.8732\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8039.6452\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7400.7174\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6783.7477\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6304.5220\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5699.3842\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 5301.5000\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4894.6888\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4477.9911\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4303.1482\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 3619.2191\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 3457.8129\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 3183.9966\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2904.6952\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 2620.9896\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2460.0063\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2122.7466\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1974.7921\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1877.4586\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1641.5616\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1467.8056\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1312.6264\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1185.9545\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1084.0834\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 933.8713\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 839.0872\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 745.2551\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 684.4890\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 575.0253\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 521.4083\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 467.5354\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 398.0338\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 357.0073\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 311.2941\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 290.7456\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 237.6591\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 211.3738\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 203.4557\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 165.8423\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 156.3314\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 128.3480\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 117.2323\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 97.5796\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 88.7579\n",
            "mean,rmse,rmse/mean,bldg: 121.48132139556034 7.988832603963941 0.0657618184605613 Eagle_office_Remedios\n",
            "Example prediction:\n",
            " 0 [150 150 150]\n",
            "Example prediction:\n",
            " 2 [136 136 136]\n",
            "Example prediction:\n",
            " 4 [137 137 137]\n",
            "Example prediction:\n",
            " 6 [136 137 136]\n",
            "Example prediction:\n",
            " 8 [143 144 143]\n",
            "Example prediction:\n",
            " 10 [140 140 140]\n",
            "Example prediction:\n",
            " 12 [132 132 131]\n",
            "Example prediction:\n",
            " 14 [141 141 141]\n",
            "Example prediction:\n",
            " 16 [129 129 129]\n",
            "Example prediction:\n",
            " 18 [156 156 156]\n",
            "Example prediction:\n",
            " 20 [154 153 154]\n",
            "Example prediction:\n",
            " 22 [143 143 143]\n",
            "Building Eagle_office_Patrice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_111 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_112 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_113 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [253 253 253]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 5s 6ms/step - loss: 32897.6968\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 30777.2471\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29362.3804\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 29081.7864\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 27536.5637\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 27822.4790\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 26182.9115\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 25994.0933\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 24449.0565\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23814.3501\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 23358.2718\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 22004.9963\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 21667.8863\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 20532.8707\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 18109.8572\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18832.7559\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 19161.6810\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 18336.2296\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16771.9623\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 16451.7517\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 15221.5570\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 15854.1129\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 15318.8042\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 14446.1614\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 13486.6558\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 12478.0668\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 13099.5297\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12838.2359\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 12304.2677\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 11794.8958\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 11153.9775\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 11068.1672\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10993.1177\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10661.9292\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10074.8238\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 9649.7082\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9534.1062\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 9015.8945\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 8085.2608\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 8496.7908\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 7778.7286\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7521.9540\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7462.8456\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7537.6306\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6939.2551\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7166.3316\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6613.3363\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6573.4474\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6044.0312\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6193.5419\n",
            "mean,rmse,rmse/mean,bldg: 165.87984746703071 88.3052673371195 0.5323447584835194 Eagle_office_Patrice\n",
            "Example prediction:\n",
            " 0 [202 201 202]\n",
            "Example prediction:\n",
            " 2 [202 201 202]\n",
            "Example prediction:\n",
            " 4 [202 201 202]\n",
            "Example prediction:\n",
            " 6 [202 201 202]\n",
            "Example prediction:\n",
            " 8 [202 201 202]\n",
            "Example prediction:\n",
            " 10 [202 201 202]\n",
            "Example prediction:\n",
            " 12 [202 201 202]\n",
            "Example prediction:\n",
            " 14 [202 201 202]\n",
            "Example prediction:\n",
            " 16 [202 201 202]\n",
            "Example prediction:\n",
            " 18 [202 201 202]\n",
            "Example prediction:\n",
            " 20 [202 201 202]\n",
            "Example prediction:\n",
            " 22 [202 201 202]\n",
            "Building Eagle_education_Shana\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_114 (GRU)                (None, 7, 16)             960       \n",
            "_________________________________________________________________\n",
            "gru_115 (GRU)                (None, 7, 16)             1632      \n",
            "_________________________________________________________________\n",
            "gru_116 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 4,275\n",
            "Trainable params: 4,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [186 186 186]\n",
            "Epoch 1/50\n",
            "275/275 [==============================] - 6s 7ms/step - loss: 13157.8920\n",
            "Epoch 2/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 11954.8697\n",
            "Epoch 3/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 10982.1192\n",
            "Epoch 4/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 10052.8641\n",
            "Epoch 5/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 9433.2361\n",
            "Epoch 6/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 8677.3496\n",
            "Epoch 7/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 8030.2792\n",
            "Epoch 8/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 7237.1871\n",
            "Epoch 9/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 6677.6611\n",
            "Epoch 10/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 6180.9828\n",
            "Epoch 11/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 5744.4074\n",
            "Epoch 12/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 5097.2843\n",
            "Epoch 13/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4738.2380\n",
            "Epoch 14/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 4420.7860\n",
            "Epoch 15/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3905.7548\n",
            "Epoch 16/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3530.2711\n",
            "Epoch 17/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 3269.9516\n",
            "Epoch 18/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 3093.5188\n",
            "Epoch 19/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2747.5947\n",
            "Epoch 20/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2549.0533\n",
            "Epoch 21/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2331.3353\n",
            "Epoch 22/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 2144.6738\n",
            "Epoch 23/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1977.5853\n",
            "Epoch 24/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1842.8413\n",
            "Epoch 25/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1697.6877\n",
            "Epoch 26/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1464.3744\n",
            "Epoch 27/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1349.2426\n",
            "Epoch 28/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 1201.3995\n",
            "Epoch 29/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 1143.5772\n",
            "Epoch 30/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 985.6414\n",
            "Epoch 31/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 899.9763\n",
            "Epoch 32/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 803.3595\n",
            "Epoch 33/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 721.0122\n",
            "Epoch 34/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 642.2823\n",
            "Epoch 35/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 555.0375\n",
            "Epoch 36/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 484.1881\n",
            "Epoch 37/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 451.9651\n",
            "Epoch 38/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 388.8460\n",
            "Epoch 39/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 340.6310\n",
            "Epoch 40/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 270.4351\n",
            "Epoch 41/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 237.4079\n",
            "Epoch 42/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 205.9259\n",
            "Epoch 43/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 177.0878\n",
            "Epoch 44/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 148.3556\n",
            "Epoch 45/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 124.2767\n",
            "Epoch 46/50\n",
            "275/275 [==============================] - 2s 7ms/step - loss: 106.2895\n",
            "Epoch 47/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 84.6281\n",
            "Epoch 48/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 65.9229\n",
            "Epoch 49/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 48.4536\n",
            "Epoch 50/50\n",
            "275/275 [==============================] - 2s 6ms/step - loss: 36.2841\n",
            "mean,rmse,rmse/mean,bldg: 103.18172944172048 2.9016414834491706 0.02812165970805992 Eagle_education_Shana\n",
            "Example prediction:\n",
            " 0 [169 170 170]\n",
            "Example prediction:\n",
            " 2 [168 168 168]\n",
            "Example prediction:\n",
            " 4 [150 151 151]\n",
            "Example prediction:\n",
            " 6 [108 108 108]\n",
            "Example prediction:\n",
            " 8 [93 93 93]\n",
            "Example prediction:\n",
            " 10 [103 103 103]\n",
            "Example prediction:\n",
            " 12 [102 102 102]\n",
            "Example prediction:\n",
            " 14 [100 100 100]\n",
            "Example prediction:\n",
            " 16 [105 105 105]\n",
            "Example prediction:\n",
            " 18 [93 93 93]\n",
            "Example prediction:\n",
            " 20 [106 107 106]\n",
            "Example prediction:\n",
            " 22 [109 109 109]\n",
            "\n",
            "History 7 Future 3\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      0.00       0.00   inf   Eagle_office_Henriette\n",
            "      0.11       0.01  0.08   Eagle_education_Wesley\n",
            "     15.76       0.30  0.02   Eagle_education_Jewell\n",
            "     35.89       0.31  0.01   Eagle_office_Mandi\n",
            "     36.93       0.08  0.00   Eagle_office_Lamont\n",
            "     43.43       5.31  0.12   Eagle_lodging_Blake\n",
            "     46.46       1.87  0.04   Eagle_education_Eileen\n",
            "     56.50      25.58  0.45   Eagle_office_Dallas\n",
            "     57.04       6.09  0.11   Eagle_education_Petra\n",
            "     62.02      10.93  0.18   Eagle_office_Sheree\n",
            "     81.96      19.79  0.24   Eagle_lodging_Edgardo\n",
            "     87.21       0.59  0.01   Eagle_office_Phyllis\n",
            "     91.27      22.83  0.25   Eagle_lodging_Trina\n",
            "     92.81      18.28  0.20   Eagle_lodging_Dawn\n",
            "    101.56      17.94  0.18   Eagle_office_Freida\n",
            "    103.18       2.90  0.03   Eagle_education_Shana\n",
            "    121.48       7.99  0.07   Eagle_office_Remedios\n",
            "    122.36      15.53  0.13   Eagle_health_Vincenza\n",
            "    148.73      28.15  0.19   Eagle_education_Teresa\n",
            "    165.88      88.31  0.53   Eagle_office_Patrice\n",
            "    174.64      69.31  0.40   Eagle_office_Tia\n",
            "    182.06      34.28  0.19   Eagle_public_Alvin\n",
            "    192.42      63.82  0.33   Eagle_health_Jodi\n",
            "    226.17      46.07  0.20   Eagle_education_Will\n",
            "    273.02     116.40  0.43   Eagle_office_Nereida\n",
            "    276.27     156.00  0.56   Eagle_food_Kay\n",
            "    336.49     203.79  0.61   Eagle_office_Francis\n",
            "    390.81     225.59  0.58   Eagle_office_Norbert\n",
            "    477.67     311.49  0.65   Eagle_health_Athena\n",
            "    659.51     757.34  1.15   Eagle_health_Gregoria\n",
            "    694.96     519.84  0.75   Eagle_education_Alberto\n",
            "    712.01     660.20  0.93   Eagle_education_Norah\n",
            "   1003.99    1859.76  1.85   Eagle_education_Shante\n",
            "   1037.64     931.83  0.90   Eagle_office_Chauncey\n",
            "   1084.43     924.37  0.85   Eagle_health_Reba\n",
            "   1199.38    1069.74  0.89   Eagle_education_Roman\n",
            "   1639.36    1633.23  1.00   Eagle_education_Brooke\n",
            "   2032.48    2078.01  1.02   Eagle_education_Sherrill\n",
            "   3154.83    3038.86  0.96   Eagle_education_Peter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}