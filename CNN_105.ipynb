{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH=''\n",
    "try:\n",
    "    # On Google Drive, set path to my drive / data directory.\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
    "except:\n",
    "    # On home computer, set path to local data directory.\n",
    "    IN_COLAB = False\n",
    "    DATAPATH='C:/'  # must end in \"/\"\n",
    "\n",
    "ZIP_FILE='BuildingData.zip'\n",
    "ZIP_PATH = DATAPATH+ZIP_FILE\n",
    "ELECT_FILE='electricity.csv'\n",
    "WEATHER_FILE='weather.csv'\n",
    "MODEL_FILE='Model'  # will be used later to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cheap-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.5.0rc0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "Requires: keras-preprocessing, tf-estimator-nightly, astunparse, protobuf, flatbuffers, six, absl-py, opt-einsum, grpcio, gast, wheel, wrapt, tensorboard, google-pasta, typing-extensions, numpy, keras-nightly, h5py, termcolor\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternative-frame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats  # mode\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deadly-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zip_to_panda(zip_filename,csv_filename):\n",
    "    zip_handle = ZipFile(zip_filename)\n",
    "    csv_handle = zip_handle.open(csv_filename)\n",
    "    panda = pd.read_csv(csv_handle)\n",
    "    return panda\n",
    "def fix_date_type(panda):\n",
    "    # Convert the given timestamp column to the pandas datetime data type.\n",
    "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
    "    indexed = panda.set_index(['timestamp'])\n",
    "    return indexed\n",
    "def get_site_timeseries(panda,site):\n",
    "    # Assume the panda dataframe has a datetime column.\n",
    "    # (If not, call fix_date_type() before this.)\n",
    "    # Extract the timeseries for one site.\n",
    "    # Convert the datetime column to a DatetimeIndex.\n",
    "    site_df = panda[panda['site_id']==site]\n",
    "    temp_col = site_df['date']\n",
    "    temp_val = temp_col.values\n",
    "    temp_ndx = pd.DatetimeIndex(temp_val)\n",
    "    dropped = site_df.drop('date',axis=1)\n",
    "    panda = dropped.set_index(temp_ndx)\n",
    "    return panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unauthorized-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CNN():\n",
    "    print(\"make_CNN\")\n",
    "    print(\"input shape:\",INPUT_SHAPE)\n",
    "    cnn = Sequential()\n",
    "    cnn.add(\n",
    "        Conv2D( input_shape=INPUT_SHAPE,\n",
    "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
    "            activation=None, padding=\"valid\"))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(STEPS_FUTURE))   \n",
    "    cnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-basin",
   "metadata": {},
   "source": [
    "## CNN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rising-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTORS= 8 ['cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n"
     ]
    }
   ],
   "source": [
    "# Before analyzing the entire dataset, we look at this subset.\n",
    "SITE = 'Eagle'\n",
    "METER = 'electricity'\n",
    "\n",
    "# Arrange \"picture\" of weather with temperatures toward the middle\n",
    "PREDICTED_VARIABLE = 'electricity' \n",
    "PREDICTORS = ['cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
    "print(\"PREDICTORS=\",len(PREDICTORS),PREDICTORS)\n",
    "\n",
    "# Downsample True means collapse 365*24 measures to 365 daily averages\n",
    "# Downsample False means replace 365*24 measures with 365*24 window averages\n",
    "DOWNSAMPLE = False   \n",
    "\n",
    "STEPS_HISTORY = 24   # length of the predictor sequence\n",
    "STEPS_FUTURE =  1    # length of the predicted sequence\n",
    "\n",
    "## CNN parameters\n",
    "EPOCHS=5\n",
    "FILTERS = 8\n",
    "WIDTH = 3\n",
    "STRIDE = (1,1)\n",
    "INPUT_SHAPE = (STEPS_HISTORY,len(PREDICTORS),1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "large-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
    "wet_df = fix_date_type(wet_df)\n",
    "elec_df = read_zip_to_panda(ZIP_PATH,ELECT_FILE)\n",
    "elec_df = fix_date_type(elec_df)\n",
    "site_specific_weather = wet_df.loc[wet_df['site_id'] == SITE]\n",
    "all_buildings = [x for x in elec_df.columns if x.startswith(SITE)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excited-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(df):\n",
    "    # For smoothing the 24 hour cycle, we do not want exponential smoothing.\n",
    "    smoothed = None\n",
    "    if DOWNSAMPLE:\n",
    "        # This alternate method samples down to 1/24 time steps.\n",
    "        smoothed = df.resample(\"24H\").mean() \n",
    "    else:\n",
    "        # This method does not reduce the number of time steps.\n",
    "        # Note the first 23 measurements get set to Nan.\n",
    "        smoothed=df.rolling(window=24).mean()\n",
    "        smoothed=smoothed[24:]\n",
    "    return smoothed\n",
    "\n",
    "# Correlation is low when buildings have many NaN and 0 meter readings.\n",
    "# We will ignore buildings that have >max bad meter readings.\n",
    "def is_usable_column(df,column_name):\n",
    "    MAX_BAD = 500 \n",
    "    bad = df[column_name].isin([0]).sum()\n",
    "    return bad<=MAX_BAD\n",
    "\n",
    "def prepare_for_learning(df):\n",
    "    # This is very slow. Is there a faster way? See...\n",
    "    # https://stackoverflow.com/questions/27852343/split-python-sequence-time-series-array-into-subsequences-with-overlap\n",
    "    # This didn't work: np.atleast_1d(p) \n",
    "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
    "    num_predictors = len(PREDICTORS)\n",
    "    X_shape = (num_samples,STEPS_HISTORY,num_predictors,1)\n",
    "    X=np.zeros(X_shape)\n",
    "    Y_shape = (num_samples,1)\n",
    "    y=np.zeros(Y_shape)\n",
    "    predictor_series = df[PREDICTORS].values  # e.g. all weather values\n",
    "    predicted_series = df[PREDICTED_VARIABLE].values  # e.g. all meter readings\n",
    "    \n",
    "    for x0 in range (0,num_samples): # Loop over all 1000 samples\n",
    "        # This is one array of weather for previous 24 time periods\n",
    "        one_sample = predictor_series[x0:x0+STEPS_HISTORY]\n",
    "        one_label =  predicted_series[x0+STEPS_HISTORY:x0+STEPS_FUTURE]\n",
    "        # Loop over all 24 time periods\n",
    "        for x1 in range (0,STEPS_HISTORY): # In 1 sample, loop over 24 time periods\n",
    "            one_period = one_sample[x1]\n",
    "            for x2 in range (0,num_predictors): # In 1 time period, loop over 8 weather metrics\n",
    "                one_predictor = one_period[x2]\n",
    "                # for x3 in range (0,X_shape[3]): # In 1 metric, loop over vector dimensions\n",
    "                # In our data, each weather metric is a scalar.\n",
    "                x3 = 0\n",
    "                X[x0,x1,x2,x3] = one_predictor\n",
    "    return X,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "married-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CNN():\n",
    "    print(\"make_CNN\")\n",
    "    print(\"input shape:\",INPUT_SHAPE)\n",
    "    cnn = Sequential()\n",
    "    cnn.add(\n",
    "        Conv2D( input_shape=INPUT_SHAPE,\n",
    "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
    "            activation=None, padding=\"valid\"))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(STEPS_FUTURE))   \n",
    "    cnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-scroll",
   "metadata": {},
   "source": [
    "## Process all buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "freelance-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Eagle_office_Lamont\n",
      "Building Eagle_health_Athena\n",
      "Building Eagle_office_Ryan\n",
      "Train on 8747 samples...\n",
      "make_CNN\n",
      "input shape: (24, 8, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'unique_object_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1189734a5283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train on\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"samples...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_train.shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f8d09358c501>\u001b[0m in \u001b[0;36mmake_CNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"make_CNN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mINPUT_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     cnn.add(\n\u001b[0;32m      6\u001b[0m         Conv2D( input_shape=INPUT_SHAPE,\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \"\"\"\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# Skip the init in FunctionalModel since model doesn't have input/output yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\n\u001b[0m\u001b[0;32m    110\u001b[0m         name=name, autocast=False)\n\u001b[0;32m    111\u001b[0m     \u001b[0mbase_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sequential'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;34m'trainable'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dynamic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'autocast'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inputs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'outputs'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     })\n\u001b[1;32m--> 245\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;31m# By default, Model is a subclass model, which is not in graph network.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_supports_masking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_set_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m     self._activity_regularizer = regularizers.get(\n\u001b[0;32m    352\u001b[0m         kwargs.pop('activity_regularizer', None))\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_init_set_name\u001b[1;34m(self, name, zero_based)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# visible class. Use \"Model\" instead,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mcls_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Model'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m       self._name = backend.unique_object_name(\n\u001b[0m\u001b[0;32m    502\u001b[0m           \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_snake_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m           zero_based=zero_based)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'unique_object_name'"
     ]
    }
   ],
   "source": [
    "cors = []\n",
    "ONE_PREDICTOR = 'dewTemperature'  ## illustrate difficulty by showing best correlate\n",
    "for BLDG in all_buildings:\n",
    "    print(\"Building\",BLDG)\n",
    "    # Get steam usage for one building.\n",
    "    bldg_specific_elect = elec_df[[BLDG]]\n",
    "    # Concatenate steam usage with weather.\n",
    "    one_bldg_df = pd.concat([bldg_specific_elect,site_specific_weather],axis=1)\n",
    "    # Drop the site, which is constant (we selected for one site).\n",
    "    one_bldg_df = one_bldg_df.drop(['site_id'],axis=1)\n",
    "    # The original steam table used column name = building name.\n",
    "    # We are processing one building, so rename to the column 'steam'.\n",
    "    one_bldg_df = one_bldg_df.rename(columns={BLDG : METER})\n",
    "    # In order to filter bad buildings, count sum of NaN + zero.\n",
    "    one_bldg_df = one_bldg_df.fillna(0)\n",
    "    \n",
    "    if is_usable_column(one_bldg_df,METER):\n",
    "        one_bldg_df = smooth(one_bldg_df) \n",
    "        X,y = prepare_for_learning(one_bldg_df)\n",
    "        # Ideally, split Year1 = train, Year2 = test.\n",
    "        # Some data is incomplete, so split 1st half and 2nd half.\n",
    "        split = len(X)//2 \n",
    "        X_train = np.asarray(X[0:split])\n",
    "        y_train = np.asarray(y[0:split])\n",
    "        X_test = np.asarray(X[split:])\n",
    "        y_test = np.asarray(y[split:])\n",
    "        print(\"Train on\",len(X_train),\"samples...\")\n",
    "        model = make_CNN()\n",
    "        print(model.summary())\n",
    "        print(\"X_train.shape:\",X_train.shape)\n",
    "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Compare. Solve the problem that predict.shape != truth.shape \n",
    "        #print(\" before ytestshape\",y_test.shape,\"ypredshape\",y_pred.shape)\n",
    "        #nsamples, nsteps, ndim = y_test.shape\n",
    "        #y_test = y_test.reshape(nsamples,nsteps*ndim)\n",
    "        #nsamples, nsteps, ndim = y_pred.shape\n",
    "        #y_pred = y_pred.reshape(nsamples,nsteps*ndim)\n",
    "        ##print(\" after ytestshape\",y_test.shape,\"ypredshape\",y_pred.shape)\n",
    "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
    "        # Keep a table for reporting later.\n",
    "        mean = one_bldg_df[METER].mean()\n",
    "        cor = one_bldg_df.corr().loc[PREDICTED_VARIABLE][ONE_PREDICTOR] \n",
    "        cors.append([cor,mean,rmse,rmse/mean,BLDG])\n",
    "        print(\"cor,mean,rmse,rmse/mean,bldg:\",cor,mean,rmse,rmse/mean,BLDG)\n",
    "\n",
    "        ## break   ## REMOVE THIS LINE TO LOOP OVER BUILDINGS!\n",
    "        \n",
    "if True:\n",
    "    print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
    "    print(\"Column 1: Correlation of\",PREDICTED_VARIABLE,\"and\",ONE_PREDICTOR)\n",
    "    print(\"          Using one weather feature as leading correlate.\")\n",
    "    print(\"Column 2: Mean usage.\")\n",
    "    print(\"          Using mean to help understand the RMSE.\")\n",
    "    print(\"Column 3: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
    "    print(\"Column 4: RMSE/mean normalized to help understand RMSE.\")\n",
    "    print(\"Column 5: Building.\")\n",
    "    for cor in sorted(cors):\n",
    "        print(\"%7.4f %10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3],cor[4]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-style",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
