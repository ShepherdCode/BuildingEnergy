{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CR_233.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# CNN + RNN \n",
        "Compare to RNN 229.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "0757af34-1c22-41e3-9ca5-2a2476cc2764"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Eagle'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "# For our CNN, min predictive features is six.\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_HISTORY = 24\n",
        "STEPS_FORWARD = 12 \n",
        "STEPS_FUTURE =  12 \n",
        "METER_FILE='steam.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3\n",
        "SCALING=1\n",
        "CELLS = 16\n",
        "FILTERS = 16\n",
        "WIDTH = 3\n",
        "STRIDE = (1,1)\n",
        "INPUT_SHAPE = (STEPS_FORWARD,NUM_PREDICTORS,1) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 12 ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "d12c36fc-ccd4-4fbd-bd1a-5f519daa3470"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvXGAH2wzBWS"
      },
      "source": [
        "def scale(df):\n",
        "    scaler=StandardScaler()\n",
        "    #scaler=MinMaxScaler()\n",
        "    scaled=scaler.fit_transform(df.values)\n",
        "    scaled = pd.DataFrame(scaled,index=df.index,columns=df.columns)\n",
        "    return scaled"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "0b9a4a62-2244-4fc4-8654-2e0c112645b4"
      },
      "source": [
        "DATE_PARSE=True  # must be true if we use one of these as predictor\n",
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    if DATE_PARSE:\n",
        "        site_df.insert(0,'hour',0)\n",
        "        site_df.insert(1,'month',0)\n",
        "        site_df.insert(2,'doy',0)\n",
        "        L=len(site_df)\n",
        "        for i in range(0,L):\n",
        "            dt=site_df.index[i]\n",
        "            hour=dt.hour\n",
        "            month=dt.month\n",
        "            doy=dt.dayofyear\n",
        "            site_df.iat[i,0] = hour\n",
        "            site_df.iat[i,1] = month\n",
        "            site_df.iat[i,2] = doy\n",
        "    #if SCALING==1:\n",
        "    #    site_df = scale(site_df) # could break if any column is empty\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 18:00:00</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1026.2</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.2</td>\n",
              "      <td>310.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>330.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>320.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 18:00:00    18     12  365  ...          1026.2          330.0        2.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1027.0          320.0        1.5\n",
              "2017-12-31 21:00:00    21     12  365  ...          1027.2          310.0        2.6\n",
              "2017-12-31 22:00:00    22     12  365  ...          1027.4          330.0        3.1\n",
              "2017-12-31 23:00:00    23     12  365  ...          1027.4          320.0        4.6\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "7d4ba9b5-b1a1-48dc-af3c-58c4186a73fa"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>92.2957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>277.5584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>280.5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>289.3302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>164.3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        meter\n",
              "timestamp                    \n",
              "2017-12-31 19:00:00   92.2957\n",
              "2017-12-31 20:00:00  277.5584\n",
              "2017-12-31 21:00:00  280.5331\n",
              "2017-12-31 22:00:00  289.3302\n",
              "2017-12-31 23:00:00  164.3474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "f9733f76-8692-451b-816c-dc6e33ac68cc"
      },
      "source": [
        "# Make X out of weather + meter features, then select wanted columns.\n",
        "# Make y out of meter features, then select meter column only.\n",
        "# Apply scaler to X only.\n",
        "# Pull out time steps for X from the past: STEPS_HISTORY.\n",
        "# Pull out proper number of time steps for X: STEPS_FORWARD.\n",
        "# Pull out proper number of time steps for y: STEPS_FUTURE.\n",
        "# Make X inefficiently with 3-deep nested for loop because\n",
        "# a) want to replace NaN with previous value\n",
        "# b) need to add the RGB dimension for CNN.\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    if SCALING==1:\n",
        "        df = scale(df) # could break if any column is empty\n",
        "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = mdf[PREDICTED_VARIABLE].values  # meter\n",
        "    #\n",
        "    X_shape = (num_samples,STEPS_FUTURE,NUM_PREDICTORS,1) # RGB = 1\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        one_sample = predictor_series[sam:sam+STEPS_FORWARD]\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = one_sample[time]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat,0] = val  # RGB dim = 0\n",
        "        for time in range (0,STEPS_FUTURE):  \n",
        "            y[sam,time]=predicted_series[sam+STEPS_HISTORY+time]\n",
        "    return X,y \n",
        "print(one_bldg_meter.head())\n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       meter\n",
            "timestamp                   \n",
            "2016-01-01 00:00:00  31.7661\n",
            "2016-01-01 01:00:00  27.4004\n",
            "2016-01-01 02:00:00  38.4989\n",
            "2016-01-01 03:00:00  59.1697\n",
            "2016-01-01 04:00:00  39.9556\n",
            "X shape: (17508, 12, 12, 1)\n",
            "y shape: (17508, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObWmpMDVuNQ",
        "outputId": "d948cfc2-1616-4fea-c830-a3757a798605"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
            "X example:\n",
            " [[[-1]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 1]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [-1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]]\n",
            "y example:\n",
            " [ 43 119 327 322 273  92 328 363 346 168 266  27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_DNN():\n",
        "    print(\"make_DNN\")\n",
        "    print(\"input shape:\",INPUT_SHAPE)\n",
        "    dnn = Sequential()\n",
        "    dnn.add(Conv2D( input_shape=INPUT_SHAPE,\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(Conv2D(\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(MaxPooling2D())\n",
        "    dnn.add(TimeDistributed(Flatten()))\n",
        "    #dnn.add(GRU(CELLS,return_sequences=True))\n",
        "    dnn.add(GRU(CELLS,return_sequences=False))\n",
        "    dnn.add(Dense(STEPS_FUTURE))   \n",
        "    dnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    dnn.build(input_shape=INPUT_SHAPE)\n",
        "    return dnn    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "0f3f0438-512d-49c6-e5f1-db7e1d96ab25"
      },
      "source": [
        "cors = []\n",
        "overall = 0\n",
        "cnt = 0\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",BLDG)\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # Smoothing window applies to inputs\n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        # Smoothing does not apply to truth\n",
        "        one_bldg_meter = load_meter_for_building(BLDG,0)\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        X_raw,y_raw = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        y_test = np.asarray(y_raw[split:])\n",
        "        # Train and predict\n",
        "        model = make_DNN()\n",
        "        print(model.summary())\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Reporting\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        cnt += 1\n",
        "        print(\"i,mean,rmse,rmse/mean,bldg:\",cnt,mean,rmse,rmse/mean,BLDG)\n",
        "        overall += rmse/mean\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Eagle_office_Lamont\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_78 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_39 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_78 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [58 64 68 72 72 72 74 73 71 69 70 71]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 1235.1109\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 932.4208\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 732.6863\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 573.4416\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 447.9436\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 372.7788\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 314.8485\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 279.6457\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 255.0278\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 235.0408\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 232.5219\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 236.5331\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 227.2521\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 228.4378\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 223.1248\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 223.7137\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 227.4353\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 192.3150\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 121.1891\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 97.4462\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 86.6899\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 81.9124\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 76.9707\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 67.5321\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 67.6694\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 64.3260\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 63.2497\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 57.4802\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 54.0733\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 51.3450\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 51.1252\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 48.9006\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 46.6910\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 46.3693\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43.6900\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 44.8981\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 41.0450\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.6744\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38.8967\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.4337\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 39.4867\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 39.1754\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 38.1452\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38.6871\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38.9496\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 37.6193\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39.2251\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36.6230\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36.9956\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34.9640\n",
            "i,mean,rmse,rmse/mean,bldg: 1 36.92671326723864 9.706858644927504 0.2628681999039276 Eagle_office_Lamont\n",
            "Example prediction:\n",
            " 0 [38 38 38 38 38 38 38 38 38 39 38 39]\n",
            "Example prediction:\n",
            " 2 [37 37 37 37 37 37 37 37 37 38 37 38]\n",
            "Example prediction:\n",
            " 4 [28 28 28 28 28 28 28 28 28 28 28 28]\n",
            "Example prediction:\n",
            " 6 [31 31 31 31 31 31 31 30 30 31 31 31]\n",
            "Example prediction:\n",
            " 8 [32 31 31 32 32 31 32 31 31 32 32 32]\n",
            "Example prediction:\n",
            " 10 [41 41 41 41 41 40 40 40 40 40 40 40]\n",
            "Example prediction:\n",
            " 12 [41 41 41 41 41 40 40 40 40 40 40 40]\n",
            "Example prediction:\n",
            " 14 [41 41 41 41 41 41 41 40 40 40 40 40]\n",
            "Example prediction:\n",
            " 16 [40 40 40 39 39 39 39 39 39 39 39 39]\n",
            "Example prediction:\n",
            " 18 [39 39 40 39 39 39 39 39 39 39 39 39]\n",
            "Example prediction:\n",
            " 20 [39 39 39 39 39 39 39 38 39 39 38 39]\n",
            "Example prediction:\n",
            " 22 [38 38 38 38 38 38 38 38 38 38 38 38]\n",
            "Building Eagle_health_Athena\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_80 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_81 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_40 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_79 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1341 1348 1320 1187 1052  960  834  819  801  999 1154 1250]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 350967.0490\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 343154.8566\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 347015.9866\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 339852.8832\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 326604.3523\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 325536.9631\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 325345.1085\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 319137.4481\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 314256.9869\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 312365.1836\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 306658.0833\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 299663.7209\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 296230.1080\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 284872.2777\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 286505.6951\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 284302.4849\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 284336.2082\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 277414.9681\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 263044.5099\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 269792.2238\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 263559.8918\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 261605.0409\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 256592.0525\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 249076.6932\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 247812.4943\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 245677.6425\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 241329.3365\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 240435.4447\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 236709.2744\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 228652.4448\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 232318.3595\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 222423.9261\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 220846.5102\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 217967.2484\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 210443.9783\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 208092.5199\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204915.5844\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204266.5326\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 195802.2036\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 193237.7261\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 192333.7161\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 188434.2913\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 186248.8337\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 178491.6006\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 182012.6761\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 179115.3316\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 172191.8592\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 172864.3091\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 168531.3451\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 163966.0724\n",
            "i,mean,rmse,rmse/mean,bldg: 2 477.70168061445776 325.63785044837135 0.6816761666601426 Eagle_health_Athena\n",
            "Example prediction:\n",
            " 0 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 2 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 4 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 6 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 8 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 10 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 12 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 14 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 16 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 18 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 20 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Example prediction:\n",
            " 22 [226 224 224 226 227 224 224 225 224 225 224 224]\n",
            "Building Eagle_assembly_Herbert\n",
            "Building Eagle_public_Alvin\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_82 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_83 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_41 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_80 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [560 549 468 422 314 298 296 434 517 515 470 450]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 56068.1252\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 54579.2248\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 51994.1547\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 50481.2458\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 48778.3463\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 46562.5361\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 44209.9060\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43804.3526\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42132.3857\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 40175.3510\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 39022.1374\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38226.1534\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35695.7157\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35163.1379\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33761.7555\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32502.4823\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31812.2371\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29972.1550\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29521.6573\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29016.3291\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 27250.5146\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25873.9351\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25849.6427\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24741.3804\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24409.3527\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22891.3916\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22208.0200\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21696.9938\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20696.9777\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20389.9911\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18969.0679\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18570.7748\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18530.2859\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17508.4007\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17096.9024\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16701.7931\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16242.1080\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15402.0774\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15062.1197\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15230.6161\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14223.4276\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14546.4442\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14419.2754\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14327.6401\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14095.9689\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13767.9468\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13182.0034\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13060.1292\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13343.5137\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13115.7403\n",
            "i,mean,rmse,rmse/mean,bldg: 3 182.08293646830808 81.62762934631074 0.44829917030977917 Eagle_public_Alvin\n",
            "Example prediction:\n",
            " 0 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 2 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 4 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 6 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 8 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 10 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 12 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 14 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 16 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 18 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 20 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Example prediction:\n",
            " 22 [192 193 193 193 192 192 193 193 193 192 192 192]\n",
            "Building Eagle_education_Raul\n",
            "Building Eagle_education_Roman\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_84 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_85 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_42 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_81 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [2589 2505 2475 2437 2673 2987 3111 2693 2817 3019 3655 3750]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 3ms/step - loss: 1711185.7577\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1693310.1936\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1682556.2527\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1671758.7109\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1671620.8200\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1649026.6727\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1639305.8182\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1631516.3445\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1583661.5386\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1568556.6586\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1595887.8009\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1609867.2941\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1577186.0768\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1554830.4436\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1538009.7205\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1551006.0768\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1554469.3573\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1532181.8518\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1517108.6695\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1494061.8032\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1500874.6345\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1475149.6950\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1468252.5755\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1470191.8923\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1434488.3800\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1453612.3014\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1408477.2720\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1425851.8227\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1416196.1855\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1411271.5018\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1379220.3350\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1387312.1777\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1374110.5391\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1359968.1218\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1313173.8216\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1353583.0705\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1341204.3632\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1327516.3809\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1322369.9055\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1338966.9223\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1266239.2389\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1281673.0177\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1262097.0355\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1259976.9682\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1272230.6341\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1251760.3586\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1256342.4468\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1224221.5250\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1243500.4773\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1231041.9209\n",
            "i,mean,rmse,rmse/mean,bldg: 4 1199.4083427425896 1085.020846532821 0.9046300645631595 Eagle_education_Roman\n",
            "Example prediction:\n",
            " 0 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 2 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 4 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 6 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 8 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 10 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 12 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 14 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 16 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 18 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 20 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Example prediction:\n",
            " 22 [230 230 230 230 229 232 228 230 231 231 232 231]\n",
            "Building Eagle_office_Mandi\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_86 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_43 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_82 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [63 63 63 63 63 63 63 63 63 63 63 63]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 1497.2478\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1169.9667\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 956.9034\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 760.7973\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 593.8542\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 507.1050\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 407.2387\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 327.8383\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 288.7100\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 240.4755\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 195.7840\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 164.6350\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 144.6105\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 122.8180\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 109.1510\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 92.7399\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 84.0539\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 76.8540\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 71.0864\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 66.1685\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 60.7769\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 58.5125\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 57.2403\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56.5263\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53.5462\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50.5786\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 51.2602\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 47.8269\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 49.5828\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 45.6804\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41.2865\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.2050\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 41.1482\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 38.6790\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37.8563\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41.7997\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39.4113\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 37.4448\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36.6765\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36.3028\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35.2628\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37.4308\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34.7669\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34.2403\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36.3962\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34.2257\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35.6662\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34.0862\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33.6149\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33.8763\n",
            "i,mean,rmse,rmse/mean,bldg: 5 35.89375163018761 9.898145901985805 0.27576236677531357 Eagle_office_Mandi\n",
            "Example prediction:\n",
            " 0 [56 56 56 57 56 57 56 57 56 56 56 56]\n",
            "Example prediction:\n",
            " 2 [56 57 57 57 57 57 57 57 57 56 56 56]\n",
            "Example prediction:\n",
            " 4 [57 58 58 58 58 58 58 58 58 57 57 57]\n",
            "Example prediction:\n",
            " 6 [57 57 57 58 58 57 57 57 57 57 57 57]\n",
            "Example prediction:\n",
            " 8 [55 55 56 56 56 56 56 55 56 55 55 55]\n",
            "Example prediction:\n",
            " 10 [53 53 53 53 53 53 54 53 53 53 53 53]\n",
            "Example prediction:\n",
            " 12 [52 53 53 53 53 53 53 53 53 53 53 53]\n",
            "Example prediction:\n",
            " 14 [51 52 52 52 52 52 52 52 52 52 52 52]\n",
            "Example prediction:\n",
            " 16 [48 48 48 48 48 49 49 49 49 49 49 49]\n",
            "Example prediction:\n",
            " 18 [46 47 47 47 47 47 48 48 47 48 47 48]\n",
            "Example prediction:\n",
            " 20 [40 41 40 40 41 41 41 41 41 41 41 41]\n",
            "Example prediction:\n",
            " 22 [41 41 41 41 41 41 41 41 41 41 41 41]\n",
            "Building Eagle_education_Jewell\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_88 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_44 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_83 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [223 207 206 207 205 205 196 203 210 217 222 222]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 4210.4682\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3914.8774\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3705.5258\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3436.6526\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3547.4979\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3226.6587\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3033.5212\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2872.0605\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2760.8966\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2696.5954\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2505.4348\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2519.6767\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2349.2325\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2065.0871\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2055.2152\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1946.7724\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1836.3735\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1706.6402\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1735.7393\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1640.9683\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1594.7546\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1557.5233\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1490.6493\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1391.2345\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1325.2270\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1225.6654\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1188.0256\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1162.1630\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1103.5514\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1065.8218\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1056.1721\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 955.7775\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 951.8833\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 918.0646\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 837.5185\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 788.8208\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 804.0128\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 765.3782\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 735.3975\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 711.7211\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 651.7940\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 648.0612\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 596.3924\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 590.0344\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 587.4939\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 555.8260\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 541.0260\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 527.2448\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 521.7391\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 506.6902\n",
            "i,mean,rmse,rmse/mean,bldg: 6 15.763073324212357 43.69735902610631 2.772134477036683 Eagle_education_Jewell\n",
            "Example prediction:\n",
            " 0 [21 20 20 20 20 19 19 19 20 20 20 20]\n",
            "Example prediction:\n",
            " 2 [17 16 16 16 16 16 15 16 16 17 16 16]\n",
            "Example prediction:\n",
            " 4 [19 18 18 18 18 17 17 17 18 18 18 18]\n",
            "Example prediction:\n",
            " 6 [15 14 14 14 14 14 13 14 14 15 14 14]\n",
            "Example prediction:\n",
            " 8 [14 13 13 13 13 12 12 12 13 13 14 14]\n",
            "Example prediction:\n",
            " 10 [31 29 31 31 30 31 29 31 31 30 30 30]\n",
            "Example prediction:\n",
            " 12 [33 31 33 34 33 34 32 32 33 33 32 32]\n",
            "Example prediction:\n",
            " 14 [46 45 47 48 47 48 46 46 46 45 44 45]\n",
            "Example prediction:\n",
            " 16 [65 64 66 67 66 67 65 65 65 64 64 64]\n",
            "Example prediction:\n",
            " 18 [69 68 70 70 69 70 69 68 68 68 67 68]\n",
            "Example prediction:\n",
            " 20 [76 76 78 78 77 77 77 76 76 75 75 76]\n",
            "Example prediction:\n",
            " 22 [57 56 57 57 56 57 56 57 57 56 56 56]\n",
            "Building Eagle_office_Henriette\n",
            " Count bad values before pseudofill: 162\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_90 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_91 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_45 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_84 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 0.0162\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.9813e-04\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.2461e-04\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6.4285e-05\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1033e-05\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.8557e-05\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.9621e-05\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.3507e-05\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.1557e-05\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7.5564e-06\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.6912e-06\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.9910e-06\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.2484e-06\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.7360e-06\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.3950e-06\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.0429e-06\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.9381e-06\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.1430e-06\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8.7127e-06\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.8470e-06\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.1700e-06\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.6924e-06\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6.4976e-06\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.3015e-06\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.2484e-06\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.2220e-06\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.2689e-06\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.2173e-06\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.3116e-06\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4385e-07\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6.6665e-07\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.5992e-06\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.0779e-06\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.2284e-07\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.3063e-07\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7.1926e-07\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.7744e-07\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.4784e-06\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7.8044e-07\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4776e-07\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.6286e-07\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2.9277e-07\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8.1362e-07\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7.5677e-07\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.8810e-07\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9.5393e-08\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7.1151e-08\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1.9828e-07\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.5895e-07\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9.3078e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i,mean,rmse,rmse/mean,bldg: 7 0.0 0.0005283642390658173 inf Eagle_office_Henriette\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building Eagle_health_Reba\n",
            " Count bad values before pseudofill: 36\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_92 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_46 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_85 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1765 1738 1266 1758 2276 3012 2801 2550 2612 2622 2627 2592]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 1450554.7168\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1423479.5427\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1442042.1709\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1433387.5673\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1406130.1093\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1398038.1400\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1413096.9745\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1368563.0577\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1379307.5027\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1348859.0364\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1358835.0827\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1316191.5691\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1320231.4227\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1329561.9895\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1318991.3195\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1288356.2845\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1297493.6036\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1288764.2282\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1282244.2936\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1277456.8823\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1275297.3655\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1254509.5759\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1219854.5175\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1229589.2573\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1221813.0982\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1230097.2995\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1202883.6959\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1191305.6220\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1216836.6464\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1175602.0495\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1149596.8468\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1152855.6505\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1146307.4505\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1149296.2905\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1120682.3123\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1122579.8945\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1113116.2134\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1096819.0055\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1097877.9159\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1095476.0959\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1090847.7445\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1071878.1827\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1036118.4236\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1046692.4643\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1024117.2270\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1045014.0086\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1031020.8930\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1046397.3132\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1018390.6098\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1005661.9989\n",
            "i,mean,rmse,rmse/mean,bldg: 8 1084.2856908630224 927.8228358633834 0.8556996036025297 Eagle_health_Reba\n",
            "Example prediction:\n",
            " 0 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 2 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 4 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 6 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 8 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 10 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 12 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 14 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 16 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 18 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 20 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Example prediction:\n",
            " 22 [230 231 231 230 231 229 229 231 228 230 229 231]\n",
            "Building Eagle_lodging_Edgardo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_94 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_47 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_86 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [203 120 203 174 176 184 226 258 235 184 243 259]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 9770.9407\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8970.1998\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8334.7462\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7826.4390\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7214.0953\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6842.0989\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6219.1476\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5851.7092\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5605.6628\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5351.8513\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4915.6445\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4672.9026\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4515.2701\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4195.9378\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4034.1639\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3892.0603\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3671.8335\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3618.0546\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3637.4576\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3574.4903\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3420.6260\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3417.7544\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3092.6947\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2942.9640\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2842.6819\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2632.8055\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2635.1292\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2477.5680\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2459.3036\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2296.7672\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2329.4739\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2199.3767\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2193.0681\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2120.4461\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2048.1460\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1991.7054\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1968.8162\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1996.1559\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1931.0521\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1838.0298\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1797.2199\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1872.5100\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1844.7058\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1782.3560\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1713.5354\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1708.0185\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1711.4059\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1686.8552\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1706.8283\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1656.7872\n",
            "i,mean,rmse,rmse/mean,bldg: 9 81.9677919573641 68.5548386083407 0.8363631247258655 Eagle_lodging_Edgardo\n",
            "Example prediction:\n",
            " 0 [135 135 135 135 135 136 135 135 135 135 135 134]\n",
            "Example prediction:\n",
            " 2 [101 101 102 101 101 101 101 102 101 101 101 101]\n",
            "Example prediction:\n",
            " 4 [79 79 80 80 79 79 80 80 79 79 80 80]\n",
            "Example prediction:\n",
            " 6 [47 46 47 47 46 45 46 46 46 46 47 48]\n",
            "Example prediction:\n",
            " 8 [77 77 77 77 77 77 78 77 77 78 78 77]\n",
            "Example prediction:\n",
            " 10 [108 109 109 109 109 109 110 110 109 109 110 109]\n",
            "Example prediction:\n",
            " 12 [101 102 101 103 102 102 103 102 102 103 103 103]\n",
            "Example prediction:\n",
            " 14 [101 102 101 104 102 102 103 102 102 103 103 103]\n",
            "Example prediction:\n",
            " 16 [124 125 124 125 125 125 125 125 124 125 125 124]\n",
            "Example prediction:\n",
            " 18 [117 117 117 118 117 118 118 117 117 118 118 117]\n",
            "Example prediction:\n",
            " 20 [117 118 118 118 118 119 118 118 118 118 118 118]\n",
            "Example prediction:\n",
            " 22 [117 118 118 118 118 119 118 118 118 119 119 118]\n",
            "Building Eagle_education_Cassie\n",
            "Building Eagle_education_Peter\n",
            " Count bad values before pseudofill: 34\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_96 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_48 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_87 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [7938 7923 8023 8046 8049 8002 8064 8097 8187 8160 8224 8161]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 12874708.4218\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12996993.4545\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12921372.6691\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13020504.1891\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12862755.5491\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12868766.4473\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12975009.7745\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12852055.3745\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12770642.8800\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12643246.2073\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12589365.0364\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12656200.9164\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12642905.1164\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12701955.8073\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12822967.2618\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12438793.1855\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12489809.6982\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12505752.5564\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12571690.6655\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12413402.2109\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12413499.3927\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12681563.5855\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12615964.9273\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12191033.6473\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12658503.5964\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12221497.8036\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12386654.1236\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12382099.1964\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12116668.1236\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12270941.2945\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12070299.4182\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12065678.4836\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12055488.5855\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11975620.9309\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11924949.9382\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12069202.8109\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11982323.0727\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11669391.5600\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12087496.8145\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11935369.9091\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11908398.7709\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11680119.2491\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11732230.4436\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11553715.9127\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11594783.8182\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11487161.5273\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11676755.1527\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11649132.9273\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11445453.7055\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11696956.9673\n",
            "i,mean,rmse,rmse/mean,bldg: 10 3153.2853604405936 3042.9948387139443 0.9650236153345668 Eagle_education_Peter\n",
            "Example prediction:\n",
            " 0 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 2 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 4 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 6 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 8 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 10 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 12 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 14 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 16 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 18 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 20 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Example prediction:\n",
            " 22 [232 233 232 233 233 232 233 233 234 232 232 232]\n",
            "Building Eagle_health_Gregoria\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_98 (Conv2D)           (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_49 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_88 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 3ms/step - loss: 661639.6873\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 652715.8364\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 622076.1902\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 623116.7119\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 616907.9370\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 648283.7109\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 617953.5077\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 590887.2332\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 585052.5317\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 623825.0856\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 579847.4551\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 600283.3148\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 590269.8934\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 608561.6741\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 591663.3111\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 595303.0525\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 573129.6112\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 577442.9103\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 576820.7474\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 592081.6768\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 577647.6823\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 557808.2333\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 569103.3466\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 556847.8626\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 537914.3682\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 533420.3302\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 579023.0586\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 535408.2426\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 558809.4375\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 504685.0567\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 531751.2939\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 523152.3602\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 524849.0986\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 526929.5537\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 493743.6286\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 519943.5466\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 498196.1819\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 509712.5591\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 519800.0580\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 502360.9024\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 503922.3265\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 496757.7972\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 481243.5283\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 496886.9984\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 466646.8627\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 497213.0041\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 468134.6034\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 476131.0066\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 469832.8143\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 471189.2208\n",
            "i,mean,rmse,rmse/mean,bldg: 11 659.65505791154 789.1984393523562 1.1963804868728651 Eagle_health_Gregoria\n",
            "Example prediction:\n",
            " 0 [65 65 66 66 66 62 65 66 66 66 64 65]\n",
            "Example prediction:\n",
            " 2 [52 52 53 53 53 50 52 53 53 53 51 52]\n",
            "Example prediction:\n",
            " 4 [ 0  0  0  0  0 -1  0  0  0  0  0  0]\n",
            "Example prediction:\n",
            " 6 [ 0  0  0  0  0 -1  0  0  0  0 -1 -1]\n",
            "Example prediction:\n",
            " 8 [ 0  0  0  0  0 -1  0  0  0  0  0  0]\n",
            "Example prediction:\n",
            " 10 [136 136 136 135 136 133 136 135 136 136 133 134]\n",
            "Example prediction:\n",
            " 12 [201 200 200 199 199 199 200 199 199 200 198 199]\n",
            "Example prediction:\n",
            " 14 [210 208 208 208 208 207 209 207 208 208 207 207]\n",
            "Example prediction:\n",
            " 16 [133 133 134 133 133 130 133 132 133 133 130 131]\n",
            "Example prediction:\n",
            " 18 [1 1 1 1 1 0 1 1 1 1 0 0]\n",
            "Example prediction:\n",
            " 20 [ 0  0  0  0  0 -1  0  0  0  0  0  0]\n",
            "Example prediction:\n",
            " 22 [ 0  0  1  1  0 -1  1  1  1  1  0  0]\n",
            "Building Eagle_lodging_Dawn\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_100 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_50 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_89 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [305 286 264 281 322 322 322 293 305 300 312 293]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 13548.9574\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12954.4512\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11965.2839\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11019.3481\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10374.2077\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9661.9068\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8902.7138\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8533.6560\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8031.2019\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7574.0331\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7034.1244\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6424.8438\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6139.2234\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5634.9531\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5594.7133\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5076.7385\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4907.3910\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4683.0050\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4377.4012\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4305.3822\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4015.0202\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4021.7596\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3909.6397\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3742.1278\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3564.7816\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3540.8920\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3474.7529\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3189.5278\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2972.2207\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2889.0804\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2750.4303\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2701.7923\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2458.0696\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2423.0891\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2390.8025\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2361.2510\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2108.7035\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2093.3786\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2139.2222\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1988.7750\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1904.1843\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1862.9829\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1777.5154\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1789.8989\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1776.2832\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1748.1723\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1636.1069\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1584.6187\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1555.7314\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1459.5919\n",
            "i,mean,rmse,rmse/mean,bldg: 12 92.82705527815749 57.100529414499476 0.6151280921644772 Eagle_lodging_Dawn\n",
            "Example prediction:\n",
            " 0 [81 81 82 82 83 81 82 81 83 83 81 82]\n",
            "Example prediction:\n",
            " 2 [59 59 60 60 60 60 60 59 60 61 60 61]\n",
            "Example prediction:\n",
            " 4 [58 58 59 59 59 58 59 58 59 59 58 59]\n",
            "Example prediction:\n",
            " 6 [50 50 51 51 50 50 50 50 51 51 50 51]\n",
            "Example prediction:\n",
            " 8 [77 77 77 77 76 77 77 77 78 77 78 78]\n",
            "Example prediction:\n",
            " 10 [112 112 114 114 114 112 113 112 113 113 112 112]\n",
            "Example prediction:\n",
            " 12 [115 115 117 117 117 115 116 115 116 116 115 115]\n",
            "Example prediction:\n",
            " 14 [107 108 109 109 109 107 108 108 109 108 108 108]\n",
            "Example prediction:\n",
            " 16 [105 107 108 108 108 106 106 106 107 107 106 106]\n",
            "Example prediction:\n",
            " 18 [121 122 122 123 122 121 122 122 123 122 122 122]\n",
            "Example prediction:\n",
            " 20 [109 109 109 109 109 108 109 109 111 110 109 110]\n",
            "Example prediction:\n",
            " 22 [117 118 118 118 118 118 118 118 118 118 118 118]\n",
            "Building Eagle_office_Nereida\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_102 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_103 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_51 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_90 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [433 474 507 535 536 537 550 540 527 515 523 530]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 72427.8566\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 69489.0513\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 67677.6232\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 65901.0661\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 64023.5746\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 60901.3459\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 59974.3168\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56963.4732\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56102.4248\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53905.3122\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 51385.9462\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50508.9833\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48738.0270\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 47195.1496\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 45267.2608\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 44764.2365\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 42625.5013\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39789.2756\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39503.5069\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 38974.1160\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 37015.9480\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35784.4012\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34795.9625\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33814.1879\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 31551.5350\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30967.7282\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30128.7896\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28652.0556\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27882.2451\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26343.6527\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26061.2485\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25025.1787\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23914.2209\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23519.4729\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22609.7915\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21556.8096\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21307.1069\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20428.4269\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19467.9110\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18473.3804\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18297.2698\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18072.8878\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16918.1137\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16789.0137\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15935.7596\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15970.9305\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14967.1130\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14871.7402\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14777.3167\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14470.3930\n",
            "i,mean,rmse,rmse/mean,bldg: 13 272.9589435921015 135.2798207643385 0.49560501291540404 Eagle_office_Nereida\n",
            "Example prediction:\n",
            " 0 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 2 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 4 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 6 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 8 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 10 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 12 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 14 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 16 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 18 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 20 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Example prediction:\n",
            " 22 [206 206 206 206 206 205 206 206 204 206 204 206]\n",
            "Building Eagle_lodging_Tressa\n",
            "Building Eagle_education_Eileen\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_104 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_105 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_52 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_91 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 95  89  84  64  47  38  35  69  92 112  99  88]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 2541.4852\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2099.2545\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1785.2992\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1531.4963\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1326.6226\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1155.5363\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1019.0233\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 935.9453\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 864.7825\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 811.5637\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 795.2644\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 729.5001\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 731.5355\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 718.0514\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 706.2709\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 695.6741\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 714.2449\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 698.9780\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 727.5352\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 621.7559\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 541.5068\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 506.0150\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 481.0606\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 457.2313\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 440.1476\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 427.0669\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 418.6570\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 390.9641\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 397.1072\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 390.8112\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 388.9139\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 388.1117\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 368.6727\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 372.9090\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 363.2114\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 353.2174\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 352.1600\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 339.2393\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 354.4349\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 345.1307\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 347.0960\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 342.2488\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 332.1938\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 335.3781\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 326.7377\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 320.3754\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 316.8051\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 311.8095\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 307.8821\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 303.8969\n",
            "i,mean,rmse,rmse/mean,bldg: 14 46.463674127906486 24.034311383910143 0.517271004392546 Eagle_education_Eileen\n",
            "Example prediction:\n",
            " 0 [44 45 45 45 45 44 44 43 41 41 41 42]\n",
            "Example prediction:\n",
            " 2 [39 40 40 40 40 38 38 37 36 36 36 36]\n",
            "Example prediction:\n",
            " 4 [41 42 43 43 42 41 40 39 37 37 37 38]\n",
            "Example prediction:\n",
            " 6 [36 37 37 37 38 37 36 35 33 32 33 34]\n",
            "Example prediction:\n",
            " 8 [46 46 45 44 44 45 45 47 49 50 51 49]\n",
            "Example prediction:\n",
            " 10 [53 52 52 51 51 52 53 55 56 57 57 56]\n",
            "Example prediction:\n",
            " 12 [54 54 55 55 55 55 54 54 52 52 52 53]\n",
            "Example prediction:\n",
            " 14 [60 61 61 63 63 61 60 58 55 54 53 55]\n",
            "Example prediction:\n",
            " 16 [43 44 45 45 46 45 44 42 40 40 40 41]\n",
            "Example prediction:\n",
            " 18 [36 37 37 38 37 37 36 35 32 33 32 34]\n",
            "Example prediction:\n",
            " 20 [37 37 37 38 37 38 37 36 33 34 34 35]\n",
            "Example prediction:\n",
            " 22 [27 27 27 28 27 27 26 26 24 24 25 26]\n",
            "Building Eagle_education_Wesley\n",
            " Count bad values before pseudofill: 112\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_106 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_107 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_53 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_92 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 0.0091\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6.7867e-04\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.4461e-04\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5.0965e-04\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.8446e-04\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.7607e-04\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.7943e-04\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.8052e-04\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.6645e-04\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.6057e-04\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.8858e-04\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.5372e-04\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.6109e-04\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.7854e-04\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4268e-04\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.5166e-04\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4965e-04\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4037e-04\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4842e-04\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.3895e-04\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.3626e-04\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.3018e-04\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4488e-04\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.3590e-04\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.4596e-04\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1891e-04\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1300e-04\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1464e-04\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1012e-04\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1368e-04\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1532e-04\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.0128e-04\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.1459e-04\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.0445e-04\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.0805e-04\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.9392e-04\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.9214e-04\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.0011e-04\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4.0069e-04\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.8959e-04\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.8378e-04\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.7606e-04\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.7809e-04\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.6785e-04\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.6992e-04\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.6580e-04\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.6645e-04\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.6281e-04\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.5542e-04\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3.4951e-04\n",
            "i,mean,rmse,rmse/mean,bldg: 15 0.10542525466510344 0.03538158436484869 0.3356082418509942 Eagle_education_Wesley\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building Eagle_health_Vincenza\n",
            " Count bad values before pseudofill: 75\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_108 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_109 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_54 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_93 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [249 254 202 138  82  79  84  86 160 230 306 296]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 17489.3692\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15834.8092\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15200.7656\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14176.4222\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13219.0271\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12449.0512\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11539.4017\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10914.2087\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10323.8736\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9509.9251\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8794.2430\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8337.8204\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7839.0174\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7499.8675\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6878.9435\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6408.2681\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6130.2225\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5642.5520\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5354.3699\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5128.8036\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4845.6139\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4514.3685\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4393.8593\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4211.2225\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4060.5818\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3876.2299\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3794.4702\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3751.7098\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3640.1788\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3600.2389\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3533.8939\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3515.7974\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3487.6833\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3480.8648\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3480.9589\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3499.0860\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2926.7033\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2737.0078\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2619.1616\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2555.2108\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2527.3999\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2385.5752\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2387.2337\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2281.0024\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2288.2789\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2267.4601\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2158.9432\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2154.2200\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2096.5050\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2079.2332\n",
            "i,mean,rmse,rmse/mean,bldg: 16 122.34687520459563 54.494437320787824 0.4454093104516076 Eagle_health_Vincenza\n",
            "Example prediction:\n",
            " 0 [121 121 122 122 123 123 122 121 121 121 120 120]\n",
            "Example prediction:\n",
            " 2 [110 110 111 111 111 112 110 110 109 110 109 109]\n",
            "Example prediction:\n",
            " 4 [69 70 71 70 69 70 69 67 67 68 68 68]\n",
            "Example prediction:\n",
            " 6 [50 52 53 52 52 52 51 49 49 49 49 50]\n",
            "Example prediction:\n",
            " 8 [105 106 107 107 106 106 107 106 106 106 105 105]\n",
            "Example prediction:\n",
            " 10 [134 134 135 135 135 135 135 135 135 135 134 134]\n",
            "Example prediction:\n",
            " 12 [144 145 145 146 147 147 147 147 147 146 146 144]\n",
            "Example prediction:\n",
            " 14 [151 152 152 153 154 153 154 154 154 153 152 151]\n",
            "Example prediction:\n",
            " 16 [147 149 149 149 150 150 150 150 150 149 149 147]\n",
            "Example prediction:\n",
            " 18 [147 148 148 149 150 150 150 150 150 149 149 147]\n",
            "Example prediction:\n",
            " 20 [149 151 151 151 153 152 153 152 152 152 151 149]\n",
            "Example prediction:\n",
            " 22 [152 154 154 155 156 155 156 156 156 155 154 153]\n",
            "Building Eagle_office_Dallas\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_110 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_111 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_55 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_94 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [113 154 154 154 117 109 105 137 178 215 219 219]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 4405.7410\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4382.0593\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3896.9240\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3770.6415\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3531.9286\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3204.3404\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3104.0468\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2973.0024\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2732.7211\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2600.4975\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2590.2932\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2666.0082\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2530.1563\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2432.2949\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2254.5904\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2237.6523\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2186.5418\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2213.4385\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2173.6151\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1947.8899\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1991.4967\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2125.5142\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1964.5464\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2034.3226\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1997.5338\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1841.1855\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2058.0059\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1982.8109\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1774.8177\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1856.6554\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2153.5473\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1834.8782\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1808.5341\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1845.3885\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1830.4568\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1627.0015\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1838.6217\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1749.0529\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1640.2718\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1644.8700\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1665.0697\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1607.9429\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1712.8339\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1675.3990\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1610.0626\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1676.7126\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1786.4064\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1544.5793\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1467.3760\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1493.3095\n",
            "i,mean,rmse,rmse/mean,bldg: 17 56.508133937526274 76.81677756332051 1.3593932804124602 Eagle_office_Dallas\n",
            "Example prediction:\n",
            " 0 [36 37 37 37 38 38 38 38 38 38 37 38]\n",
            "Example prediction:\n",
            " 2 [32 32 32 32 33 33 33 34 34 33 33 34]\n",
            "Example prediction:\n",
            " 4 [17 17 17 17 18 19 18 19 19 18 19 19]\n",
            "Example prediction:\n",
            " 6 [15 15 15 15 15 16 15 17 17 16 17 17]\n",
            "Example prediction:\n",
            " 8 [19 18 19 19 19 20 20 22 22 21 23 23]\n",
            "Example prediction:\n",
            " 10 [31 31 31 31 31 31 31 31 30 31 30 30]\n",
            "Example prediction:\n",
            " 12 [34 34 35 34 34 34 34 34 33 34 33 33]\n",
            "Example prediction:\n",
            " 14 [46 46 46 45 46 45 44 44 43 44 42 43]\n",
            "Example prediction:\n",
            " 16 [46 46 47 45 47 47 46 48 48 48 48 49]\n",
            "Example prediction:\n",
            " 18 [38 37 38 37 38 38 37 39 39 39 40 40]\n",
            "Example prediction:\n",
            " 20 [43 43 44 43 44 45 44 46 46 45 46 46]\n",
            "Example prediction:\n",
            " 22 [54 55 55 55 56 55 56 56 55 55 54 55]\n",
            "Building Eagle_education_Shante\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_112 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_113 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_56 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_95 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 175642.4284\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 153097.5990\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 168840.0829\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 157283.3071\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 169805.4418\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 161140.7227\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 175936.5557\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 180532.9708\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 198056.3769\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 183240.9107\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 179118.6346\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 163668.4122\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 150999.0178\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 187927.7288\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 172998.6169\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 186576.8027\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 152782.0999\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 181063.5177\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 171363.1820\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 175671.7939\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 154660.9259\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 165163.1191\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 158091.9798\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 170176.7933\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 171328.9591\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 164345.0273\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 157734.8065\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 149514.7361\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 158698.5202\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 145180.4873\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 157661.4687\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 160942.7078\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 154703.3667\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 147179.0195\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 166987.5294\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 162936.1262\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 161096.2650\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 151370.4805\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 149718.7022\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 162009.0543\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 146716.7832\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 146285.7013\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 162914.2935\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 164728.1966\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 147553.0471\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 166376.8609\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 158270.3402\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 139213.2918\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 145452.1888\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 151739.0178\n",
            "i,mean,rmse,rmse/mean,bldg: 18 1004.1571291019169 1985.0351985816728 1.9768173137973128 Eagle_education_Shante\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [-2 -3 -3 -2 -2 -2 -2 -3 -3 -3 -3 -3]\n",
            "Example prediction:\n",
            " 16 [-3 -3 -3 -2 -2 -2 -2 -3 -3 -3 -3 -3]\n",
            "Example prediction:\n",
            " 18 [-3 -4 -3 -3 -3 -3 -3 -4 -4 -4 -3 -3]\n",
            "Example prediction:\n",
            " 20 [-3 -4 -3 -3 -3 -3 -3 -4 -4 -4 -3 -3]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building Eagle_office_Chauncey\n",
            " Count bad values before pseudofill: 116\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_114 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_115 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_57 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_96 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1735 1706 1698 1718 1726 1771 1713 1731 1755 1796 1823 1753]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 1093219.1105\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1083979.1018\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1073680.1718\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1067759.8900\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1058698.0984\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1035813.0855\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1035370.1505\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1029639.7091\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1016050.7366\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1017057.7266\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1003319.3732\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 994424.5430\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 993937.7725\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 980127.9991\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 972124.1464\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 963349.0045\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 952709.8039\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 944476.0455\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 933084.2236\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 930933.3882\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 923965.5898\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 918107.2118\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 913231.7955\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 896607.3368\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 891188.3150\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 884849.9391\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 872533.0464\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 858837.1893\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 855993.6639\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 858534.9009\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 835449.7039\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 843225.7786\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 833933.2118\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 817496.7909\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 827490.2345\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 802714.1150\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 798041.0786\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 787325.4430\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 781179.7357\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 775709.9836\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 760624.9720\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 763606.7150\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 763340.2795\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 759651.5805\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 737930.8659\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 734746.0259\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 728066.4664\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 713575.1341\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 718697.7198\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 711677.8825\n",
            "i,mean,rmse,rmse/mean,bldg: 19 1037.6877345457663 937.6234380784871 0.9035699342528309 Eagle_office_Chauncey\n",
            "Example prediction:\n",
            " 0 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 2 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 4 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 6 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 8 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 10 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 12 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 14 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 16 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 18 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 20 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Example prediction:\n",
            " 22 [230 230 231 230 231 229 230 230 229 227 229 230]\n",
            "Building Eagle_office_Phyllis\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_116 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_117 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_58 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_58 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_97 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [154 154 154 154 154 154 154 154 155 155 154 153]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 9288.1772\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8503.4064\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7606.2336\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6999.4319\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6427.8835\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5877.1007\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5255.6321\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4821.7129\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4433.3902\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4077.1888\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3759.7488\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3374.0920\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3145.4464\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2878.8548\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2562.8865\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2358.5401\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2207.4648\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2065.6517\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1964.2786\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1816.1183\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1778.1130\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1687.3398\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1683.4145\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1617.0916\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1621.3515\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1567.0034\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1590.4813\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1590.8991\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1546.7548\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1494.2447\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 897.1087\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 773.8867\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 654.2813\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 582.4112\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 534.5797\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 487.0313\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 451.9483\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 418.8894\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 369.7768\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 365.4994\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 329.6164\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 316.2542\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 304.2225\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 292.6889\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 301.3399\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 265.1412\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 270.5651\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 258.8106\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 257.5028\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 249.0293\n",
            "i,mean,rmse,rmse/mean,bldg: 20 87.211947406521 23.405717215557214 0.26837741744782 Eagle_office_Phyllis\n",
            "Example prediction:\n",
            " 0 [137 137 137 137 137 137 137 137 137 136 136 136]\n",
            "Example prediction:\n",
            " 2 [138 138 138 138 138 138 138 138 138 137 137 137]\n",
            "Example prediction:\n",
            " 4 [139 139 139 139 139 139 139 139 139 138 138 138]\n",
            "Example prediction:\n",
            " 6 [137 138 138 137 137 138 137 138 138 137 137 137]\n",
            "Example prediction:\n",
            " 8 [135 135 135 135 135 135 135 135 135 134 134 134]\n",
            "Example prediction:\n",
            " 10 [139 139 139 139 139 139 139 139 139 138 138 138]\n",
            "Example prediction:\n",
            " 12 [139 139 139 139 139 139 139 139 139 138 138 138]\n",
            "Example prediction:\n",
            " 14 [139 139 139 139 139 139 139 139 139 138 138 138]\n",
            "Example prediction:\n",
            " 16 [124 124 123 124 124 124 124 123 123 123 123 124]\n",
            "Example prediction:\n",
            " 18 [122 122 121 122 122 122 122 121 121 122 121 122]\n",
            "Example prediction:\n",
            " 20 [109 110 109 110 109 109 109 109 109 109 109 110]\n",
            "Example prediction:\n",
            " 22 [111 112 111 112 111 111 111 111 111 112 111 112]\n",
            "Building Eagle_office_Freida\n",
            " Count bad values before pseudofill: 63\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_118 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_119 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_59 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_98 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [417 437 390 223 148  54  65  75 200 350 504 409]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 20277.7034\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19254.1994\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18144.6864\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16957.1859\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15998.8198\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15156.0217\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14419.4377\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13873.8377\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13124.2602\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12520.6217\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11564.0856\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11054.4952\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10685.5108\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9886.3161\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9662.7708\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9110.5155\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8631.7973\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8095.3000\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7790.5849\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7540.0266\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7297.0255\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6944.7550\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6795.7236\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6500.2490\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6593.8063\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6132.1925\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5955.7089\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5975.5027\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5847.3828\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5306.6655\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5338.5353\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4907.7138\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4799.5093\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4666.4224\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4639.6817\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4381.6619\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4346.2908\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4217.5827\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4167.6775\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4194.3571\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4074.0377\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3884.8802\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3833.1485\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3839.2813\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3720.0182\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3665.0161\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3640.5482\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3602.1987\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3578.5518\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3571.4961\n",
            "i,mean,rmse,rmse/mean,bldg: 21 101.60147001775249 65.47499484352284 0.6444296015803965 Eagle_office_Freida\n",
            "Example prediction:\n",
            " 0 [147 148 149 150 150 151 151 151 150 150 149 148]\n",
            "Example prediction:\n",
            " 2 [105 105 105 106 105 106 106 105 106 105 106 105]\n",
            "Example prediction:\n",
            " 4 [105 106 106 106 106 107 107 106 107 106 107 106]\n",
            "Example prediction:\n",
            " 6 [ 98  98  98  98  99  98  99  99 100  98  99  99]\n",
            "Example prediction:\n",
            " 8 [91 91 91 91 92 91 91 91 92 91 92 91]\n",
            "Example prediction:\n",
            " 10 [100 101 101 101 102 101 102 102 103 102 102 101]\n",
            "Example prediction:\n",
            " 12 [114 115 115 115 116 116 116 116 117 117 116 115]\n",
            "Example prediction:\n",
            " 14 [129 130 131 131 132 132 133 132 133 133 132 130]\n",
            "Example prediction:\n",
            " 16 [130 131 132 132 133 133 134 133 134 133 132 131]\n",
            "Example prediction:\n",
            " 18 [120 121 121 122 122 122 123 122 123 122 122 120]\n",
            "Example prediction:\n",
            " 20 [106 106 107 107 107 107 108 106 109 108 108 107]\n",
            "Example prediction:\n",
            " 22 [80 80 80 80 80 81 81 79 82 81 82 81]\n",
            "Building Eagle_office_Francis\n",
            " Count bad values before pseudofill: 20\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_120 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_121 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_60 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_99 (GRU)                 (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [396 408 439 451 463 432 451 481 487 475 475 469]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 88569.2518\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 84982.9621\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 82217.4719\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 80740.7859\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 77809.0098\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 75094.8039\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 73620.9763\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 70791.6211\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 69884.4729\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 67344.1305\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 64242.9613\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 62040.2632\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 59579.7709\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 57952.9413\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56652.8792\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 54393.0012\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53040.4041\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50749.7706\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48948.7270\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48189.7254\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46189.4689\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 44177.9309\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 42798.5412\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41092.6323\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 40006.0746\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 38360.8166\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36926.2977\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35460.7532\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33701.5735\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33044.7373\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 31209.7997\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30045.2215\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28598.1209\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27582.6986\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26623.6323\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25889.1363\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24666.9270\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23344.6136\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22586.6897\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21712.6059\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20950.9523\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19909.3818\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18611.9501\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18159.4974\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17854.0908\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16697.7713\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16040.5117\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15099.4707\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14482.8367\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14311.1679\n",
            "i,mean,rmse,rmse/mean,bldg: 22 336.4746029213393 213.3975673187454 0.6342159719217598 Eagle_office_Francis\n",
            "Example prediction:\n",
            " 0 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 2 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 4 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 6 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 8 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 10 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 12 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 14 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 16 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 18 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 20 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Example prediction:\n",
            " 22 [212 212 214 214 211 212 214 212 214 214 212 213]\n",
            "Building Eagle_office_Sheree\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_122 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_123 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_61 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_100 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [165 214 171 103  35  49  79  81 125 157 170 169]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 4873.2074\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4235.3733\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3887.4439\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3532.9037\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3183.7542\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3034.1148\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2791.3332\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2517.6145\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2408.8538\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2335.5486\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2204.4717\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2132.5650\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2067.5430\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2025.1937\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1979.0398\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1779.0712\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1645.5858\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1636.5832\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1519.4157\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1475.9626\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1405.6824\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1356.1903\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1383.1480\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1302.2745\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1285.5519\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1251.6438\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1245.5294\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1207.0183\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1204.9211\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1168.1837\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1138.9669\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1132.2780\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1130.2914\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1107.5148\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1090.8010\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1084.8338\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1072.5577\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1044.1969\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1038.3827\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1061.9759\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1063.4991\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1022.3720\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1034.4665\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1036.2969\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1012.7385\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1001.7853\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1005.9332\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 980.5707\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 992.1816\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1000.1805\n",
            "i,mean,rmse,rmse/mean,bldg: 23 62.01927753077944 56.46298096585006 0.910410169448171 Eagle_office_Sheree\n",
            "Example prediction:\n",
            " 0 [61 62 62 63 62 60 59 58 56 56 57 56]\n",
            "Example prediction:\n",
            " 2 [31 31 30 32 31 29 27 26 24 23 25 25]\n",
            "Example prediction:\n",
            " 4 [32 32 32 32 32 29 27 27 25 24 26 27]\n",
            "Example prediction:\n",
            " 6 [43 43 43 43 42 42 42 42 42 41 43 43]\n",
            "Example prediction:\n",
            " 8 [93 95 95 96 95 93 92 91 90 88 89 88]\n",
            "Example prediction:\n",
            " 10 [105 106 107 108 108 108 107 106 105 104 103 102]\n",
            "Example prediction:\n",
            " 12 [105 106 107 108 108 107 107 106 105 104 103 102]\n",
            "Example prediction:\n",
            " 14 [101 102 103 103 103 103 102 101  99  99  97  96]\n",
            "Example prediction:\n",
            " 16 [104 105 106 107 107 106 105 105 103 103 101 100]\n",
            "Example prediction:\n",
            " 18 [105 106 107 108 108 108 107 106 105 104 103 102]\n",
            "Example prediction:\n",
            " 20 [104 106 107 108 108 107 107 106 105 104 103 101]\n",
            "Example prediction:\n",
            " 22 [105 107 108 109 109 108 108 107 105 105 104 102]\n",
            "Building Eagle_education_Sherrill\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_124 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_125 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_62 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_62 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_101 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4351 4417 4462 4503 4501 4299 4100 3898 3908 3882 3982 3918]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 5498949.7764\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5465001.4036\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5386312.3273\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5408042.7218\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5577356.8164\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5459805.5836\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5478441.2491\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5423975.0836\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5342697.6618\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5389276.8073\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5331587.6164\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5407892.0836\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5353708.9164\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5289182.3327\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5212223.3036\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5169295.3864\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5239087.7782\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5250503.7345\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5224747.9727\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5176555.2473\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5111283.9055\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5232173.2291\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5075090.4327\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5049042.1182\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5107884.7618\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5043056.0345\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5086110.5000\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5058544.9764\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5062882.0009\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5017340.0491\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5017872.7891\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5008182.6473\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4865549.3927\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4940122.9364\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4940237.9073\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4875533.5800\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4902479.0127\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4884568.4418\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4954228.0545\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4834460.5364\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4827903.9682\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4769042.2191\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4797105.0600\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4838997.0127\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4682695.5618\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4694485.3109\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4785628.9964\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4756749.5091\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4659326.3655\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4760738.1109\n",
            "i,mean,rmse,rmse/mean,bldg: 24 2032.6741548164603 2085.2802324227823 1.025880231458481 Eagle_education_Sherrill\n",
            "Example prediction:\n",
            " 0 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 2 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 4 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 6 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 8 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 10 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 12 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 14 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 16 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 18 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 20 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Example prediction:\n",
            " 22 [233 231 230 230 231 229 231 228 231 230 231 232]\n",
            "Building Eagle_education_Brooke\n",
            " Count bad values before pseudofill: 56\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_63\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_126 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_127 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_63 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_102 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [3417 3599 4324 4893 5456 4802 4206 3638 4293 4751 4707 4611]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 4788022.5636\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4643335.6564\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4694662.6582\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4603111.8327\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4700589.5982\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4652400.0018\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4622972.1982\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4509600.9309\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4570360.7055\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4607734.3527\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4536492.6636\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4540065.2236\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4522420.8636\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4503034.9473\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4531473.0036\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4502670.3000\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4398810.8745\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4395327.1173\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4417732.5709\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4409771.0164\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4305258.3191\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4314508.5636\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4336269.4818\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4252448.9700\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4272039.2273\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4280019.9391\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4317375.7964\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4264251.3264\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4173003.5782\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4232913.4164\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4285442.3618\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4258955.3664\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4108493.6391\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4216756.7927\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4229533.7945\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4132534.6264\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4056167.3755\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4035754.4564\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4050315.9655\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4092917.6636\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4060103.8955\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4002793.4864\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3966835.3800\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3940766.3582\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3974370.6200\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3963733.8027\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3927700.7055\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3988814.8264\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3900661.4018\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3901794.9973\n",
            "i,mean,rmse,rmse/mean,bldg: 25 1638.6896927614282 1814.3578554209807 1.1072003829861934 Eagle_education_Brooke\n",
            "Example prediction:\n",
            " 0 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 2 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 4 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 6 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 8 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 10 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 12 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 14 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 16 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 18 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 20 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Example prediction:\n",
            " 22 [229 231 230 231 231 231 231 228 232 232 230 230]\n",
            "Building Eagle_education_Alberto\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_128 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_129 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_64 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_103 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1231 1231 1231 1234 1234 1234 1233 1234 1238 1238 1230 1225]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 622092.9802\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 611724.3841\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 600267.2811\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 593085.6407\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 586036.4427\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 577662.3900\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 569448.2712\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 566521.4952\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 558621.8000\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 546186.2362\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 549923.1861\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 543883.6139\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 538289.2477\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 541348.4884\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 518438.1813\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 519453.6048\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 503895.4408\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 504434.9882\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 497355.8889\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 496038.3443\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 491183.3977\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 482874.0284\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 471838.8522\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 472099.4784\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 461293.2500\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 456481.1314\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 454432.8418\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 450489.2932\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 448713.5483\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 444628.5091\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 440891.7468\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 431966.6678\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 426329.3488\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 419602.7766\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 421160.8051\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 406514.0317\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 399929.5658\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 400654.4344\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 400532.1711\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 388608.5159\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 390858.0226\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 385794.4284\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 371801.6728\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 376203.8425\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 367775.1749\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 359862.9391\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 358542.2557\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 355983.3760\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 347481.4958\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 340446.7062\n",
            "i,mean,rmse,rmse/mean,bldg: 26 694.9820078317341 528.4756332527935 0.7604162802740436 Eagle_education_Alberto\n",
            "Example prediction:\n",
            " 0 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 2 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 4 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 6 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 8 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 10 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 12 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 14 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 16 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 18 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 20 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Example prediction:\n",
            " 22 [228 228 229 227 229 226 226 230 227 227 226 224]\n",
            "Building Eagle_food_Kay\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_65\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_130 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_131 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_65 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_104 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [769 710 591 484 385 366 332 292 254 420 593 804]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 126008.6335\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 125238.7094\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 122627.3896\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 119474.5253\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 113576.3629\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 115393.9279\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 110849.6665\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 108230.4346\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 108824.1695\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 105840.9769\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 102825.7379\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 101529.0536\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 97466.3851\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 95964.3504\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 95093.6120\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 89376.5558\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 88517.5964\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 87241.1626\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 87512.9570\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 83635.4771\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 80480.8919\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 80092.7051\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 78561.7902\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 76468.3130\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 74631.5309\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 74426.6337\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 72537.6166\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 70220.0275\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 68088.7306\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 66924.0985\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 66599.8689\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 65052.9380\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 64312.0156\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 63279.4681\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 61841.8267\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 60545.5998\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56813.7808\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 58433.1811\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56219.8503\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 54240.3765\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 54006.5450\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53503.3738\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53138.1920\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53027.4162\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 49364.7488\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 49524.5763\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 49073.7713\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46437.6904\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48216.1939\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46459.6317\n",
            "i,mean,rmse,rmse/mean,bldg: 27 276.26271949384346 209.62451769008948 0.7587868463546381 Eagle_food_Kay\n",
            "Example prediction:\n",
            " 0 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 2 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 4 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 6 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 8 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 10 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 12 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 14 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 16 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 18 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 20 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Example prediction:\n",
            " 22 [212 212 212 213 213 212 211 213 212 211 212 213]\n",
            "Building Eagle_health_Jodi\n",
            " Count bad values before pseudofill: 41\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_66\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_132 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_133 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_66 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_105 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [355 365 365 365 365 365 365 365 365 365 365 353]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 35713.4474\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 33921.5253\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 32313.5491\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 31205.2809\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29267.1239\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28241.1386\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27567.1775\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25343.7391\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24764.2664\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23464.1617\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22614.3521\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21465.8467\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20638.2501\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19624.8929\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18484.9425\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17703.2404\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16918.6109\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15753.5349\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15018.0871\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14460.2042\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14008.3449\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13209.9242\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12510.9329\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12129.9015\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11501.2354\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10923.4621\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10770.4855\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10067.6125\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9596.2676\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9378.6451\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8936.4832\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8662.7105\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8353.3600\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7878.0863\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7938.7165\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7780.2056\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7377.6315\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7356.8223\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7102.9070\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6929.6026\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6950.9751\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6669.7370\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6781.1396\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5158.1015\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4825.6130\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4634.1542\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4409.6161\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4237.1049\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3887.6580\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3715.7311\n",
            "i,mean,rmse,rmse/mean,bldg: 28 192.41198593089595 77.79318497963341 0.4043052962801005 Eagle_health_Jodi\n",
            "Example prediction:\n",
            " 0 [184 185 184 185 185 185 185 186 185 185 184 185]\n",
            "Example prediction:\n",
            " 2 [161 160 160 161 162 161 161 161 161 161 161 160]\n",
            "Example prediction:\n",
            " 4 [132 131 131 132 133 132 132 132 132 132 132 131]\n",
            "Example prediction:\n",
            " 6 [100 100 100  99 100 100 100 100  99 100 100  99]\n",
            "Example prediction:\n",
            " 8 [112 112 112 112 113 113 112 113 112 113 112 112]\n",
            "Example prediction:\n",
            " 10 [157 157 157 158 159 158 158 158 158 158 158 157]\n",
            "Example prediction:\n",
            " 12 [173 174 173 174 174 174 174 175 174 174 174 174]\n",
            "Example prediction:\n",
            " 14 [175 175 174 175 176 175 175 176 175 175 175 175]\n",
            "Example prediction:\n",
            " 16 [178 178 177 178 179 178 178 179 178 178 178 178]\n",
            "Example prediction:\n",
            " 18 [162 162 162 162 163 163 163 163 162 163 162 162]\n",
            "Example prediction:\n",
            " 20 [150 150 149 150 151 150 150 151 150 151 150 150]\n",
            "Example prediction:\n",
            " 22 [150 150 149 150 151 150 151 151 151 151 150 150]\n",
            "Building Eagle_education_Norah\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_134 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_135 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_67 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_67 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_106 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1834 1841 2051 2083 2227 2267 2373 2379 2382 2320 2133 2034]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 2s 4ms/step - loss: 629593.9157\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 616933.4443\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 610880.0530\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 605993.8373\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 595017.9411\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 591618.5791\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 594655.0680\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 586206.5539\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 574578.6665\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 558184.4825\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 569897.4145\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 555332.8091\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 558062.0127\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 557477.2657\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 543438.1865\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 544339.8633\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 524263.8033\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 532714.8230\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 527127.4464\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 506673.9726\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 513291.3506\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 495091.6657\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 500366.2812\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 486682.8372\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 497899.1417\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 501252.7734\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 478373.1924\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 471774.6598\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 469850.7076\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 467692.4185\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 463797.0111\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 449672.3075\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 447991.5610\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 437109.9960\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 439843.4570\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 431532.0760\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 437166.5189\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 432836.2799\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 413785.5005\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 413868.7823\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 423856.4931\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 404427.8544\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 389788.2715\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 397203.4531\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 403219.6376\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 391332.1610\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 397610.1567\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 384788.2898\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 374872.0939\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 378843.8445\n",
            "i,mean,rmse,rmse/mean,bldg: 29 712.0697702804412 671.9546957592712 0.9436641236639338 Eagle_education_Norah\n",
            "Example prediction:\n",
            " 0 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 2 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 4 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 6 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 8 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 10 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 12 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 14 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 16 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 18 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 20 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Example prediction:\n",
            " 22 [226 226 227 227 228 225 225 226 226 226 225 228]\n",
            "Building Eagle_education_Will\n",
            " Count bad values before pseudofill: 15\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_136 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_137 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_68 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_68 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_107 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [440 418 442 391 383 361 362 370 360 362 363 373]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 77059.5053\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 75073.6003\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 71289.7636\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 70945.3919\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 67369.0862\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 64597.6755\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 63620.3759\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 61365.6109\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 59621.4500\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 57891.3744\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 54522.4715\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 53657.1509\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 51468.9185\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50401.2790\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48414.2556\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46807.2968\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 45421.7788\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 42879.0964\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41848.2370\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39999.2542\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 37593.9594\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36581.8327\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35972.6861\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 35023.2364\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 32440.6467\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 32664.5967\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30145.9966\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29322.8798\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27951.6264\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27356.4655\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25922.6143\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25628.7559\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23822.5484\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22775.4618\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21718.0512\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21102.5664\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19938.9943\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19304.9738\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18265.8188\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18173.0458\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17318.3331\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16179.3171\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15601.0907\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14880.1450\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14470.9221\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13612.6668\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13013.2221\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12623.3429\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12144.5007\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11794.5915\n",
            "i,mean,rmse,rmse/mean,bldg: 30 226.31931160799405 55.11184455447765 0.24351366289915397 Eagle_education_Will\n",
            "Example prediction:\n",
            " 0 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 2 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 4 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 6 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 8 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 10 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 12 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 14 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 16 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 18 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 20 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Example prediction:\n",
            " 22 [208 210 210 210 208 211 207 211 208 209 209 211]\n",
            "Building Eagle_lodging_Blake\n",
            " Count bad values before pseudofill: 8\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_138 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_69 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_69 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_108 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [  8   8   8   8   8   8   8   8 296 296 297   6]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 29505.1443\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30026.2566\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28876.8554\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29177.7350\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27322.4363\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27831.2348\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28196.1447\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28256.5184\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27773.4652\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26534.8881\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27069.9053\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27515.1571\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28081.6012\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26351.0665\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26444.1940\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26559.8592\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26805.3125\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26807.8501\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25534.9590\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25311.3827\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26282.4936\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25799.4127\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24610.4481\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24668.3367\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24265.8759\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25408.3160\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24905.6928\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26642.4934\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25783.3313\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25328.4877\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25081.1111\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24826.4345\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25675.7519\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24961.6098\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23253.6154\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24392.1725\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24922.4446\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25034.6080\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25032.3539\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23676.9909\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24514.3000\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24046.5165\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24125.8875\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24997.6763\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23816.8029\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24224.7587\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24472.1533\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23647.8076\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23664.6434\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23715.9430\n",
            "i,mean,rmse,rmse/mean,bldg: 31 43.440914326264924 55.50379575610367 1.2776847959331596 Eagle_lodging_Blake\n",
            "Example prediction:\n",
            " 0 [65 67 67 66 67 68 68 67 68 67 68 66]\n",
            "Example prediction:\n",
            " 2 [68 70 71 69 70 71 71 71 71 71 71 70]\n",
            "Example prediction:\n",
            " 4 [82 84 85 83 84 86 86 84 85 85 85 84]\n",
            "Example prediction:\n",
            " 6 [107 109 110 110 110 111 111 110 111 110 108 107]\n",
            "Example prediction:\n",
            " 8 [113 115 116 116 117 117 117 117 117 116 115 113]\n",
            "Example prediction:\n",
            " 10 [112 114 115 115 116 116 116 115 116 115 114 112]\n",
            "Example prediction:\n",
            " 12 [103 105 105 105 106 107 107 105 106 105 104 103]\n",
            "Example prediction:\n",
            " 14 [67 68 68 67 67 70 69 68 69 67 69 69]\n",
            "Example prediction:\n",
            " 16 [94 97 96 96 97 97 97 97 97 96 96 94]\n",
            "Example prediction:\n",
            " 18 [107 110 110 110 111 111 111 111 111 110 109 107]\n",
            "Example prediction:\n",
            " 20 [113 115 116 116 117 117 117 117 117 116 115 113]\n",
            "Example prediction:\n",
            " 22 [114 116 116 117 117 117 118 117 117 116 115 113]\n",
            "Building Eagle_education_Petra\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_140 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_141 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_70 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_70 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_109 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [166 175 180 184 182 180 184 190 193 193 192 190]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 4397.5127\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3878.7487\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3385.7030\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3082.0627\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2759.6254\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2492.1359\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2322.6880\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2047.3838\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1880.5683\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1705.5172\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1599.1131\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1553.6293\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1506.3964\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1451.9948\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1438.3941\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1327.5034\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1085.6520\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 951.3512\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 907.3269\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 769.7609\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 743.3494\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 656.7850\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 620.7583\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 597.6882\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 568.3725\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 512.7070\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 465.0756\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 437.2150\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 422.7649\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 408.2584\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 385.4626\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 348.2289\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 345.9103\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 299.4810\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 304.6620\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 296.0514\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 272.7388\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 258.0899\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 256.5608\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 234.9911\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 237.8812\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 229.3509\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 222.7506\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 224.1629\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 215.3135\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 203.7051\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 204.9874\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 199.3379\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 198.4638\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 196.5386\n",
            "i,mean,rmse,rmse/mean,bldg: 32 57.05018764819895 35.62297477553966 0.6244146819500297 Eagle_education_Petra\n",
            "Example prediction:\n",
            " 0 [63 64 63 63 63 63 64 64 64 64 65 63]\n",
            "Example prediction:\n",
            " 2 [63 63 63 63 63 63 63 63 63 64 64 63]\n",
            "Example prediction:\n",
            " 4 [52 53 52 52 52 52 52 53 53 53 53 52]\n",
            "Example prediction:\n",
            " 6 [37 37 37 37 37 38 37 38 38 38 38 37]\n",
            "Example prediction:\n",
            " 8 [50 50 50 50 50 49 50 50 50 50 51 50]\n",
            "Example prediction:\n",
            " 10 [52 51 53 52 52 52 53 53 53 53 53 53]\n",
            "Example prediction:\n",
            " 12 [67 66 68 67 67 67 67 67 68 67 67 68]\n",
            "Example prediction:\n",
            " 14 [91 91 92 92 92 91 92 92 92 91 90 91]\n",
            "Example prediction:\n",
            " 16 [102 102 103 103 103 104 104 104 104 104 103 103]\n",
            "Example prediction:\n",
            " 18 [106 106 107 107 107 108 108 108 108 108 108 107]\n",
            "Example prediction:\n",
            " 20 [107 107 108 108 108 108 108 109 108 108 108 108]\n",
            "Example prediction:\n",
            " 22 [93 93 93 93 94 94 94 94 94 94 94 94]\n",
            "Building Eagle_lodging_Trina\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_142 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_143 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_71 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_110 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [316 317 318 331 326 327 320 325 335 365 381 365]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 12814.5625\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12061.0521\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11585.5926\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10364.7486\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10058.3314\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9403.8256\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9063.5169\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8488.4800\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8232.6989\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7709.4552\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7510.5381\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7148.5212\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6871.8282\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6377.9066\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6589.0826\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6339.9232\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6055.1967\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5826.5595\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5726.4748\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5345.7610\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5218.8505\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5140.1440\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4986.4036\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4687.0482\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4756.1956\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4426.1010\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4609.7290\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4341.6174\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4187.8731\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3801.6146\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3829.5251\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3738.1964\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3679.4578\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3607.1773\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3470.8520\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3275.9682\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3335.6667\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3224.9698\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3128.8995\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3094.6355\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3114.1166\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3000.8440\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2873.3189\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2746.0893\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2768.2828\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2648.6897\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2586.6521\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2610.5033\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2485.4513\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2541.3839\n",
            "i,mean,rmse,rmse/mean,bldg: 33 91.28035335157311 76.31488874955129 0.8360494448966338 Eagle_lodging_Trina\n",
            "Example prediction:\n",
            " 0 [74 74 74 74 75 74 74 75 75 75 76 75]\n",
            "Example prediction:\n",
            " 2 [84 82 83 82 84 83 83 84 84 84 85 84]\n",
            "Example prediction:\n",
            " 4 [67 65 66 65 67 66 66 67 67 66 68 67]\n",
            "Example prediction:\n",
            " 6 [45 44 44 45 45 44 44 44 45 44 46 45]\n",
            "Example prediction:\n",
            " 8 [88 89 89 90 89 89 88 88 89 89 89 89]\n",
            "Example prediction:\n",
            " 10 [144 144 145 145 144 144 145 144 144 144 143 143]\n",
            "Example prediction:\n",
            " 12 [155 155 156 155 155 155 155 155 155 155 154 154]\n",
            "Example prediction:\n",
            " 14 [164 165 165 165 165 164 165 165 165 165 164 163]\n",
            "Example prediction:\n",
            " 16 [165 166 166 166 166 165 166 166 166 166 165 164]\n",
            "Example prediction:\n",
            " 18 [161 162 162 162 162 162 162 162 162 162 161 161]\n",
            "Example prediction:\n",
            " 20 [165 166 166 166 166 165 166 166 165 165 165 164]\n",
            "Example prediction:\n",
            " 22 [127 129 129 129 129 128 129 129 129 129 129 129]\n",
            "Building Eagle_health_Reuben\n",
            "Building Eagle_education_Teresa\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_72\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_144 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_145 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_72 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_111 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [319 328 253 169 146 189 234 284 352 432 447 463]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 5ms/step - loss: 30707.3120\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29629.6889\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28512.2701\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26054.2939\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25193.5491\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24173.4229\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23287.8628\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21766.8590\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 21361.6329\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20317.5197\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19226.4731\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17959.3494\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17730.6009\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16778.1793\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16774.5321\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14957.0598\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14703.1923\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13958.9536\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13526.1415\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12538.3838\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12486.9664\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11954.0062\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11494.7707\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11198.1372\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10203.7600\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10509.6228\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9863.9513\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9397.2998\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9399.9580\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9519.2567\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8809.7112\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8911.0251\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8565.9032\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8155.8423\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8543.5582\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7602.7319\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6933.5374\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6631.2612\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6937.1740\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6277.8421\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5981.1910\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5988.9472\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5840.9601\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5513.3971\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5196.8990\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4982.6952\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4959.3234\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4859.3643\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4864.9705\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4631.8089\n",
            "i,mean,rmse,rmse/mean,bldg: 34 148.73671883114207 56.532660486695725 0.3800854350624486 Eagle_education_Teresa\n",
            "Example prediction:\n",
            " 0 [144 143 143 144 143 143 143 143 142 142 143 143]\n",
            "Example prediction:\n",
            " 2 [125 124 123 124 124 124 123 124 122 123 124 124]\n",
            "Example prediction:\n",
            " 4 [91 88 88 89 89 89 87 88 87 89 88 90]\n",
            "Example prediction:\n",
            " 6 [101  99 100 100  99  99  99  98  98 100  99 100]\n",
            "Example prediction:\n",
            " 8 [122 120 120 120 120 120 119 119 119 120 120 120]\n",
            "Example prediction:\n",
            " 10 [147 146 146 147 147 146 146 146 145 146 146 146]\n",
            "Example prediction:\n",
            " 12 [157 157 157 157 157 156 157 157 156 156 156 157]\n",
            "Example prediction:\n",
            " 14 [166 166 166 167 167 166 166 166 166 165 165 166]\n",
            "Example prediction:\n",
            " 16 [185 186 186 187 186 185 186 186 186 185 185 184]\n",
            "Example prediction:\n",
            " 18 [169 169 169 170 170 169 170 170 169 168 169 168]\n",
            "Example prediction:\n",
            " 20 [173 173 173 174 174 173 174 174 173 172 173 172]\n",
            "Example prediction:\n",
            " 22 [179 179 179 180 179 178 179 179 179 178 178 178]\n",
            "Building Eagle_office_Norbert\n",
            " Count bad values before pseudofill: 52\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_146 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_147 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_73 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_112 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [643 656 671 690 692 704 696 696 695 699 709 696]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 151984.7593\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 149807.9389\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 144298.7807\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 143172.1900\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 137122.0566\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 133756.6752\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 131064.8467\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 129871.6232\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 125421.5256\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 124110.2321\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 119582.0688\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 116808.0486\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 113959.4396\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 112723.7988\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 109493.6410\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 108450.3747\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 104827.7896\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 102076.3555\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 98964.1263\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 96520.6159\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 94743.9399\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 90846.6913\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 89946.2655\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 88027.4621\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 84982.9705\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 84565.2254\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 81144.2201\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 79193.0378\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 76967.2909\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 74045.9283\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 73081.9036\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 70484.7608\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 70100.7414\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 66591.8027\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 64980.0143\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 63636.0100\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 61715.0908\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 61001.9934\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 57870.6442\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 57498.0189\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 55810.6358\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53724.7966\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51723.1392\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50660.4020\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50098.6875\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48552.9242\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47970.9758\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46355.3015\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44293.9215\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42847.9653\n",
            "i,mean,rmse,rmse/mean,bldg: 35 390.7765009878811 236.0960269524267 0.604171505593548 Eagle_office_Norbert\n",
            "Example prediction:\n",
            " 0 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 2 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 4 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 6 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 8 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 10 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 12 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 14 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 16 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 18 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 20 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Example prediction:\n",
            " 22 [218 219 219 221 220 218 218 219 217 220 220 221]\n",
            "Building Eagle_lodging_Casey\n",
            "Building Eagle_office_Tia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_148 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_149 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_74 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_113 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [417 393 372 281 208 166 153 304 405 493 436 388]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 51231.8961\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 48104.7617\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 46324.3254\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 44592.3525\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 43590.5572\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41170.0142\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 41215.2342\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 39187.2634\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36966.5288\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 36726.4787\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34918.8407\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 34090.8746\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32313.6841\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 31728.4077\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30401.4770\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29441.8895\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28947.8994\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 27074.5024\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25908.8444\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25629.8065\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24813.7950\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 23207.0617\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22438.3708\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22273.6403\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21456.5896\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20684.3189\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20278.7423\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 19771.4340\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19049.4936\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18759.8909\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18609.0579\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17195.1491\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16736.3973\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16881.7040\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16353.5992\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15813.7268\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15815.8831\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15061.0157\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15312.8200\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14678.7465\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 15337.6869\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14079.8379\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14561.2520\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13965.3451\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14051.2488\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 14008.7223\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13785.5400\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13567.3318\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12076.8329\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11431.8934\n",
            "i,mean,rmse,rmse/mean,bldg: 36 174.62244338235027 110.68297027246926 0.6338416078059322 Eagle_office_Tia\n",
            "Example prediction:\n",
            " 0 [145 145 144 144 145 144 145 144 144 145 145 145]\n",
            "Example prediction:\n",
            " 2 [139 140 139 139 139 139 139 138 138 140 139 139]\n",
            "Example prediction:\n",
            " 4 [130 131 129 129 129 129 129 128 129 130 129 130]\n",
            "Example prediction:\n",
            " 6 [114 115 115 114 114 114 114 113 114 115 114 114]\n",
            "Example prediction:\n",
            " 8 [188 188 187 188 188 187 188 188 188 189 189 189]\n",
            "Example prediction:\n",
            " 10 [188 189 188 188 188 188 189 189 188 189 189 189]\n",
            "Example prediction:\n",
            " 12 [188 189 188 188 188 188 189 189 188 189 189 189]\n",
            "Example prediction:\n",
            " 14 [188 189 188 188 188 188 189 189 188 189 189 189]\n",
            "Example prediction:\n",
            " 16 [188 189 188 188 188 188 189 189 188 189 189 189]\n",
            "Example prediction:\n",
            " 18 [184 184 183 183 183 183 184 184 184 184 185 184]\n",
            "Example prediction:\n",
            " 20 [178 179 178 178 178 178 178 178 178 179 179 179]\n",
            "Example prediction:\n",
            " 22 [184 184 183 184 183 183 184 184 184 185 185 184]\n",
            "Building Eagle_office_Remedios\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_150 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_151 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_75 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_75 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_114 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [192 211 225 238 238 238 244 240 234 229 232 236]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 5ms/step - loss: 14027.3954\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13124.1174\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12077.1131\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11313.3670\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10594.4891\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9825.1751\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8907.6453\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8383.5399\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7759.8633\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7090.6772\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6712.9272\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6271.1649\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5826.4093\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5494.2884\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4929.4694\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4588.2382\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4281.2107\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3986.0315\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3737.3583\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3596.9195\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3396.2989\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3192.9157\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3015.2169\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2862.1329\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2894.0333\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2734.6591\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2633.9574\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2611.7261\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2605.6151\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2549.9734\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2552.4892\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2602.5544\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2482.7381\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2550.8914\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2509.4958\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2471.7637\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2546.7112\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2496.8879\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2464.5810\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2420.7558\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1341.9802\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1121.8482\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1025.7126\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 955.6456\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 862.2023\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 807.9976\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 742.4887\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 706.6170\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 664.8078\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 598.7466\n",
            "i,mean,rmse,rmse/mean,bldg: 37 121.45535634240541 34.287126419092225 0.282302299804962 Eagle_office_Remedios\n",
            "Example prediction:\n",
            " 0 [114 115 116 115 115 115 116 115 115 115 115 115]\n",
            "Example prediction:\n",
            " 2 [101 102 103 103 102 102 103 103 102 102 103 102]\n",
            "Example prediction:\n",
            " 4 [79 80 80 81 80 80 80 80 80 80 80 79]\n",
            "Example prediction:\n",
            " 6 [72 72 72 73 72 72 72 72 72 71 72 71]\n",
            "Example prediction:\n",
            " 8 [112 112 112 112 112 113 112 112 112 112 111 112]\n",
            "Example prediction:\n",
            " 10 [134 135 134 134 134 134 134 134 134 134 134 134]\n",
            "Example prediction:\n",
            " 12 [136 136 136 136 136 136 136 136 136 136 136 136]\n",
            "Example prediction:\n",
            " 14 [136 136 136 136 136 136 136 136 136 136 136 136]\n",
            "Example prediction:\n",
            " 16 [135 136 135 135 135 135 135 135 135 135 135 135]\n",
            "Example prediction:\n",
            " 18 [139 139 139 138 139 139 139 139 139 139 139 139]\n",
            "Example prediction:\n",
            " 20 [125 126 126 124 125 126 126 125 125 126 125 126]\n",
            "Example prediction:\n",
            " 22 [140 140 140 139 139 140 140 140 139 139 139 140]\n",
            "Building Eagle_office_Patrice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_76\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_152 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_153 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_76 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_76 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_115 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [498 460 394 367 320 292 239 212 212 221 295 459]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 4ms/step - loss: 33915.4502\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 32289.0264\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 31701.6411\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 30083.0875\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 29463.6676\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 28058.4593\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26702.3514\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 26459.8907\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 25199.2506\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 24958.4203\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23353.6011\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 22301.3027\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21766.3244\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20761.3610\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 20815.2357\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19579.5184\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18847.9216\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 18213.8511\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 17562.4363\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17824.4253\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16220.6236\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 16328.2169\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16126.9745\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14784.9306\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14609.0844\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14045.7019\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13783.3950\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 13292.6660\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12837.3566\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11575.6517\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11232.7945\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11332.5208\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11244.2279\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10490.5988\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10776.0975\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9832.4156\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 10283.1560\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10249.2334\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9356.4568\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8612.6081\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8864.7509\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8951.0735\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8338.7587\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8382.3467\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8039.9748\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8107.4046\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7249.6073\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7252.7147\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7277.1692\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7200.4434\n",
            "i,mean,rmse,rmse/mean,bldg: 38 165.8827588691298 104.5064507172983 0.6300018846427964 Eagle_office_Patrice\n",
            "Example prediction:\n",
            " 0 [33 32 32 33 33 32 32 32 32 31 33 32]\n",
            "Example prediction:\n",
            " 2 [28 27 26 27 28 27 26 26 26 25 27 27]\n",
            "Example prediction:\n",
            " 4 [26 25 25 25 26 25 25 24 24 24 25 25]\n",
            "Example prediction:\n",
            " 6 [-20 -19 -20 -20 -20 -20 -20 -21 -20 -20 -20 -20]\n",
            "Example prediction:\n",
            " 8 [194 193 194 194 194 194 194 195 193 194 194 194]\n",
            "Example prediction:\n",
            " 10 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 12 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 14 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 16 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 18 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 20 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Example prediction:\n",
            " 22 [196 195 196 197 196 197 196 197 196 196 196 196]\n",
            "Building Eagle_education_Shana\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_154 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_155 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_77 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_77 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_116 (GRU)                (None, 16)                3936      \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 6,620\n",
            "Trainable params: 6,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [182 182 182 183 183 183 183 183 183 183 182 181]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 3s 5ms/step - loss: 13329.6114\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 12084.8575\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 11286.8706\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10368.7375\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 9629.0943\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8873.2338\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 8271.2768\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7566.2819\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 7071.4105\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 6396.8499\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5945.3471\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5482.4513\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 5007.2732\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4714.8700\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4320.3761\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4088.4987\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3719.1664\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3472.1559\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3212.3510\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3097.5990\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2927.0075\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2739.1124\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2627.3464\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2487.6035\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2460.2844\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2368.9209\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2305.4717\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2261.0165\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2207.9943\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2229.5290\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1920.0561\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1606.9563\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1484.0144\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1393.1526\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 1273.8139\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1250.4203\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1017.2265\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 958.5817\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 888.4178\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 876.4527\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 817.8153\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 789.6076\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 711.1997\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 679.2067\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 653.7302\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 604.9459\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 572.0731\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 542.5135\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 537.3995\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 498.3259\n",
            "i,mean,rmse,rmse/mean,bldg: 39 103.1846223894125 29.07829678091138 0.2818084333455392 Eagle_education_Shana\n",
            "Example prediction:\n",
            " 0 [153 152 152 152 153 152 152 153 152 152 152 152]\n",
            "Example prediction:\n",
            " 2 [152 152 152 152 153 152 152 153 152 152 152 152]\n",
            "Example prediction:\n",
            " 4 [153 152 153 153 153 152 152 153 152 152 153 152]\n",
            "Example prediction:\n",
            " 6 [150 150 150 150 151 150 150 150 150 150 150 150]\n",
            "Example prediction:\n",
            " 8 [148 148 148 148 148 148 148 148 147 147 148 147]\n",
            "Example prediction:\n",
            " 10 [153 152 153 153 153 152 152 153 152 152 153 152]\n",
            "Example prediction:\n",
            " 12 [153 152 153 153 153 152 152 153 152 152 153 152]\n",
            "Example prediction:\n",
            " 14 [153 152 153 153 153 152 152 153 152 152 153 152]\n",
            "Example prediction:\n",
            " 16 [153 152 153 153 153 152 152 153 152 152 153 152]\n",
            "Example prediction:\n",
            " 18 [137 136 137 137 137 137 136 137 136 136 136 136]\n",
            "Example prediction:\n",
            " 20 [141 141 141 141 141 141 141 141 141 141 141 140]\n",
            "Example prediction:\n",
            " 22 [142 142 142 142 142 142 142 142 142 142 142 142]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5cv9fQ8jieQ",
        "outputId": "c27733de-0c8a-4f16-9c11-48484a2ddc0b"
      },
      "source": [
        "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))  \n",
        "overall = overall/cnt\n",
        "print (\"overall = \",overall)  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "History 24 Future 12\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      0.00       0.00   inf   Eagle_office_Henriette\n",
            "      0.11       0.04  0.34   Eagle_education_Wesley\n",
            "     15.76      43.70  2.77   Eagle_education_Jewell\n",
            "     35.89       9.90  0.28   Eagle_office_Mandi\n",
            "     36.93       9.71  0.26   Eagle_office_Lamont\n",
            "     43.44      55.50  1.28   Eagle_lodging_Blake\n",
            "     46.46      24.03  0.52   Eagle_education_Eileen\n",
            "     56.51      76.82  1.36   Eagle_office_Dallas\n",
            "     57.05      35.62  0.62   Eagle_education_Petra\n",
            "     62.02      56.46  0.91   Eagle_office_Sheree\n",
            "     81.97      68.55  0.84   Eagle_lodging_Edgardo\n",
            "     87.21      23.41  0.27   Eagle_office_Phyllis\n",
            "     91.28      76.31  0.84   Eagle_lodging_Trina\n",
            "     92.83      57.10  0.62   Eagle_lodging_Dawn\n",
            "    101.60      65.47  0.64   Eagle_office_Freida\n",
            "    103.18      29.08  0.28   Eagle_education_Shana\n",
            "    121.46      34.29  0.28   Eagle_office_Remedios\n",
            "    122.35      54.49  0.45   Eagle_health_Vincenza\n",
            "    148.74      56.53  0.38   Eagle_education_Teresa\n",
            "    165.88     104.51  0.63   Eagle_office_Patrice\n",
            "    174.62     110.68  0.63   Eagle_office_Tia\n",
            "    182.08      81.63  0.45   Eagle_public_Alvin\n",
            "    192.41      77.79  0.40   Eagle_health_Jodi\n",
            "    226.32      55.11  0.24   Eagle_education_Will\n",
            "    272.96     135.28  0.50   Eagle_office_Nereida\n",
            "    276.26     209.62  0.76   Eagle_food_Kay\n",
            "    336.47     213.40  0.63   Eagle_office_Francis\n",
            "    390.78     236.10  0.60   Eagle_office_Norbert\n",
            "    477.70     325.64  0.68   Eagle_health_Athena\n",
            "    659.66     789.20  1.20   Eagle_health_Gregoria\n",
            "    694.98     528.48  0.76   Eagle_education_Alberto\n",
            "    712.07     671.95  0.94   Eagle_education_Norah\n",
            "   1004.16    1985.04  1.98   Eagle_education_Shante\n",
            "   1037.69     937.62  0.90   Eagle_office_Chauncey\n",
            "   1084.29     927.82  0.86   Eagle_health_Reba\n",
            "   1199.41    1085.02  0.90   Eagle_education_Roman\n",
            "   1638.69    1814.36  1.11   Eagle_education_Brooke\n",
            "   2032.67    2085.28  1.03   Eagle_education_Sherrill\n",
            "   3153.29    3042.99  0.97   Eagle_education_Peter\n",
            "overall =  inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}