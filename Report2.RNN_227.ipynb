{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFNRPftWw9pK"
   },
   "source": [
    "# RNN \n",
    "Test effect of scaling on RNN. Compare to RNN 228.\n",
    "\n",
    "Input weather + time, output steam. Given 12 hour, predict same 12 hr next day. No smoothing. \n",
    "\n",
    "With standard scaler on inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5deM-us2w9pZ"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats  # mode\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZgkgsP6w9pg",
    "outputId": "221946c3-2b2b-473a-b167-d3e503d88adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTORS= 10 ['hour', 'month', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
    "SITE = 'Eagle'\n",
    "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
    "PREDICTORS = ['hour','month','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
    "NUM_PREDICTORS=len(PREDICTORS)\n",
    "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
    "PREDICTED_VARIABLE = 'meter'  \n",
    "STEPS_HISTORY = 24\n",
    "STEPS_FORWARD = 12 \n",
    "STEPS_FUTURE =  12 \n",
    "METER_FILE='steam.csv'\n",
    "WEATHER_FILE='weather.csv'\n",
    "EXAMPLE='Eagle_lodging_Edgardo'\n",
    "SITE_BUILDINGS = None\n",
    "SMOOTHING_WINDOW=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgeDotTmw9pX",
    "outputId": "e6cfd356-50f0-4b35-8ba7-14097a8d30f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "DATAPATH=''\n",
    "try:\n",
    "    # On Google Drive, set path to my drive / data directory.\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    PATH='/content/drive/'\n",
    "    drive.mount(PATH)\n",
    "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
    "except:\n",
    "    # On home computer, set path to local data directory.\n",
    "    IN_COLAB = False\n",
    "    DATAPATH='data/'  # must end in \"/\"\n",
    "\n",
    "ZIP_FILE='BuildingData.zip'\n",
    "ZIP_PATH = DATAPATH+ZIP_FILE\n",
    "MODEL_FILE='Model'  # will be used later to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xvXGAH2wzBWS"
   },
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "    scaler=StandardScaler()\n",
    "    #scaler=MinMaxScaler()\n",
    "    scaled=scaler.fit_transform(df.values)\n",
    "    scaled = pd.DataFrame(scaled,index=df.index,columns=df.columns)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ONdk510Cw9pc"
   },
   "outputs": [],
   "source": [
    "def read_zip_to_panda(zip_filename,csv_filename):\n",
    "    zip_handle = ZipFile(zip_filename)\n",
    "    csv_handle = zip_handle.open(csv_filename)\n",
    "    panda = pd.read_csv(csv_handle)\n",
    "    return panda\n",
    "def fix_date_type(panda):\n",
    "    # Convert the given timestamp column to the pandas datetime data type.\n",
    "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
    "    indexed = panda.set_index(['timestamp'])\n",
    "    return indexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "6YVYM_bqw9pi",
    "outputId": "c1e69491-9db0-462f-9125-ecbde03fe4af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>doy</th>\n",
       "      <th>airTemperature</th>\n",
       "      <th>cloudCoverage</th>\n",
       "      <th>dewTemperature</th>\n",
       "      <th>precipDepth1HR</th>\n",
       "      <th>precipDepth6HR</th>\n",
       "      <th>seaLvlPressure</th>\n",
       "      <th>windDirection</th>\n",
       "      <th>windSpeed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-31 18:00:00</th>\n",
       "      <td>0.938811</td>\n",
       "      <td>1.588662</td>\n",
       "      <td>1.722611</td>\n",
       "      <td>-2.403469</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>-2.519146</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.317034</td>\n",
       "      <td>1.172113</td>\n",
       "      <td>-0.231861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 20:00:00</th>\n",
       "      <td>1.227759</td>\n",
       "      <td>1.588662</td>\n",
       "      <td>1.722611</td>\n",
       "      <td>-2.512407</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>-2.566121</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.422443</td>\n",
       "      <td>1.090560</td>\n",
       "      <td>-0.764679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 21:00:00</th>\n",
       "      <td>1.372234</td>\n",
       "      <td>1.588662</td>\n",
       "      <td>1.722611</td>\n",
       "      <td>-2.571828</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>-2.566121</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.448795</td>\n",
       "      <td>1.009007</td>\n",
       "      <td>-0.231861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:00:00</th>\n",
       "      <td>1.516708</td>\n",
       "      <td>1.588662</td>\n",
       "      <td>1.722611</td>\n",
       "      <td>-2.571828</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>-2.519146</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.475147</td>\n",
       "      <td>1.172113</td>\n",
       "      <td>0.010328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 23:00:00</th>\n",
       "      <td>1.661182</td>\n",
       "      <td>1.588662</td>\n",
       "      <td>1.722611</td>\n",
       "      <td>-2.571828</td>\n",
       "      <td>-0.399371</td>\n",
       "      <td>-2.519146</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.475147</td>\n",
       "      <td>1.090560</td>\n",
       "      <td>0.736897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         hour     month  ...  windDirection  windSpeed\n",
       "timestamp                                ...                          \n",
       "2017-12-31 18:00:00  0.938811  1.588662  ...       1.172113  -0.231861\n",
       "2017-12-31 20:00:00  1.227759  1.588662  ...       1.090560  -0.764679\n",
       "2017-12-31 21:00:00  1.372234  1.588662  ...       1.009007  -0.231861\n",
       "2017-12-31 22:00:00  1.516708  1.588662  ...       1.172113   0.010328\n",
       "2017-12-31 23:00:00  1.661182  1.588662  ...       1.090560   0.736897\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATE_PARSE=True  # must be true if we use one of these as predictor\n",
    "def load_weather_for_site(site):\n",
    "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
    "    wet_df = fix_date_type(wet_df)\n",
    "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
    "    # Drop the site, which is constant (we selected for one site).\n",
    "    site_df = site_df.drop(['site_id'],axis=1)\n",
    "    if DATE_PARSE:\n",
    "        site_df.insert(0,'hour',0)\n",
    "        site_df.insert(1,'month',0)\n",
    "        site_df.insert(2,'doy',0)\n",
    "        L=len(site_df)\n",
    "        for i in range(0,L):\n",
    "            dt=site_df.index[i]\n",
    "            hour=dt.hour\n",
    "            month=dt.month\n",
    "            doy=dt.dayofyear\n",
    "            site_df.iat[i,0] = hour\n",
    "            site_df.iat[i,1] = month\n",
    "            site_df.iat[i,2] = doy\n",
    "    site_df = scale(site_df) # could break if any column is empty\n",
    "    return site_df\n",
    "\n",
    "one_site_weather = load_weather_for_site(SITE)\n",
    "one_site_weather.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "s-EKuCBibz9d",
    "outputId": "e8e6fdf6-44c2-4ab1-86a1-fa08e59662c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-31 19:00:00</th>\n",
       "      <td>92.2957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 20:00:00</th>\n",
       "      <td>277.5584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 21:00:00</th>\n",
       "      <td>280.5331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 22:00:00</th>\n",
       "      <td>289.3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31 23:00:00</th>\n",
       "      <td>164.3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        meter\n",
       "timestamp                    \n",
       "2017-12-31 19:00:00   92.2957\n",
       "2017-12-31 20:00:00  277.5584\n",
       "2017-12-31 21:00:00  280.5331\n",
       "2017-12-31 22:00:00  289.3302\n",
       "2017-12-31 23:00:00  164.3474"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_meter_for_building(bldg,smooth=0):\n",
    "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
    "    all_df = fix_date_type(all_df)\n",
    "    global SITE_BUILDINGS\n",
    "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
    "    site_series = all_df[bldg]\n",
    "    site_df = site_series.to_frame()\n",
    "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
    "    # Change column name from building name to meter.\n",
    "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
    "    if smooth>0:\n",
    "        site_df = site_df.rolling(smooth).mean()\n",
    "    return site_df\n",
    "\n",
    "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
    "print(type(one_bldg_meter))\n",
    "one_bldg_meter.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VynRgLt9w9pk",
    "outputId": "256cb60e-f0ef-4c1b-a582-9659764dea5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       meter\n",
      "timestamp                   \n",
      "2016-01-01 00:00:00  31.7661\n",
      "2016-01-01 01:00:00  27.4004\n",
      "2016-01-01 02:00:00  38.4989\n",
      "2016-01-01 03:00:00  59.1697\n",
      "2016-01-01 04:00:00  39.9556\n",
      "X shape: (17508, 12, 10)\n",
      "y shape: (17508, 12)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_learning(wdf,mdf):\n",
    "    # Concatenate weather and meter.\n",
    "    df = pd.concat([wdf,mdf],axis=1)\n",
    "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
    "    X_shape = (num_samples,STEPS_FUTURE,NUM_PREDICTORS)\n",
    "    Y_shape = (num_samples,STEPS_FUTURE)\n",
    "    X=np.zeros(X_shape)\n",
    "    y=np.zeros(Y_shape)\n",
    "    predictor_series = df[PREDICTORS].values  # selected features\n",
    "    predicted_series = df[PREDICTED_VARIABLE].values  # meter\n",
    "    # TO DO: can we take predicted from mdf instead?\n",
    "    for sam in range (0,num_samples): \n",
    "        prev_val = 0\n",
    "        one_sample = predictor_series[sam:sam+STEPS_FORWARD]\n",
    "        for time in range (0,STEPS_FORWARD): \n",
    "            one_period = one_sample[time]\n",
    "            for feat in range (0,NUM_PREDICTORS):\n",
    "                val = one_period[feat]\n",
    "                if np.isnan(val):\n",
    "                    val = prev_val\n",
    "                else:\n",
    "                    prev_val = val\n",
    "                X[sam,time,feat] = val\n",
    "        for time in range (0,STEPS_FUTURE):  \n",
    "            y[sam,time]=predicted_series[sam+STEPS_HISTORY+time]\n",
    "    return X,y \n",
    "print(one_bldg_meter.head())\n",
    "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
    "print(\"X shape:\",X.shape)\n",
    "print(\"y shape:\",y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mObWmpMDVuNQ",
    "outputId": "e9891734-77f0-47c7-b75d-6d20b34dc8f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X columns: ['hour', 'month', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
      "X example:\n",
      " [[-1 -1  0 -2 -2  0  0  2  1  1]\n",
      " [ 0 -1  0 -2 -2  0  0  2  1  1]\n",
      " [ 0 -1  0 -2 -2  0  0  2 -1  1]\n",
      " [ 0 -1  0 -2 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -2 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -2 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -2 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -1 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -1 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -1 -2  0  0  2  1  0]\n",
      " [ 0 -1 -1 -1 -2  0  0  2  1  0]\n",
      " [ 0 -1  0 -1 -2  0  0  2  1  0]]\n",
      "y example:\n",
      " [ 43 119 327 322 273  92 328 363 346 168 266  27]\n"
     ]
    }
   ],
   "source": [
    "print(\"X columns:\",PREDICTORS)\n",
    "print(\"X example:\\n\",X[100].astype(int))\n",
    "print(\"y example:\\n\",y[100].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "z_8rzumTw9p2"
   },
   "outputs": [],
   "source": [
    "def make_RNN():\n",
    "    # The GRU in Keras is optimized for speed on CoLab GPU.\n",
    "    rnn = Sequential([\n",
    "        GRU(16,return_sequences=True, \n",
    "                  input_shape=(STEPS_FORWARD,NUM_PREDICTORS)), \n",
    "        GRU(16,return_sequences=True),\n",
    "        GRU(16,return_sequences=False),\n",
    "        Dense(STEPS_FUTURE)\n",
    "    ])\n",
    "    rnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XypnRqq9w9p4",
    "outputId": "4297eb00-c6ab-4774-8508-c81db5c05cea",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Eagle_office_Lamont\n",
      " Count bad values before pseudofill: 8\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [60 74 71 71 74 71 77 70 65 72 74 69]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 20s 5ms/step - loss: 1259.2522\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 924.2496\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 732.0830\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 572.5516\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 460.0360\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 380.9905\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 325.9398\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 281.8179\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 267.0267\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 257.6949\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 248.2706\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 250.4910\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 238.8327\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 247.6621\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 236.6458\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 145.1731\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 125.1914\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 104.0619\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 91.1483\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 84.0953\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 79.2469\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 72.4233\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 66.0353\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 66.0501\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 60.7243\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62.0269\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63.6412\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62.3294\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63.3849\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56.9622\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 58.3457\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56.8818\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 55.6890\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 51.8226\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53.2472\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 56.0930\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 51.5676\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 53.3772\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53.5525\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49.3844\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48.8579\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 50.3586\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 50.8234\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47.6237\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 45.9449\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48.5632\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49.3999\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49.3462\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49.5001\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48.3984\n",
      "mean,rmse,rmse/mean,bldg: 36.926709665830415 20.7914774714427 0.5630471184569631 Eagle_office_Lamont\n",
      "Example prediction:\n",
      " 0 [42 42 42 42 42 42 42 42 42 42 42 42]\n",
      "Example prediction:\n",
      " 2 [44 44 43 44 44 43 43 43 43 43 43 43]\n",
      "Example prediction:\n",
      " 4 [43 43 42 43 43 43 43 43 43 43 42 43]\n",
      "Example prediction:\n",
      " 6 [42 42 42 42 42 42 42 42 42 42 42 42]\n",
      "Example prediction:\n",
      " 8 [43 43 43 43 43 43 43 43 43 43 42 43]\n",
      "Example prediction:\n",
      " 10 [43 43 43 43 43 43 43 43 43 43 43 43]\n",
      "Example prediction:\n",
      " 12 [43 43 43 43 43 43 43 43 43 43 43 43]\n",
      "Example prediction:\n",
      " 14 [42 42 42 42 42 42 42 42 42 42 42 42]\n",
      "Example prediction:\n",
      " 16 [41 41 41 42 42 41 42 41 41 42 41 42]\n",
      "Example prediction:\n",
      " 18 [41 41 41 41 41 41 42 41 41 41 41 41]\n",
      "Example prediction:\n",
      " 20 [42 42 42 42 42 42 42 42 42 42 42 42]\n",
      "Example prediction:\n",
      " 22 [42 42 42 42 42 42 42 42 42 42 42 42]\n",
      "Building Eagle_health_Athena\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_3 (GRU)                  (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [1383 1299 1280  984  892 1005  606  847  948 1203 1312 1234]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 362630.1716\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 361023.2502\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 357978.9576\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 354408.5425\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 341631.1948\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 337406.1402\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 341298.4082\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 334981.5203\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 320994.9135\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 321474.9367\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 312696.4422\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 311411.8173\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 302124.6031\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 298372.4107\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 292781.4691\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 296121.9603\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 4ms/step - loss: 291916.1191\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 283890.6172\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 280760.8369\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 280212.4870\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 272091.8017\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 271804.1212\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 266967.3761\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 263655.2965\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 265940.7306\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 257853.8449\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 249221.2835\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 248919.6898\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 246215.3232\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 241502.7618\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 241711.1845\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 237612.8199\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 227975.7440\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 227887.8285\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 227455.8387\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 220799.9105\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 219289.1376\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 211771.4734\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 210783.7852\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 208238.2548\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 206060.5263\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 206363.3600\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 199317.5124\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 194678.9420\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 192549.2554\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 193203.6824\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 189627.2021\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 190018.0799\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 178059.4723\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 182331.1725\n",
      "mean,rmse,rmse/mean,bldg: 477.70168061445776 325.21505956148695 0.6807911145365232 Eagle_health_Athena\n",
      "Example prediction:\n",
      " 0 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 2 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 4 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 6 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 8 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 10 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 12 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 14 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 16 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 18 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 20 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Example prediction:\n",
      " 22 [227 224 225 227 226 225 226 226 224 225 225 226]\n",
      "Building Eagle_assembly_Herbert\n",
      " Count bad values before pseudofill: 393\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [135 149  44  80  12  32  30  30  26  29  33 172]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 84645.9905\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 82423.5917\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 78864.6594\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 80061.1936\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 78246.1087\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 78260.6247\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 76476.4095\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 73261.9846\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 71505.5134\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 69139.9196\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 70058.3826\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 67104.4368\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 66298.8605\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 64360.6716\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 65147.7835\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63221.7413\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62750.8555\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 60709.7140\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62374.9934\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 59045.6451\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 61805.0942\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 57797.6919\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 55405.7412\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 57138.2486\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53719.6303\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56545.5398\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 54281.8212\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 52525.1391\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53199.3336\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53381.2244\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53934.2138\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 51916.8660\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 50933.4496\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49114.6196\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46944.3855\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46104.8540\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47610.6993\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47153.4654\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44265.7638\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46013.3864\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 43067.0462\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44497.9198\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41052.2785\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41057.8831\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40605.6109\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40896.3656\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41005.9841\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 37698.9185\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 39399.7559\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 37294.8015\n",
      "mean,rmse,rmse/mean,bldg: 216.03328870036609 259.0838400215033 1.1992773964610959 Eagle_assembly_Herbert\n",
      "Example prediction:\n",
      " 0 [135 134 134 135 133 134 135 135 135 134 135 134]\n",
      "Example prediction:\n",
      " 2 [145 143 144 144 143 144 145 145 144 143 144 143]\n",
      "Example prediction:\n",
      " 4 [153 151 152 153 152 152 153 154 153 152 153 152]\n",
      "Example prediction:\n",
      " 6 [152 151 151 152 151 152 152 153 152 151 152 151]\n",
      "Example prediction:\n",
      " 8 [154 152 153 154 152 153 154 154 154 152 154 153]\n",
      "Example prediction:\n",
      " 10 [152 150 151 152 151 152 152 153 152 151 152 151]\n",
      "Example prediction:\n",
      " 12 [151 149 150 151 149 150 151 152 151 150 151 150]\n",
      "Example prediction:\n",
      " 14 [154 153 153 154 153 154 154 155 154 153 154 153]\n",
      "Example prediction:\n",
      " 16 [154 152 153 154 153 154 154 155 154 153 154 153]\n",
      "Example prediction:\n",
      " 18 [152 151 152 152 151 152 152 153 152 151 152 151]\n",
      "Example prediction:\n",
      " 20 [152 150 151 152 150 151 152 153 152 151 152 151]\n",
      "Example prediction:\n",
      " 22 [153 151 152 152 151 152 152 153 153 151 152 151]\n",
      "Building Eagle_public_Alvin\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_9 (GRU)                  (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [687 432 286 547 109 237 541 523 486 535 389 426]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 59946.1518\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 59241.5356\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56867.1574\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 54456.2052\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53789.5811\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 52264.7516\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 50512.2537\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48707.3576\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47442.7001\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 45502.7836\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 43623.4725\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 42995.3086\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41092.5977\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40585.7936\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40176.4429\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 38348.5016\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 37161.2467\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 35729.4666\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 35640.3025\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 33736.4174\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 32573.1639\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 31931.4696\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 30443.1588\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 30188.7162\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 28799.3916\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 28301.3898\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 27335.9412\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 26462.1553\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 25955.1736\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 25702.1627\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 24734.0442\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 24351.3178\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 23843.6250\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 23287.1383\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 23004.8094\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 21816.6269\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 22005.4128\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20815.9094\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20650.2691\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20758.0254\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20024.9333\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19850.5350\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19432.9444\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18581.9825\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17798.8160\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17633.5981\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16892.6102\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16918.9700\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16567.2024\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15806.3544\n",
      "mean,rmse,rmse/mean,bldg: 182.08293646830808 65.43523104348765 0.3593704732177185 Eagle_public_Alvin\n",
      "Example prediction:\n",
      " 0 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 2 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 4 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 6 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 8 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 10 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 12 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 14 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 16 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 18 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 20 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Example prediction:\n",
      " 22 [201 201 200 201 201 199 201 201 201 200 200 202]\n",
      "Building Eagle_education_Raul\n",
      "Building Eagle_education_Roman\n",
      " Count bad values before pseudofill: 23\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [2875 2422 2129 2760 3130 3071 3131 1877 3442 3736 3785 3729]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 1754099.5850\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1727443.6573\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1719193.1200\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1737996.3155\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1682209.4891\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1698153.2782\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1716604.2018\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1622609.2386\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1643737.3382\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1664849.6655\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1632080.9936\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1611958.0336\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1596652.5195\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1628139.1791\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1552515.4182\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1593276.1345\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1557926.5532\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1575342.5441\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1523051.1136\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1513486.7795\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1531589.9523\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1559957.0414\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1495108.4711\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1489508.6277\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1479558.3714\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1502328.8277\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1486846.0968\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1468490.6700\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1443552.5905\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1417093.3323\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1430503.3082\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1419261.4461\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1413550.6559\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1411181.6868\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1364969.8927\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1396365.4714\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1365416.2945\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1342548.4623\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1340555.8355\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1342968.3386\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1323233.9832\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1325562.2932\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1302685.0055\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1294166.5855\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1307579.8323\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1279297.1920\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1262042.9718\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1314652.7718\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1252926.5995\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1240999.5575\n",
      "mean,rmse,rmse/mean,bldg: 1199.4083831231103 1084.3451021015048 0.9040666359843217 Eagle_education_Roman\n",
      "Example prediction:\n",
      " 0 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 2 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 4 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 6 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 8 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 10 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 12 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 14 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 16 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 18 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 20 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Example prediction:\n",
      " 22 [231 230 231 232 233 232 232 231 230 231 230 230]\n",
      "Building Eagle_office_Mandi\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_15 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [63 63 63 63 63 63 63 63 64 63 62 63]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 1521.1332\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1127.2019\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 899.0594\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 714.2023\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 588.6558\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 490.4944\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 412.5698\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 362.3023\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 314.9704\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 300.6610\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 290.2955\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 280.1183\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 274.8828\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 271.5171\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 275.1081\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 274.6218\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 190.2366\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 141.8431\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 113.1467\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 97.0218\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 88.8904\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 73.2126\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 66.1021\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63.3422\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62.6299\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 58.5143\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56.0433\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 57.5852\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 52.1696\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46.8782\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48.3786\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 51.7071\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44.5133\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48.0097\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46.2169\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 45.0021\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46.4776\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46.1664\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47.5815\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44.7887\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 42.1278\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 42.3917\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46.4532\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 42.1785\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 43.0312\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41.8372\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40.5059\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40.9297\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 38.2606\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 37.7712\n",
      "mean,rmse,rmse/mean,bldg: 35.89375163018761 16.27909756291229 0.45353569419645545 Eagle_office_Mandi\n",
      "Example prediction:\n",
      " 0 [54 54 54 54 55 54 54 54 54 54 54 54]\n",
      "Example prediction:\n",
      " 2 [56 56 56 56 56 56 56 56 56 56 56 55]\n",
      "Example prediction:\n",
      " 4 [55 55 55 55 55 55 55 55 55 55 55 54]\n",
      "Example prediction:\n",
      " 6 [52 52 52 52 52 52 52 52 52 52 52 52]\n",
      "Example prediction:\n",
      " 8 [53 53 53 53 53 53 53 53 53 53 53 53]\n",
      "Example prediction:\n",
      " 10 [48 48 48 48 48 48 48 48 48 48 48 48]\n",
      "Example prediction:\n",
      " 12 [49 48 49 49 49 48 49 48 48 48 48 48]\n",
      "Example prediction:\n",
      " 14 [53 53 53 53 53 53 53 53 53 53 53 52]\n",
      "Example prediction:\n",
      " 16 [53 53 53 53 53 53 53 53 53 53 53 52]\n",
      "Example prediction:\n",
      " 18 [49 49 49 49 49 49 49 49 49 49 49 49]\n",
      "Example prediction:\n",
      " 20 [61 61 61 61 61 61 61 61 61 61 61 60]\n",
      "Example prediction:\n",
      " 22 [61 61 61 61 61 61 61 61 61 61 61 60]\n",
      "Building Eagle_education_Jewell\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_18 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [214 204 199 217 199 198 192 219 219 212 235 220]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 4440.4738\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4181.2891\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4125.7282\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3773.9257\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3702.0918\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3591.6842\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3242.6056\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3251.2600\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3076.7253\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2783.9003\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2856.7890\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2688.6970\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2567.7118\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2542.1643\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2448.7697\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2315.5838\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2244.4463\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2128.6606\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2153.6754\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1984.4070\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1965.1770\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1852.4485\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1715.0298\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1647.5412\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1586.2930\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1545.3106\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1547.0913\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1427.7419\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1424.0176\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1379.8675\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1334.0469\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1266.9767\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1268.4263\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1205.5080\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1129.6527\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1149.2997\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1118.5692\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1076.0774\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1014.7045\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 975.4497\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 958.3397\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 928.8956\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 932.4942\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 912.5555\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 904.8853\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 895.8623\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 875.2814\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 863.0708\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 827.4860\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 836.3186\n",
      "mean,rmse,rmse/mean,bldg: 15.763073324212357 53.095537263674544 3.3683493168885326 Eagle_education_Jewell\n",
      "Example prediction:\n",
      " 0 [13 12 12 11 12 11 12 10 11 12 11 13]\n",
      "Example prediction:\n",
      " 2 [-2 -3 -3 -4 -4 -4 -4 -5 -4 -3 -4 -3]\n",
      "Example prediction:\n",
      " 4 [ 0 -1 -2 -2 -2 -2 -2 -4 -2 -1 -2 -1]\n",
      "Example prediction:\n",
      " 6 [2 2 1 1 1 1 1 0 1 2 1 2]\n",
      "Example prediction:\n",
      " 8 [2 2 1 1 0 0 1 0 0 1 1 2]\n",
      "Example prediction:\n",
      " 10 [2 1 1 1 0 0 0 0 0 1 1 2]\n",
      "Example prediction:\n",
      " 12 [3 2 2 1 1 1 1 0 1 2 2 3]\n",
      "Example prediction:\n",
      " 14 [14 13 13 13 13 12 12 11 12 13 12 13]\n",
      "Example prediction:\n",
      " 16 [20 19 19 19 19 18 18 17 18 19 18 19]\n",
      "Example prediction:\n",
      " 18 [31 31 30 31 30 30 30 28 29 30 29 30]\n",
      "Example prediction:\n",
      " 20 [40 40 40 40 40 40 39 38 39 39 38 39]\n",
      "Example prediction:\n",
      " 22 [17 16 16 16 15 15 15 15 15 15 15 16]\n",
      "Building Eagle_office_Henriette\n",
      " Count bad values before pseudofill: 117\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_21 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_22 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_23 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 0.0059\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.2027e-04\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.9195e-05\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.7349e-05\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.8206e-05\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.2635e-05\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.2395e-06\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6.7539e-06\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5.4894e-06\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.2370e-06\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3.4676e-06\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.8819e-06\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.3241e-06\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.9402e-06\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.6367e-06\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.4196e-06\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.1568e-06\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.6678e-07\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8.5142e-07\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7.5135e-07\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6.0417e-07\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6.1803e-07\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.8233e-07\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.2935e-07\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.3065e-07\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3.4805e-07\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.9980e-07\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.5304e-07\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.9378e-07\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.2562e-07\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.8144e-07\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3.2820e-07\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.2883e-07\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.5354e-07\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.1075e-07\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.5564e-07\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.3995e-08\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.1729e-07\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.5256e-07\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1.0048e-07\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8.1597e-08\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2.3096e-07\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.0518e-08\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7.9716e-08\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4.6965e-08\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4.2078e-08\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5.7143e-08\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3.4969e-08\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6.0871e-08\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5.8063e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean,rmse,rmse/mean,bldg: 0.0 0.0001289497597374273 inf Eagle_office_Henriette\n",
      "Example prediction:\n",
      " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Building Eagle_health_Reba\n",
      " Count bad values before pseudofill: 26\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_24 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_25 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [1763 1718  316 3241 3271 2523 2609 2517 2709 2640 2532 2604]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 1458646.8314\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1454537.2709\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1444569.0418\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1409946.2491\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1424310.3077\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1417462.7973\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1378422.2664\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1379828.5073\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1399609.7627\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1356747.9218\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1358916.4482\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1370237.7177\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1346758.0077\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1320984.8664\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1318959.9493\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1313754.1045\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1300744.1866\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1279994.8573\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1302344.2695\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1295902.7886\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1276047.5986\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1287189.2659\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1212898.4577\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1257458.8195\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1224881.6836\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1209173.6823\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1199428.4570\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1189131.9164\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1170091.9643\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1175598.3402\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1160897.0832\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1153091.2727\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1166717.5941\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1149715.4527\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1143063.1105\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1121153.2825\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1128363.9927\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1119317.3414\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1113011.1545\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1083744.8998\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1111394.3745\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1074313.8589\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1057826.6893\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1054525.5125\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1057148.4218\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1053509.9043\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1043474.4161\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1003694.5482\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 999906.2861\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1006690.9800\n",
      "mean,rmse,rmse/mean,bldg: 1084.285472399815 927.1432214410983 0.8550729905004456 Eagle_health_Reba\n",
      "Example prediction:\n",
      " 0 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 2 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 4 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 6 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 8 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 10 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 12 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 14 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 16 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 18 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 20 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Example prediction:\n",
      " 22 [231 231 231 232 231 231 232 232 231 230 231 229]\n",
      "Building Eagle_lodging_Edgardo\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_27 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_28 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_29 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [302  31 277 213  37 302 339 133 232 186 311 281]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 12246.3095\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11192.1289\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10553.2830\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10230.8770\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9790.4260\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9192.1246\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8861.8273\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8317.5594\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7827.1248\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7731.2211\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7454.1250\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7143.3735\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6749.3738\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6519.1390\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6456.9362\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6355.8955\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6029.7697\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6111.0366\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6001.7538\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5951.4735\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5846.3993\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5984.5776\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5785.4440\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5813.3525\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5687.7891\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5483.9607\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5320.9382\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5073.8775\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4990.4450\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4986.6343\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4854.5684\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4833.8762\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4763.5926\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4721.4293\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4651.6974\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4669.1083\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4556.7113\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4464.1536\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4563.3459\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4334.1260\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4418.0326\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4310.2236\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4310.7573\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4130.1918\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4190.1861\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4336.9309\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4267.7209\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4131.3924\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4215.5865\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4169.3981\n",
      "mean,rmse,rmse/mean,bldg: 81.9677919573641 70.21742444035335 0.8566465286374586 Eagle_lodging_Edgardo\n",
      "Example prediction:\n",
      " 0 [133 133 134 134 134 134 134 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 2 [133 133 134 134 134 134 134 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 4 [133 133 134 134 134 134 134 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 6 [133 133 134 134 134 134 134 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 8 [133 133 134 134 134 134 134 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 10 [126 127 127 127 127 127 127 127 126 126 126 126]\n",
      "Example prediction:\n",
      " 12 [114 115 115 116 115 115 115 115 115 114 114 114]\n",
      "Example prediction:\n",
      " 14 [122 123 123 124 123 123 123 123 123 122 122 122]\n",
      "Example prediction:\n",
      " 16 [130 131 132 132 131 131 131 131 131 130 130 130]\n",
      "Example prediction:\n",
      " 18 [133 133 134 134 134 133 133 134 133 133 132 132]\n",
      "Example prediction:\n",
      " 20 [132 132 133 133 133 133 133 133 132 132 132 131]\n",
      "Example prediction:\n",
      " 22 [132 133 134 134 133 133 133 133 133 132 132 132]\n",
      "Building Eagle_education_Cassie\n",
      "Building Eagle_education_Peter\n",
      " Count bad values before pseudofill: 20\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_30 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_31 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_32 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [7939 7898 8232 8007 7908 8090 8193 8008 8359 8113 8199 8170]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 13122800.5018\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12868728.5236\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13401581.6909\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12993127.0727\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13071437.6618\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13050293.1200\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12820101.5855\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12784242.7236\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12624210.0618\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12782475.1382\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12804116.0436\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12640929.4945\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12559061.7673\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12714337.7200\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12748376.3455\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12685252.3782\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12448409.0982\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12710158.3782\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12401406.1745\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12678841.6291\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12300319.5273\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12372100.7455\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12259191.8218\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12301191.8182\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12404993.4218\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12362354.9236\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12131008.7636\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12215641.7818\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12188268.0691\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12333891.2655\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12117174.6364\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12150940.1055\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12088633.5636\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12087738.1818\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12092983.6255\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12044263.5418\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12126905.6218\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12071894.0618\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12040852.1709\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11978412.0982\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11938675.2582\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11981998.0727\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12117555.8073\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11604400.9673\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11903677.8473\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11835879.1709\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11760017.3855\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11536545.4509\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11573464.4291\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11520044.4145\n",
      "mean,rmse,rmse/mean,bldg: 3153.28359767175 3042.3994332336474 0.964835334024515 Eagle_education_Peter\n",
      "Example prediction:\n",
      " 0 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 2 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 4 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 6 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 8 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 10 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 12 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 14 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 16 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 18 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 20 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Example prediction:\n",
      " 22 [234 233 233 235 232 234 234 233 233 232 234 234]\n",
      "Building Eagle_health_Gregoria\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_33 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_34 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_35 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 4s 5ms/step - loss: 657038.9174\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 675976.6798\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 653679.2175\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 638859.3959\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 654733.2105\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 644307.5136\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 622774.1015\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 661523.9527\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 621699.3377\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 631334.1730\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 594355.5494\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 615966.8943\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 609542.6757\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 594677.1189\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 610252.6189\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 608146.0767\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 580564.0683\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 594108.7218\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 579233.8452\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 574791.6614\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 564478.0847\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 550626.5659\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 568827.5011\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 540036.3256\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 572959.0307\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 561436.7412\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 562684.0995\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 548764.1601\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 569887.1740\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 565731.2900\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 557288.1525\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 548889.5492\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 542728.0332\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 532205.8694\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 513589.0781\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 529968.8570\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 513953.4545\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 516698.7953\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 513807.5856\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 511181.2993\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 517099.4934\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 529265.8085\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 523016.1514\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 480572.9436\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 503866.7969\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 508467.7594\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 525167.8233\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 490316.5369\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 488286.9081\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 492314.2036\n",
      "mean,rmse,rmse/mean,bldg: 659.65505791154 780.729085664873 1.1835414225984289 Eagle_health_Gregoria\n",
      "Example prediction:\n",
      " 0 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 2 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 4 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 6 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 8 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 10 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 12 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 14 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 16 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 18 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 20 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Example prediction:\n",
      " 22 [216 216 218 217 216 218 220 218 216 216 216 217]\n",
      "Building Eagle_lodging_Dawn\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_36 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_37 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_38 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [269 247 276 320 371 276 320 283 312 305 320 254]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 15017.4913\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13825.9566\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12959.8112\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12203.0206\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11410.2645\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10786.2926\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10194.1452\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9560.8135\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8985.7183\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8339.7755\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8103.7794\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7606.7585\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7477.6775\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7099.0701\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6632.7353\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6264.2866\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5889.7802\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5906.7640\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5408.8045\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5504.0259\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5270.6839\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5161.6752\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5120.3741\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4897.7592\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4722.4749\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4902.9540\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4823.1908\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4712.6652\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4693.4631\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4614.9074\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4685.0511\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4573.0894\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4109.2100\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4084.4564\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3870.6976\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3811.1544\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3658.9252\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3607.8567\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3650.8742\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3452.1606\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3406.8237\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3345.9357\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3202.7230\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3297.6702\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3182.2115\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3095.0913\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3074.0214\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2911.0290\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2876.8554\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2900.6317\n",
      "mean,rmse,rmse/mean,bldg: 92.82705527815749 62.98292986614021 0.6784975530831074 Eagle_lodging_Dawn\n",
      "Example prediction:\n",
      " 0 [98 97 98 98 98 98 98 98 98 98 98 98]\n",
      "Example prediction:\n",
      " 2 [101 101 101 101 101 101 101 101 101 101 101 101]\n",
      "Example prediction:\n",
      " 4 [100 100  99  99 100 100  99  99 100 100 100 100]\n",
      "Example prediction:\n",
      " 6 [111 112 111 112 112 112 111 111 112 112 111 112]\n",
      "Example prediction:\n",
      " 8 [114 114 114 114 114 114 114 114 115 114 114 114]\n",
      "Example prediction:\n",
      " 10 [119 119 119 119 119 119 119 119 119 119 119 119]\n",
      "Example prediction:\n",
      " 12 [139 140 140 140 140 140 140 140 140 139 140 140]\n",
      "Example prediction:\n",
      " 14 [145 145 146 146 145 146 146 146 146 145 145 145]\n",
      "Example prediction:\n",
      " 16 [132 132 132 132 132 132 132 133 133 132 132 132]\n",
      "Example prediction:\n",
      " 18 [143 143 143 143 143 143 143 144 143 143 143 143]\n",
      "Example prediction:\n",
      " 20 [154 155 155 155 155 155 155 156 156 154 155 155]\n",
      "Example prediction:\n",
      " 22 [148 148 148 148 148 149 148 149 149 148 148 148]\n",
      "Building Eagle_office_Nereida\n",
      " Count bad values before pseudofill: 8\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_39 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_40 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_41 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [448 548 525 531 554 525 571 524 487 533 548 510]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 4s 5ms/step - loss: 73060.0785\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 70935.5739\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 68246.3736\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 65247.5766\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63904.2335\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 62731.9100\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 60056.3832\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 58027.2653\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 55922.7688\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 54408.8777\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 52829.1221\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 50785.1783\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 49199.9483\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47248.4900\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 46294.8605\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44211.5906\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 43849.9073\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40919.2687\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40159.6429\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 38800.9055\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 37403.8061\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 35921.7560\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 35210.4865\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 33624.9094\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 33176.9604\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 31675.2094\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 29918.0836\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 29676.8359\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 28410.4065\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 27508.8920\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 26407.2861\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 25770.6173\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 24644.1101\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 23946.7129\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 22898.5485\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 22761.3752\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 21461.1939\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20906.0324\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19750.1591\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19583.4882\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18911.4540\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18246.1153\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17680.1523\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17864.4681\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17292.2375\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17042.6821\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16218.4269\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15720.8982\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15319.7134\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 14857.0254\n",
      "mean,rmse,rmse/mean,bldg: 272.95891697080225 138.88916146676058 0.508828079361178 Eagle_office_Nereida\n",
      "Example prediction:\n",
      " 0 [206 207 207 205 205 206 207 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 2 [206 207 207 205 205 206 207 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 4 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 6 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 8 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 10 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 12 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 14 [206 207 207 205 205 206 207 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 16 [206 207 207 205 205 206 207 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 18 [206 207 207 205 205 206 207 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 20 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Example prediction:\n",
      " 22 [206 207 207 205 205 206 206 205 206 206 207 205]\n",
      "Building Eagle_lodging_Tressa\n",
      "Building Eagle_education_Eileen\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_42 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_43 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_44 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [110  80  64  48  30  35  39 133 105  99  94  72]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 2712.5292\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2237.6028\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1931.1773\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1708.2116\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1437.3590\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1352.7965\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1220.4304\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1103.3978\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1041.5901\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 989.3654\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 951.1596\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 917.3755\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 917.5937\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 880.8537\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 886.6484\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 881.8094\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 871.3243\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 886.8575\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 873.4905\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 887.6816\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 796.9459\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 730.4186\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 701.3697\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 698.6445\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 660.0763\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 636.1878\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 635.2889\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 631.3476\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 613.8648\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 611.3213\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 609.3495\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 584.4860\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 604.5207\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 580.1476\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 562.6887\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 573.8452\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 560.7090\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 565.9138\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 560.8887\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 551.2181\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 553.2681\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 547.6840\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 539.4035\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 535.9025\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 556.2406\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 532.2242\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 522.2489\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 520.2850\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 530.7540\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 523.4845\n",
      "mean,rmse,rmse/mean,bldg: 46.463674127906486 32.38248413366295 0.6969419603908109 Eagle_education_Eileen\n",
      "Example prediction:\n",
      " 0 [63 63 63 64 64 64 64 64 63 64 64 63]\n",
      "Example prediction:\n",
      " 2 [61 61 61 62 62 62 62 62 62 62 62 62]\n",
      "Example prediction:\n",
      " 4 [61 61 61 61 62 61 61 61 61 61 62 62]\n",
      "Example prediction:\n",
      " 6 [61 62 62 62 62 62 62 62 62 62 63 62]\n",
      "Example prediction:\n",
      " 8 [65 66 66 66 66 66 66 66 66 66 66 66]\n",
      "Example prediction:\n",
      " 10 [72 72 72 73 73 73 73 73 73 73 72 71]\n",
      "Example prediction:\n",
      " 12 [58 58 58 58 58 58 59 58 58 59 57 56]\n",
      "Example prediction:\n",
      " 14 [53 53 53 53 54 54 54 54 54 54 53 52]\n",
      "Example prediction:\n",
      " 16 [53 53 53 54 54 54 54 54 54 54 53 53]\n",
      "Example prediction:\n",
      " 18 [59 59 59 59 59 59 60 59 59 59 60 59]\n",
      "Example prediction:\n",
      " 20 [57 57 57 57 57 57 58 57 57 57 58 57]\n",
      "Example prediction:\n",
      " 22 [47 47 47 47 47 47 47 47 47 47 47 47]\n",
      "Building Eagle_education_Wesley\n",
      " Count bad values before pseudofill: 40\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_45 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_46 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_47 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 0.0092\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0012\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0012\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0011\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.9847e-04\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 0.0010\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.9580e-04\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.8577e-04\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.8629e-04\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.9130e-04\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.7640e-04\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.7941e-04\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.7355e-04\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.6504e-04\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9.6564e-04\n",
      "mean,rmse,rmse/mean,bldg: 0.10542488002742212 0.03779922077513979 0.3585417480703588 Eagle_education_Wesley\n",
      "Example prediction:\n",
      " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Building Eagle_health_Vincenza\n",
      " Count bad values before pseudofill: 57\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_48 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_49 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_50 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [264 252  91  71  82  85  86  87 306 298 313 277]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 18046.1740\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16407.9373\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15633.1734\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 14718.4960\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13885.0423\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13006.6880\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12097.4311\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11403.5418\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10580.4783\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10057.7326\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9588.9507\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9022.7219\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8364.1234\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7997.9842\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7526.6733\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7135.7874\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6839.8848\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6336.8176\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6098.3492\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5807.2885\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5534.1874\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5103.7692\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5047.6545\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4844.2285\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4807.7077\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4601.7079\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4561.2844\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4458.3266\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4359.1123\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4350.9222\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4276.1362\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4223.5584\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4204.9286\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4171.8462\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4155.0665\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4150.1793\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4182.2337\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4152.3838\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4170.1623\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3749.0827\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3556.8986\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3319.3710\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3232.4225\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3190.6101\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3147.3371\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3098.7359\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3059.7632\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3000.9525\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2957.9813\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2880.6670\n",
      "mean,rmse,rmse/mean,bldg: 122.34682110710833 56.355879069634796 0.46062397502177954 Eagle_health_Vincenza\n",
      "Example prediction:\n",
      " 0 [147 148 149 149 150 150 150 150 150 149 149 148]\n",
      "Example prediction:\n",
      " 2 [132 132 133 133 133 133 134 133 133 133 132 132]\n",
      "Example prediction:\n",
      " 4 [132 132 132 133 133 133 134 133 133 133 132 132]\n",
      "Example prediction:\n",
      " 6 [135 135 136 136 136 136 137 136 136 136 135 135]\n",
      "Example prediction:\n",
      " 8 [145 145 146 146 147 147 148 147 147 146 146 145]\n",
      "Example prediction:\n",
      " 10 [148 149 150 150 151 151 151 151 151 150 149 148]\n",
      "Example prediction:\n",
      " 12 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Example prediction:\n",
      " 14 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Example prediction:\n",
      " 16 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Example prediction:\n",
      " 18 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Example prediction:\n",
      " 20 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Example prediction:\n",
      " 22 [149 150 151 151 152 152 152 152 152 151 150 149]\n",
      "Building Eagle_office_Dallas\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_51 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_52 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_53 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [121 219 121 121 109  97 109 206 219 219 219 219]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 7267.8300\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7001.0468\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6712.1920\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5876.5135\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6336.7795\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5644.9357\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5472.0959\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6067.1143\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5364.5438\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4872.1367\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5354.7265\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5251.9228\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5110.5344\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5586.4627\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5151.3304\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4858.1123\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5088.9273\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4405.9547\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4565.2454\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5563.7674\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4403.3883\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4438.3996\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4581.1963\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4611.2885\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4667.8583\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4333.5756\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4427.0385\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4231.7478\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4260.4822\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4826.3439\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4212.0035\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4232.1405\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4232.9994\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4160.2679\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4278.0519\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3938.6082\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3662.6871\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4134.6990\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4218.8227\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3821.5171\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3589.8411\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3883.5711\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4004.3784\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4477.1282\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4244.4846\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4336.4019\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4032.6919\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4314.3000\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4136.7772\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3963.3394\n",
      "mean,rmse,rmse/mean,bldg: 56.508133937526274 81.92615087453811 1.4498116495071887 Eagle_office_Dallas\n",
      "Example prediction:\n",
      " 0 [50 50 51 51 52 51 52 51 51 51 50 51]\n",
      "Example prediction:\n",
      " 2 [65 66 67 67 68 68 69 68 67 67 66 66]\n",
      "Example prediction:\n",
      " 4 [ 95  96  98  99  99 100 100  99  98  98  96  95]\n",
      "Example prediction:\n",
      " 6 [94 95 97 98 98 98 99 98 97 97 95 94]\n",
      "Example prediction:\n",
      " 8 [92 93 94 95 96 96 97 96 95 94 93 92]\n",
      "Example prediction:\n",
      " 10 [ 95  96  98  99  99  99 100  99  98  98  96  95]\n",
      "Example prediction:\n",
      " 12 [94 95 96 97 98 98 98 98 97 96 95 94]\n",
      "Example prediction:\n",
      " 14 [93 94 96 97 97 97 98 97 96 96 94 93]\n",
      "Example prediction:\n",
      " 16 [91 93 94 95 95 96 96 95 95 94 93 92]\n",
      "Example prediction:\n",
      " 18 [92 93 95 96 96 96 97 96 95 95 93 92]\n",
      "Example prediction:\n",
      " 20 [94 95 97 97 98 98 99 98 97 96 95 94]\n",
      "Example prediction:\n",
      " 22 [56 56 57 57 58 57 58 57 57 57 56 56]\n",
      "Building Eagle_education_Shante\n",
      " Count bad values before pseudofill: 15\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_54 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_55 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_56 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 181605.6111\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 161982.2042\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 154747.0597\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 157310.7798\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 173115.9926\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 170237.0481\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 183026.2543\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 181016.6955\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 189403.2998\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 172632.4807\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 175167.7963\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 170622.1293\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 165015.0130\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 170878.4271\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 193680.0409\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 176188.6636\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 157502.1332\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 177575.0101\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 168237.1594\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 159202.0809\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 159973.4685\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 160935.0645\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 162844.7625\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 151892.3135\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 170913.9467\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 158196.4926\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 161855.0827\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 145198.7299\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 161217.8553\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 149743.3677\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 160739.4840\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 156460.6528\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 164792.7870\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 173045.3087\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 175820.5479\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 159534.7282\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 162495.2878\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 168446.8148\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 173437.8313\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 149882.5310\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 163268.6374\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 141962.5313\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 156466.0001\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 150927.5946\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 141869.8201\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 136331.2664\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 153854.8031\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 163323.5214\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 160275.7278\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 176768.6585\n",
      "mean,rmse,rmse/mean,bldg: 1004.1572685492623 2013.2810058484547 2.004945907285126 Eagle_education_Shante\n",
      "Example prediction:\n",
      " 0 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 2 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 4 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 6 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 8 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 10 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 12 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 14 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 16 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 18 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 20 [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Example prediction:\n",
      " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Building Eagle_office_Chauncey\n",
      " Count bad values before pseudofill: 102\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_57 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_58 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_59 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [1756 1668 1671 1816 1690 1807 1642 1743 1880 1765 1824 1671]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 1092379.3232\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1101821.7718\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1091966.0836\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1066713.7877\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1053345.2443\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1058780.1066\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1037425.0045\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1041096.4195\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1028782.5280\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1030221.2166\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 997279.4945\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 985804.5425\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 986712.2102\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 978073.6548\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 971607.9948\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 976749.7630\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 959531.6800\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 962659.4636\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 937924.7814\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 937048.9389\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 943998.6077\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 916502.5234\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 904375.2109\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 907118.5784\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 892286.7755\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 885322.6091\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 879808.4698\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 877967.3159\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 861448.4832\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 856840.8991\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 841102.8814\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 851821.7809\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 842545.0825\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 823261.7673\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 813936.8891\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 820327.3902\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 794219.3493\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 795555.2489\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 789899.1880\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 784811.7159\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 775837.2600\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 766017.2123\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 761480.2768\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 755061.4748\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 740551.0968\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 736648.3909\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 742244.5048\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 726189.9655\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 718250.9980\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 709166.3739\n",
      "mean,rmse,rmse/mean,bldg: 1037.6880318770845 936.6821629972108 0.902662586657029 Eagle_office_Chauncey\n",
      "Example prediction:\n",
      " 0 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 2 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 4 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 6 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 8 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 10 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 12 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 14 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 16 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 18 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 20 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Example prediction:\n",
      " 22 [230 232 229 230 230 232 231 232 230 232 230 231]\n",
      "Building Eagle_office_Phyllis\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_60 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_61 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_62 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [154 154 154 155 154 154 155 155 156 154 152 154]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 9422.9323\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8411.9056\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7760.2740\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6965.8636\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6430.0663\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5853.5220\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5356.7966\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4770.0674\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4437.6450\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4005.4992\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3726.3776\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3343.5963\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3111.6415\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2885.7592\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2604.4564\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2441.2494\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2265.6418\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2166.4028\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2035.0220\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1936.5963\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1800.8190\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1774.7336\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1747.0583\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1710.7360\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1667.4837\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1640.2843\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1652.8987\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1256.6462\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1007.2024\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 888.0445\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 777.9888\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 722.8843\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 639.9328\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 574.6957\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 525.5743\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 494.7762\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 440.6374\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 422.8738\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 388.7622\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 364.3493\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 369.4878\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 350.0936\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 329.0331\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 304.7183\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 315.3659\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 299.6567\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 287.1947\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 282.9384\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 292.0427\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 265.1445\n",
      "mean,rmse,rmse/mean,bldg: 87.211947406521 38.18312736696998 0.4378199146154481 Eagle_office_Phyllis\n",
      "Example prediction:\n",
      " 0 [107 107 108 108 108 108 108 108 108 108 107 108]\n",
      "Example prediction:\n",
      " 2 [111 111 111 111 111 111 111 111 111 111 110 111]\n",
      "Example prediction:\n",
      " 4 [132 132 131 132 131 132 132 132 131 131 131 131]\n",
      "Example prediction:\n",
      " 6 [139 139 139 139 139 139 139 139 139 139 138 138]\n",
      "Example prediction:\n",
      " 8 [139 140 139 139 139 140 139 139 139 139 139 139]\n",
      "Example prediction:\n",
      " 10 [139 139 139 139 139 139 139 139 139 138 138 138]\n",
      "Example prediction:\n",
      " 12 [141 141 140 141 141 141 141 141 140 140 140 140]\n",
      "Example prediction:\n",
      " 14 [141 141 141 141 141 141 141 141 140 140 140 140]\n",
      "Example prediction:\n",
      " 16 [140 140 139 140 139 140 140 140 139 139 139 139]\n",
      "Example prediction:\n",
      " 18 [128 128 128 128 128 128 128 128 128 127 127 127]\n",
      "Example prediction:\n",
      " 20 [110 110 110 111 110 110 110 110 111 111 110 111]\n",
      "Example prediction:\n",
      " 22 [109 109 110 110 110 110 110 110 110 110 109 110]\n",
      "Building Eagle_office_Freida\n",
      " Count bad values before pseudofill: 46\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_63 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_64 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_65 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [532 297 341  30  73  59  63 103 434 512 567 149]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 23414.7735\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 22100.4455\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20811.8717\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19473.5700\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18903.5483\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17970.2700\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17212.0895\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 16625.8661\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15431.1473\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 14504.0989\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 14366.2009\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13722.0645\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 13185.1051\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12403.5438\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 12056.4197\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11797.9395\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 11276.6624\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10842.2432\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10465.0295\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 10114.7303\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9767.4930\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9709.2193\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9363.7477\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9324.2300\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 9072.6016\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8889.7186\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8736.2792\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8770.8543\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8534.1285\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8377.1348\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8673.8727\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8419.5931\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8433.0543\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8570.6706\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8207.0087\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8327.9030\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8225.8514\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8248.9572\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7763.1058\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7360.7375\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7192.1677\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7192.3425\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7025.9746\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 7126.7999\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 7007.1122\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6749.5980\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6447.6551\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6590.4917\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6455.3154\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 6523.2650\n",
      "mean,rmse,rmse/mean,bldg: 101.60158504400508 76.86814036004417 0.756564381616207 Eagle_office_Freida\n",
      "Example prediction:\n",
      " 0 [153 153 155 155 155 156 156 156 155 155 154 154]\n",
      "Example prediction:\n",
      " 2 [152 153 154 155 155 155 156 155 155 155 153 153]\n",
      "Example prediction:\n",
      " 4 [151 152 153 153 154 154 154 154 154 154 152 152]\n",
      "Example prediction:\n",
      " 6 [153 153 154 155 155 156 156 156 155 155 154 153]\n",
      "Example prediction:\n",
      " 8 [154 154 155 156 156 157 157 157 156 156 155 155]\n",
      "Example prediction:\n",
      " 10 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 12 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 14 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 16 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 18 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 20 [155 155 156 157 157 158 158 157 157 157 156 155]\n",
      "Example prediction:\n",
      " 22 [154 155 156 156 157 157 158 157 156 157 155 155]\n",
      "Building Eagle_office_Francis\n",
      " Count bad values before pseudofill: 6\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_66 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_67 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_68 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [402 420 493 439 457 402 493 548 420 457 548 402]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 90275.2905\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 86445.1647\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 83675.0001\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 81705.9225\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 80081.6331\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 76970.8531\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 74836.7603\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 72610.6979\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 70085.6359\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 67899.2275\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 66314.7304\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 64413.4850\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 61906.5705\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 60617.5435\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 57855.9492\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 56035.6833\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 55302.1751\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 53013.8270\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 51698.8994\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 48823.2030\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 47999.8392\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 45927.1162\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 44478.0419\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 43802.4928\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 41733.9415\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 40166.4042\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 38866.2248\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 37057.8014\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 36178.0882\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 34609.7969\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 33712.0446\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 32320.8314\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 30403.4681\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 29646.3522\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 28627.8360\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 26961.8860\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 26063.2098\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 25355.4000\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 24329.0903\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 23646.0982\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 22519.4587\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 22063.7408\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 20623.8699\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19787.7692\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 19043.3208\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18397.5698\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17964.1837\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17431.0284\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 16239.6735\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 16506.2411\n",
      "mean,rmse,rmse/mean,bldg: 336.47459885391214 213.02347735705658 0.6331041870104002 Eagle_office_Francis\n",
      "Example prediction:\n",
      " 0 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 2 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 4 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 6 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 8 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 10 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 12 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 14 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 16 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 18 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 20 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Example prediction:\n",
      " 22 [214 212 212 212 214 213 213 214 213 214 216 212]\n",
      "Building Eagle_office_Sheree\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_69 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_70 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_71 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [206 280  27   1  77  70  90  82 203 185 123 198]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 6115.8940\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5539.6438\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5196.0659\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4925.5150\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4583.1937\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4262.8278\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4150.0674\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3798.5419\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3756.4861\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3549.8725\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3534.9972\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3381.8099\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3382.8546\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3320.8197\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3320.4280\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3289.0706\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3193.0585\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 3151.6801\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2952.7173\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2857.2015\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2757.6549\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2755.6318\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2768.4659\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2693.1629\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2637.0815\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2597.4625\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2589.5193\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2539.0465\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2537.9013\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2494.4658\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2466.8108\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2426.1930\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2435.0652\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2446.6571\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2465.3886\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2392.8565\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2406.3762\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2377.7044\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2355.3812\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2390.9756\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2343.1357\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2402.3286\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2359.7940\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2326.9143\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2336.9261\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2312.3983\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2310.7608\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2344.4033\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2273.6018\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 2303.4647\n",
      "mean,rmse,rmse/mean,bldg: 62.01927753077944 59.13265010187097 0.9534559649219411 Eagle_office_Sheree\n",
      "Example prediction:\n",
      " 0 [78 78 79 79 79 78 77 77 76 74 74 73]\n",
      "Example prediction:\n",
      " 2 [75 74 76 75 75 74 73 73 72 71 70 70]\n",
      "Example prediction:\n",
      " 4 [73 73 74 74 74 73 73 73 72 71 70 70]\n",
      "Example prediction:\n",
      " 6 [80 80 81 81 81 81 80 81 80 78 78 77]\n",
      "Example prediction:\n",
      " 8 [ 96  97  98  99 100 100 100 100  99  98  97  96]\n",
      "Example prediction:\n",
      " 10 [100 101 102 103 104 105 105 105 104 103 102 100]\n",
      "Example prediction:\n",
      " 12 [100 101 102 103 104 105 105 105 104 103 102 100]\n",
      "Example prediction:\n",
      " 14 [100 101 102 103 104 105 105 105 104 103 102 100]\n",
      "Example prediction:\n",
      " 16 [100 101 102 103 104 105 105 105 104 103 102 100]\n",
      "Example prediction:\n",
      " 18 [100 101 102 103 104 105 105 105 104 103 102 100]\n",
      "Example prediction:\n",
      " 20 [ 97  98  99 101 102 102 102 102 101 100  99  98]\n",
      "Example prediction:\n",
      " 22 [76 76 77 77 77 76 75 75 74 73 72 72]\n",
      "Building Eagle_education_Sherrill\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_72 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_73 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_74 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [4387 4517 4480 4511 4513 3873 3913 3907 3903 3835 4208 3712]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 5626951.7491\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5559365.4873\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5603359.0164\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5499198.9691\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5594856.6855\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5524991.3982\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5516724.1436\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5449627.6600\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5332409.3400\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5419186.6000\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5406806.0818\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5297459.8455\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5344254.5618\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5365331.4182\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5370265.1309\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5317821.3964\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5262810.7055\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5294996.3236\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5317096.9291\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5246096.3636\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5112070.8418\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5197908.7582\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5129607.2982\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5163148.6291\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5114468.6564\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5102863.7782\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5119908.1773\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5099350.1691\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5061959.0400\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5009398.5564\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4975899.2636\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5005444.5382\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4907280.6836\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5002042.2509\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4953080.8782\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4986743.3109\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5018223.5382\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4967993.6473\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4936584.4800\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4905144.5673\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4801522.8691\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 4820029.6109\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4918590.4127\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4846893.0982\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4712149.7736\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4698673.9082\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4735058.6755\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4756488.9355\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4808111.8582\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4666449.5818\n",
      "mean,rmse,rmse/mean,bldg: 2032.6741548164603 2084.442469552072 1.0254680833191812 Eagle_education_Sherrill\n",
      "Example prediction:\n",
      " 0 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 2 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 4 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 6 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 8 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 10 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 12 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 14 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 16 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 18 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 20 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Example prediction:\n",
      " 22 [233 231 232 232 231 231 231 232 233 231 231 231]\n",
      "Building Eagle_education_Brooke\n",
      " Count bad values before pseudofill: 36\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_75 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_76 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_77 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [3753 3797 5422 5459 5486 3461 3671 3781 5425 5046 3650 5136]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 5305107.3509\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5311999.7545\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5200754.9109\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5206567.5491\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5229848.7218\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5251712.5673\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5248963.9964\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5127490.3709\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5143037.7600\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5105808.0709\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5062551.7200\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5073038.2255\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5129038.2891\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5105068.4800\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4987443.9727\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4960336.9655\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5024747.9655\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5003459.1127\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4963411.4491\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4947122.9782\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5011142.4582\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 4912708.4309\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5001776.5164\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5040406.1527\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4916579.3418\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4857038.7836\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4797420.6673\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4849713.6745\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4754958.7855\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4871827.8727\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4810743.3527\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 4743618.9291\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4813285.0782\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4720535.8873\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4679539.8309\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4769242.5018\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4635086.2736\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4677922.8673\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4603836.1755\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 4696902.5673\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4665732.7145\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4676274.0964\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4612993.6318\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 4627577.4127\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4621947.4382\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4505781.7782\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4634928.9564\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4555079.0345\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4547789.9236\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 4444449.9645\n",
      "mean,rmse,rmse/mean,bldg: 1638.6883219271176 1814.103707533786 1.1070462169403745 Eagle_education_Brooke\n",
      "Example prediction:\n",
      " 0 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 2 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 4 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 6 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 8 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 10 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 12 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 14 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 16 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 18 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 20 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Example prediction:\n",
      " 22 [230 232 230 230 230 229 230 231 232 232 232 231]\n",
      "Building Eagle_education_Alberto\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_78 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_79 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_80 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [1229 1231 1233 1238 1231 1232 1236 1236 1244 1234 1212 1229]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 620570.4075\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 609885.7805\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 595537.5052\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 595935.7664\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 591354.0643\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 582750.3593\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 570867.5059\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 569677.6970\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 565108.8473\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 558631.6283\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 558476.3005\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 541368.4603\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 542161.7982\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 534709.0039\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 524359.6226\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 514474.3401\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 510228.9434\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 512037.8725\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 496435.2745\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 496964.2120\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 487330.6034\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 485670.2953\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 478457.8452\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 478578.2372\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 470079.1789\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 465420.4343\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 460389.2918\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 455641.5650\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 449524.8555\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 434831.9216\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 435551.2091\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 429174.0870\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 428069.4437\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 421115.5599\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 416634.8217\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 417172.8317\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 409522.2627\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 407802.4926\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 400597.8610\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 388863.0080\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 388797.1281\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 382292.3830\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 386756.8278\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 373806.6805\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 368756.3785\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 364297.6611\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 360122.3701\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 361597.8143\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 350828.6398\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 345485.3638\n",
      "mean,rmse,rmse/mean,bldg: 694.9820078317341 527.5338138363813 0.7590611093404096 Eagle_education_Alberto\n",
      "Example prediction:\n",
      " 0 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 2 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 4 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 6 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 8 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 10 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 12 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 14 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 16 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 18 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 20 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Example prediction:\n",
      " 22 [227 228 229 228 227 230 230 228 227 230 229 228]\n",
      "Building Eagle_food_Kay\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_81 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_82 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_83 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [668 670 435 348 371 378 249 250 263 747 769 897]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 135837.6687\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 133651.3278\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 131740.8203\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 130132.5923\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 124668.0547\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 122778.6678\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 118952.5705\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 116913.7501\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 115394.2308\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 112542.5600\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 110871.8497\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 107673.1520\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 104882.8163\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 103539.6648\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 103490.0956\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 99964.1707\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 97338.2363\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 96103.0207\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 95506.3579\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 92995.9743\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 90956.6382\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 89042.6012\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 87759.9542\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 85262.8177\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 83779.7516\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 82494.3064\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 80245.4136\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 79009.3049\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 78324.6961\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 77538.2494\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74198.2930\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 72989.1717\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 71913.8023\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71856.7137\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 69861.1167\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 66688.1032\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 67198.9888\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 65007.0945\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 66391.0323\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 64242.3209\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 63644.0742\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 64439.2834\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 60483.6468\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 59545.9731\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 58688.3460\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 55225.4274\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 57047.2218\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 56374.0889\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 54849.7656\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 52705.4925\n",
      "mean,rmse,rmse/mean,bldg: 276.26271949384346 209.63567170058872 0.7588272210042458 Eagle_food_Kay\n",
      "Example prediction:\n",
      " 0 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 2 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 4 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 6 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 8 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 10 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 12 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 14 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 16 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 18 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 20 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Example prediction:\n",
      " 22 [212 212 212 210 212 212 212 213 212 214 214 212]\n",
      "Building Eagle_health_Jodi\n",
      " Count bad values before pseudofill: 33\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_84 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_85 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_86 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [365 365 365 365 365 365 365 365 365 365 364 331]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 36769.2383\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 33762.8884\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 32551.9673\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 31326.4710\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 30332.8581\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 28347.6351\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 27368.8845\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 26079.9716\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 25041.3559\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 23805.5569\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22673.2307\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 21648.4994\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20596.0519\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19668.2204\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18799.0844\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 18504.6874\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17112.8607\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 16284.1587\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 15532.0979\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 14792.1667\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14211.6057\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13624.2395\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13022.9648\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 12395.5278\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 12172.4989\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11510.9887\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 11134.1551\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 10222.0372\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 10046.5351\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9888.3132\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9528.2439\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9158.7506\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8884.0691\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 8558.3490\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8274.8909\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8124.6410\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8007.4932\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7669.7187\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7569.0630\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7385.0590\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 7461.8605\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 7433.9350\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7373.6384\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7272.7986\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7223.1048\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7154.6635\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6592.0091\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6062.3386\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5870.3199\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5717.2806\n",
      "mean,rmse,rmse/mean,bldg: 192.41196647250106 85.59540114325205 0.4448548742184654 Eagle_health_Jodi\n",
      "Example prediction:\n",
      " 0 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 2 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 4 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 6 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 8 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 10 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 12 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 14 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 16 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 18 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 20 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Example prediction:\n",
      " 22 [180 180 181 181 181 181 181 181 181 181 181 180]\n",
      "Building Eagle_education_Norah\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_87 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_88 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_89 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [1954 1912 2288 2048 2346 2408 2366 2362 2419 2179 1802 2123]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 624857.5200\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 612076.1202\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 629824.5761\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 613817.9502\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 621003.3016\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 607222.3066\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 586540.9030\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 603543.9925\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 588179.0702\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 587575.8077\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 556804.7611\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 564948.2795\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 559135.7305\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 559710.0770\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 557503.6834\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 553417.6148\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 535295.7215\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 543247.3155\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 524383.6325\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 539619.1694\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 528051.2524\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 507066.0398\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 505041.9776\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 497586.7135\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 493500.9838\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 496005.0778\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 492208.5347\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 475023.6462\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 470232.8775\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 466434.2419\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 471909.3705\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 461101.2090\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 459208.1650\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 444726.3215\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 445855.0636\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 436854.0950\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 452400.4028\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 436192.6674\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 438021.3566\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 420856.5816\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 414146.6493\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 426381.1249\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 418305.4343\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 415153.4347\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 408062.5155\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 394846.5201\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 396432.5255\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 385629.9727\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 381720.0302\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 378740.8499\n",
      "mean,rmse,rmse/mean,bldg: 712.0697702804412 671.6555025108355 0.9432439496010497 Eagle_education_Norah\n",
      "Example prediction:\n",
      " 0 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 2 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 4 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 6 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 8 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 10 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 12 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 14 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 16 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 18 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 20 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Example prediction:\n",
      " 22 [226 226 226 227 227 227 228 226 226 228 229 226]\n",
      "Building Eagle_education_Will\n",
      " Count bad values before pseudofill: 5\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_90 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_91 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_92 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [482 396 450 326 374 382 330 399 352 336 403 381]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 78207.5506\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 75090.8012\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 73491.2291\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 71296.1264\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 68387.0697\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 65932.4400\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 64142.4228\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 62865.1635\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 59526.2791\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 58941.9737\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 56547.6482\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 54965.4664\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 53287.4036\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 51292.2550\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 48786.2314\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 47130.6204\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 45583.8726\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 43977.3082\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 43208.6874\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 40173.2546\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 39461.8960\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 37678.0735\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 36263.6825\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 35810.4601\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 34443.1265\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 32526.5388\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 31802.4688\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 30423.0392\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 28124.9406\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 27615.9254\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 27054.2021\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 24970.6978\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 24916.0736\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23248.1030\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22177.5555\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22214.3679\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21231.5303\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20062.3217\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19615.5363\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18724.4280\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17921.9639\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17410.3516\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 15789.9205\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 16135.4172\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 15400.8900\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 14687.5396\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14387.2269\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14282.4137\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 13916.4339\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12896.8921\n",
      "mean,rmse,rmse/mean,bldg: 226.3193540623744 55.33234057307935 0.24448788660747753 Eagle_education_Will\n",
      "Example prediction:\n",
      " 0 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 2 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 4 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 6 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 8 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 10 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 12 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 14 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 16 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 18 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 20 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Example prediction:\n",
      " 22 [210 210 209 210 211 210 210 209 210 211 208 211]\n",
      "Building Eagle_lodging_Blake\n",
      " Count bad values before pseudofill: 2\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_93 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_94 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_95 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [  8   8   8   8   8   8   8   8 871  10  10   0]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 76487.8042\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 72443.3701\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 75833.1975\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 75353.7218\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 74868.5987\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74782.3539\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74515.2595\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70330.4520\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 75859.1079\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74348.9574\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 71716.2295\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 74226.9052\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71846.2945\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 72108.9187\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70602.7641\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 73659.2399\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74596.4614\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71409.6339\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 72791.0224\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 73054.7241\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 74285.0511\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 72214.0608\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 72095.5312\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74773.6541\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71299.8884\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 72608.9457\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 68791.3324\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 72585.7726\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71063.8452\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70917.9151\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71871.0923\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 72691.2951\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74186.3597\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 72434.9707\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70859.9633\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70757.2160\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 69836.8354\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70513.6593\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71298.5375\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 68668.1739\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71498.2114\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 72084.9563\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70504.4491\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 68991.8400\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 69214.6077\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71881.8116\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70284.4818\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 70359.2568\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71670.7167\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 69849.8115\n",
      "mean,rmse,rmse/mean,bldg: 43.440915032495504 56.73079509167212 1.3059300212538172 Eagle_lodging_Blake\n",
      "Example prediction:\n",
      " 0 [91 92 93 93 94 93 94 93 93 93 93 92]\n",
      "Example prediction:\n",
      " 2 [91 92 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 4 [80 81 81 81 81 81 81 81 81 81 81 80]\n",
      "Example prediction:\n",
      " 6 [78 79 79 80 79 80 79 79 80 79 79 79]\n",
      "Example prediction:\n",
      " 8 [91 92 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 10 [91 93 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 12 [90 91 91 92 92 92 92 92 92 91 91 91]\n",
      "Example prediction:\n",
      " 14 [90 91 92 93 93 93 93 93 93 92 92 91]\n",
      "Example prediction:\n",
      " 16 [91 92 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 18 [91 92 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 20 [91 93 93 94 94 94 94 94 94 93 93 92]\n",
      "Example prediction:\n",
      " 22 [91 93 93 94 94 94 94 94 94 93 93 92]\n",
      "Building Eagle_education_Petra\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_96 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_97 (GRU)                 (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_98 (GRU)                 (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [168 186 187 178 181 179 191 199 188 192 196 182]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 4525.0033\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3976.0295\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 3425.7580\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3151.3990\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2861.6582\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2563.9689\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 2379.8117\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2188.0625\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1987.4098\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1940.5878\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1769.2712\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 1728.5233\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1658.1607\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1668.5840\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 1575.3042\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1541.4277\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 1545.6952\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1513.4481\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1278.5391\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1112.6328\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 988.2248\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 927.4366\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 842.8544\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 782.0552\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 725.3934\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 682.4421\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 648.1154\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 590.6494\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 578.7369\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 569.1387\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 522.7693\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 519.7056\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 496.4319\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 477.4451\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 472.9760\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 454.1109\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 457.7940\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 465.5442\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 418.4297\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 407.3709\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 409.6487\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 400.0700\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 400.7386\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 385.4151\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 378.1140\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 380.7381\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 364.0468\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 356.8196\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 360.6327\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 345.6911\n",
      "mean,rmse,rmse/mean,bldg: 57.05018764819895 36.13601456621329 0.6334074620235555 Eagle_education_Petra\n",
      "Example prediction:\n",
      " 0 [83 84 84 84 84 84 85 84 84 84 84 84]\n",
      "Example prediction:\n",
      " 2 [85 86 86 86 86 86 87 86 86 86 86 86]\n",
      "Example prediction:\n",
      " 4 [86 87 87 87 87 87 88 87 87 87 87 87]\n",
      "Example prediction:\n",
      " 6 [84 84 84 85 85 85 85 85 85 85 84 84]\n",
      "Example prediction:\n",
      " 8 [84 84 84 84 85 84 85 84 84 84 84 84]\n",
      "Example prediction:\n",
      " 10 [87 88 88 88 88 88 88 88 88 88 88 88]\n",
      "Example prediction:\n",
      " 12 [90 90 90 90 90 90 90 90 90 90 90 90]\n",
      "Example prediction:\n",
      " 14 [84 85 84 85 85 85 85 85 84 85 84 84]\n",
      "Example prediction:\n",
      " 16 [80 80 80 80 81 80 80 80 80 80 80 80]\n",
      "Example prediction:\n",
      " 18 [79 79 79 79 80 79 80 79 79 80 79 79]\n",
      "Example prediction:\n",
      " 20 [81 81 81 81 82 81 82 81 81 82 81 81]\n",
      "Example prediction:\n",
      " 22 [85 85 85 85 86 86 86 86 86 86 85 85]\n",
      "Building Eagle_lodging_Trina\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_99 (GRU)                 (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_100 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_101 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [299 331 324 338 315 327 317 330 359 407 378 310]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 5ms/step - loss: 15030.2365\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14114.8728\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13622.3830\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13383.8718\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12417.3026\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11548.0548\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11247.2161\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10707.4400\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10614.6531\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10075.3314\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9865.2823\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9397.1472\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8883.1364\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8481.6611\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8868.6561\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8453.7775\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8389.4159\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8246.7946\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7859.7142\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 7924.2174\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7799.6592\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8050.9622\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7931.9026\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7771.3485\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 7337.5869\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7148.6860\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6746.0381\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6835.8685\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6573.7923\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6526.4370\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6550.5176\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6113.6832\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6428.3498\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6057.5968\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6001.6729\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5897.9817\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5781.9450\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5788.9572\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 5686.7942\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5601.7614\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 5546.1382\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5629.3020\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5366.3492\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5175.7218\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5314.3436\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5250.4285\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5202.1612\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4999.4887\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5067.7177\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4925.0039\n",
      "mean,rmse,rmse/mean,bldg: 91.28035335157311 77.87361782886082 0.8531257271641439 Eagle_lodging_Trina\n",
      "Example prediction:\n",
      " 0 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 2 [156 158 158 159 159 159 159 158 158 157 157 157]\n",
      "Example prediction:\n",
      " 4 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 6 [157 158 158 159 160 160 159 158 159 158 158 157]\n",
      "Example prediction:\n",
      " 8 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 10 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 12 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 14 [157 158 158 159 160 160 159 159 159 158 158 157]\n",
      "Example prediction:\n",
      " 16 [157 158 158 159 160 160 159 158 158 158 158 157]\n",
      "Example prediction:\n",
      " 18 [155 157 157 157 158 158 157 157 157 156 156 155]\n",
      "Example prediction:\n",
      " 20 [137 138 139 139 139 139 138 139 138 137 137 137]\n",
      "Example prediction:\n",
      " 22 [152 154 154 154 155 155 154 154 154 153 153 152]\n",
      "Building Eagle_health_Reuben\n",
      "Building Eagle_education_Teresa\n",
      " Count bad values before pseudofill: 27\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_102 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_103 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_104 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [331 332  97  79 262 226 213 413 431 454 456 480]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 31483.3930\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 30025.0703\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 28841.4942\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 27365.4682\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 25795.0252\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 24339.6234\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23709.7250\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23188.6631\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21878.4753\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 20772.3858\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19905.7989\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 18915.8131\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18378.1577\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 17478.8963\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 16273.8503\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 15647.4050\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 15195.5324\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14923.5903\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14589.4707\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13595.6403\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13360.9252\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12819.4019\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12310.8032\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11574.6323\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11830.5519\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11099.3382\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10964.3159\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10335.2954\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10407.2609\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9733.6537\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9478.8184\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9488.7401\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9376.0930\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9207.5018\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9369.6873\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9365.6612\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8996.1855\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9211.0589\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8919.0480\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8796.7842\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8462.3330\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8753.8419\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9036.0465\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 1s 5ms/step - loss: 8887.9881\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7631.0863\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7574.6267\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7010.5973\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6871.2685\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6467.4088\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6003.9374\n",
      "mean,rmse,rmse/mean,bldg: 148.73672326311552 59.92303183278948 0.40287987067447717 Eagle_education_Teresa\n",
      "Example prediction:\n",
      " 0 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 2 [176 175 175 175 175 175 175 176 175 175 176 175]\n",
      "Example prediction:\n",
      " 4 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 6 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 8 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 10 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 12 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 14 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 16 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 18 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 20 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Example prediction:\n",
      " 22 [176 175 175 175 175 175 175 176 176 175 176 175]\n",
      "Building Eagle_office_Norbert\n",
      " Count bad values before pseudofill: 40\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_105 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_106 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_107 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [655 684 673 712 692 707 689 690 706 701 722 666]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 153459.4159\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 149338.2314\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 144423.5129\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 140540.2618\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 139165.0930\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 135534.3397\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 133341.6808\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 129325.3100\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 124449.7530\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 123763.3323\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 120295.4429\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 118021.0045\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 115705.4678\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 112851.4820\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 110056.1902\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 107112.5783\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 104959.4253\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 102592.2214\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 99579.0443\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 96978.8749\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 94916.9116\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 92756.0947\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 89467.2674\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 86981.6031\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 85646.1504\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 82025.2442\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 80685.1772\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 77948.1879\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 77094.0219\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 75898.7207\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 74003.8196\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 71013.1385\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 69916.9839\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 67187.5486\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 66159.9115\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 62931.7481\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 61798.2643\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 61651.2971\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 59467.9880\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 56011.8635\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 56267.8000\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 53964.1615\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 52619.2567\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 51787.4276\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 50201.0678\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 49006.9464\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 47503.1356\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 46402.0661\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 45337.3939\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 43235.8806\n",
      "mean,rmse,rmse/mean,bldg: 390.77642429158755 235.53056419327888 0.6027246004419445 Eagle_office_Norbert\n",
      "Example prediction:\n",
      " 0 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 2 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 4 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 6 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 8 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 10 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 12 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 14 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 16 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 18 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 20 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Example prediction:\n",
      " 22 [221 220 219 220 220 220 220 222 219 219 220 218]\n",
      "Building Eagle_lodging_Casey\n",
      " Count bad values before pseudofill: 485\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_108 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_109 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_110 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [132 137 231 119  97  91 115 131 179  93 173 145]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 2181.7137\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2008.7392\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1845.1240\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1810.1981\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1780.1560\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1729.9413\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1727.9538\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1649.5316\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1648.2093\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1650.7347\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1542.4511\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1572.3243\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1473.5301\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1400.0744\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1490.9924\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1389.7312\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1362.7066\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1417.0788\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1426.9710\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1366.5580\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 5ms/step - loss: 1335.7348\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1332.5272\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1295.6086\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1310.3899\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1264.4445\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1269.4525\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1266.4419\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1235.0205\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1217.9551\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1205.5335\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1199.4542\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1248.7854\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1190.0993\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1209.1552\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1213.0487\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1185.3903\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1166.4959\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1164.2456\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1197.7295\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1152.5047\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1168.7391\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1167.6209\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1076.8546\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1133.1618\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1120.0414\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1093.6231\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1090.0031\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1077.7657\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1163.3199\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1081.2097\n",
      "mean,rmse,rmse/mean,bldg: 22.565705504426337 50.85644707987739 2.2537051664483405 Eagle_lodging_Casey\n",
      "Example prediction:\n",
      " 0 [49 51 50 51 51 51 51 51 51 50 49 49]\n",
      "Example prediction:\n",
      " 2 [49 51 51 52 52 52 52 52 52 51 50 50]\n",
      "Example prediction:\n",
      " 4 [35 36 36 37 36 37 37 37 37 36 35 36]\n",
      "Example prediction:\n",
      " 6 [40 42 42 43 42 43 42 43 43 42 41 41]\n",
      "Example prediction:\n",
      " 8 [43 44 45 45 45 45 45 45 45 44 43 43]\n",
      "Example prediction:\n",
      " 10 [51 53 53 54 53 54 53 53 54 52 51 51]\n",
      "Example prediction:\n",
      " 12 [60 61 63 63 63 64 63 62 63 61 60 58]\n",
      "Example prediction:\n",
      " 14 [54 56 57 57 57 57 56 56 57 55 54 53]\n",
      "Example prediction:\n",
      " 16 [45 47 47 48 47 47 47 46 47 45 45 44]\n",
      "Example prediction:\n",
      " 18 [29 29 29 29 29 29 28 28 29 28 28 28]\n",
      "Example prediction:\n",
      " 20 [21 21 21 21 21 21 21 21 21 21 21 21]\n",
      "Example prediction:\n",
      " 22 [21 21 21 21 21 21 21 21 22 22 21 22]\n",
      "Building Eagle_office_Tia\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_111 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_112 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_113 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [484 351 282 210 133 156 171 585 460 434 414 316]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 54154.4625\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 51434.9988\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 49817.1544\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 47061.3304\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 46318.6267\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 45128.2551\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 43132.9523\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 41951.0689\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 41594.0260\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 39707.7138\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 38623.4218\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 37003.8163\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 35838.0102\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 35174.9844\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 34152.3866\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 32600.9640\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 31714.2439\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 30301.1853\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 29875.9128\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 29946.1245\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 28221.8507\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 27699.4230\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 26279.8561\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 25939.2009\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 25009.8945\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 24178.3141\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23452.4138\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23030.5405\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22218.6679\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22244.1114\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21390.7119\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21111.0306\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20208.7887\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19765.5817\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19600.0241\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19881.7685\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18676.8369\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19473.4900\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19142.1349\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18778.1500\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18286.6132\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17786.0809\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17982.4722\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17359.5893\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17561.5540\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17955.5833\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17154.8398\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17442.0543\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17240.4062\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17162.1790\n",
      "mean,rmse,rmse/mean,bldg: 174.62244338235027 134.97525062649356 0.7729547703725236 Eagle_office_Tia\n",
      "Example prediction:\n",
      " 0 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 2 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 4 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 6 [184 183 184 184 184 184 184 184 184 184 183 183]\n",
      "Example prediction:\n",
      " 8 [184 184 184 184 184 184 184 184 184 184 183 183]\n",
      "Example prediction:\n",
      " 10 [184 184 184 184 184 184 184 184 184 184 183 183]\n",
      "Example prediction:\n",
      " 12 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 14 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 16 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 18 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 20 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Example prediction:\n",
      " 22 [184 184 184 184 184 184 184 184 184 184 183 184]\n",
      "Building Eagle_office_Remedios\n",
      " Count bad values before pseudofill: 8\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_114 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_115 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_116 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [199 244 233 236 246 233 254 233 216 237 243 227]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 14484.3229\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13062.0527\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12198.9932\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11235.6748\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10514.1506\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9684.2040\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 9068.9944\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8427.1152\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7859.6537\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7132.1121\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6742.3342\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6207.0871\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5858.7501\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5385.2464\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5044.9112\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4639.0041\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4370.2529\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4129.4162\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3872.8354\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3657.7399\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3471.4229\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3251.9153\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3128.5487\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3035.1514\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2956.8966\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2889.3570\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2793.7215\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2707.9395\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2719.4609\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2622.9904\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2672.6192\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2596.6050\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2531.9038\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1849.9105\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1679.0800\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1491.9898\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1422.2483\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1309.9612\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1260.0019\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1153.2751\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1067.5314\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1079.2077\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 993.9227\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 934.3581\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 858.0545\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 842.4227\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 789.7983\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 849.6353\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 775.0066\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 707.8000\n",
      "mean,rmse,rmse/mean,bldg: 121.45534449703446 66.19326084221588 0.5450008076329 Eagle_office_Remedios\n",
      "Example prediction:\n",
      " 0 [134 134 134 134 134 134 134 134 134 134 134 134]\n",
      "Example prediction:\n",
      " 2 [138 138 138 138 138 138 138 138 138 138 138 138]\n",
      "Example prediction:\n",
      " 4 [136 136 136 136 136 136 136 136 136 136 136 136]\n",
      "Example prediction:\n",
      " 6 [132 132 132 132 132 132 132 132 132 132 132 132]\n",
      "Example prediction:\n",
      " 8 [135 135 135 135 135 135 135 134 134 135 135 134]\n",
      "Example prediction:\n",
      " 10 [140 140 140 140 141 141 141 141 140 141 140 140]\n",
      "Example prediction:\n",
      " 12 [147 147 147 147 147 147 147 147 147 147 147 147]\n",
      "Example prediction:\n",
      " 14 [150 150 150 150 150 150 150 150 150 150 150 150]\n",
      "Example prediction:\n",
      " 16 [147 147 147 147 147 147 147 147 147 147 147 147]\n",
      "Example prediction:\n",
      " 18 [147 148 147 147 148 148 148 148 147 148 148 147]\n",
      "Example prediction:\n",
      " 20 [148 148 148 148 148 148 148 148 148 148 148 148]\n",
      "Example prediction:\n",
      " 22 [154 154 154 153 154 154 154 154 154 154 154 154]\n",
      "Building Eagle_office_Patrice\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_117 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_118 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_119 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [453 435 295 371 293 213 213 210 213 240 432 704]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 34432.0354\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 32472.3373\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 32379.5752\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 30597.7213\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 30156.9627\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 29381.8335\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 28100.3138\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 26141.7182\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 27180.0970\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 26054.6886\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 25733.6595\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23865.4756\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 23664.2758\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22957.5207\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22237.5551\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 22864.5554\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21404.1441\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 21515.1498\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 21174.6577\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20897.6587\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20068.0116\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19938.0467\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 20563.9461\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18824.2556\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 20005.2503\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19440.4333\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 19420.6479\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18453.8235\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18269.9752\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17930.9085\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18142.0139\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18507.0474\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18401.3576\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17571.9373\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18343.4834\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 17445.2757\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 18122.8150\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 15343.3774\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 14375.4985\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 13516.2814\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12805.7850\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12510.4194\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12217.4592\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11850.8989\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11240.2291\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11431.2149\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10682.7331\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10967.3620\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 10928.0329\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10902.9086\n",
      "mean,rmse,rmse/mean,bldg: 165.8827588691298 163.15464306437906 0.9835539520601835 Eagle_office_Patrice\n",
      "Example prediction:\n",
      " 0 [34 35 34 36 35 35 35 35 35 34 35 36]\n",
      "Example prediction:\n",
      " 2 [102 102 102 102 103 103 102 103 102 102 102 102]\n",
      "Example prediction:\n",
      " 4 [140 140 140 140 140 140 141 140 140 140 139 140]\n",
      "Example prediction:\n",
      " 6 [140 140 141 141 141 141 141 141 141 141 140 141]\n",
      "Example prediction:\n",
      " 8 [140 140 141 141 141 141 141 141 141 141 140 141]\n",
      "Example prediction:\n",
      " 10 [140 140 141 141 141 141 141 141 141 141 140 141]\n",
      "Example prediction:\n",
      " 12 [139 139 140 140 140 140 140 140 140 140 139 140]\n",
      "Example prediction:\n",
      " 14 [140 140 141 140 141 141 141 141 141 141 140 141]\n",
      "Example prediction:\n",
      " 16 [140 140 141 140 141 141 141 140 141 141 140 141]\n",
      "Example prediction:\n",
      " 18 [139 139 140 140 140 140 140 140 140 140 139 140]\n",
      "Example prediction:\n",
      " 20 [128 128 128 128 129 129 129 129 129 128 127 128]\n",
      "Example prediction:\n",
      " 22 [106 106 106 107 106 106 106 107 107 106 105 106]\n",
      "Building Eagle_education_Shana\n",
      " Count bad values before pseudofill: 0\n",
      " Count bad values after pseudofill: 0\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_120 (GRU)                (None, 12, 16)            1344      \n",
      "_________________________________________________________________\n",
      "gru_121 (GRU)                (None, 12, 16)            1632      \n",
      "_________________________________________________________________\n",
      "gru_122 (GRU)                (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 12)                204       \n",
      "=================================================================\n",
      "Total params: 4,812\n",
      "Trainable params: 4,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Example y train:\n",
      " [182 182 183 183 182 182 183 183 184 183 180 182]\n",
      "Epoch 1/50\n",
      "274/274 [==============================] - 5s 6ms/step - loss: 13366.6332\n",
      "Epoch 2/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 12040.4591\n",
      "Epoch 3/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 11146.6781\n",
      "Epoch 4/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 10481.4353\n",
      "Epoch 5/50\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 9627.1693\n",
      "Epoch 6/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8940.2071\n",
      "Epoch 7/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 8287.0097\n",
      "Epoch 8/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 7552.6087\n",
      "Epoch 9/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6979.9111\n",
      "Epoch 10/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6548.8280\n",
      "Epoch 11/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 6076.3112\n",
      "Epoch 12/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5540.5668\n",
      "Epoch 13/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 5161.9533\n",
      "Epoch 14/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4814.1105\n",
      "Epoch 15/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4480.3362\n",
      "Epoch 16/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 4167.8556\n",
      "Epoch 17/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3763.6077\n",
      "Epoch 18/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3551.5641\n",
      "Epoch 19/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3416.6916\n",
      "Epoch 20/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3170.2003\n",
      "Epoch 21/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 3010.6353\n",
      "Epoch 22/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2825.7034\n",
      "Epoch 23/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2697.3257\n",
      "Epoch 24/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2525.0292\n",
      "Epoch 25/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2490.9009\n",
      "Epoch 26/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2432.6552\n",
      "Epoch 27/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2408.1040\n",
      "Epoch 28/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2367.5628\n",
      "Epoch 29/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2271.6653\n",
      "Epoch 30/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2369.0586\n",
      "Epoch 31/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2238.5698\n",
      "Epoch 32/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2273.5667\n",
      "Epoch 33/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 2260.4885\n",
      "Epoch 34/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1674.7661\n",
      "Epoch 35/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1334.9006\n",
      "Epoch 36/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1219.5690\n",
      "Epoch 37/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 1050.1851\n",
      "Epoch 38/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 940.2367\n",
      "Epoch 39/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 858.3309\n",
      "Epoch 40/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 771.8931\n",
      "Epoch 41/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 719.5295\n",
      "Epoch 42/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 668.7683\n",
      "Epoch 43/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 627.5323\n",
      "Epoch 44/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 596.2205\n",
      "Epoch 45/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 545.2184\n",
      "Epoch 46/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 540.5341\n",
      "Epoch 47/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 493.5805\n",
      "Epoch 48/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 520.7860\n",
      "Epoch 49/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 465.7655\n",
      "Epoch 50/50\n",
      "274/274 [==============================] - 2s 6ms/step - loss: 474.7772\n",
      "mean,rmse,rmse/mean,bldg: 103.1846223894125 44.74990639964346 0.4336877469082557 Eagle_education_Shana\n",
      "Example prediction:\n",
      " 0 [148 148 148 148 148 148 148 148 148 148 148 148]\n",
      "Example prediction:\n",
      " 2 [149 150 150 150 150 150 150 150 150 150 150 149]\n",
      "Example prediction:\n",
      " 4 [155 155 156 156 156 155 156 155 156 155 156 155]\n",
      "Example prediction:\n",
      " 6 [155 156 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 8 [155 156 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 10 [155 156 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 12 [155 156 156 156 156 156 156 156 156 156 156 155]\n",
      "Example prediction:\n",
      " 14 [155 156 156 156 156 156 156 156 156 156 156 155]\n",
      "Example prediction:\n",
      " 16 [155 156 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 18 [155 155 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 20 [155 155 156 156 156 156 156 156 156 155 156 155]\n",
      "Example prediction:\n",
      " 22 [154 155 155 155 155 155 155 155 155 155 155 154]\n",
      "\n",
      "History 24 Future 12\n",
      "Column 1: Mean usage.\n",
      "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
      "Column 3: RMSE/mean normalized to help understand RMSE.\n",
      "Column 4: Building.\n",
      "      0.00       0.00   inf   Eagle_office_Henriette\n",
      "      0.11       0.04  0.36   Eagle_education_Wesley\n",
      "     15.76      53.10  3.37   Eagle_education_Jewell\n",
      "     22.57      50.86  2.25   Eagle_lodging_Casey\n",
      "     35.89      16.28  0.45   Eagle_office_Mandi\n",
      "     36.93      20.79  0.56   Eagle_office_Lamont\n",
      "     43.44      56.73  1.31   Eagle_lodging_Blake\n",
      "     46.46      32.38  0.70   Eagle_education_Eileen\n",
      "     56.51      81.93  1.45   Eagle_office_Dallas\n",
      "     57.05      36.14  0.63   Eagle_education_Petra\n",
      "     62.02      59.13  0.95   Eagle_office_Sheree\n",
      "     81.97      70.22  0.86   Eagle_lodging_Edgardo\n",
      "     87.21      38.18  0.44   Eagle_office_Phyllis\n",
      "     91.28      77.87  0.85   Eagle_lodging_Trina\n",
      "     92.83      62.98  0.68   Eagle_lodging_Dawn\n",
      "    101.60      76.87  0.76   Eagle_office_Freida\n",
      "    103.18      44.75  0.43   Eagle_education_Shana\n",
      "    121.46      66.19  0.55   Eagle_office_Remedios\n",
      "    122.35      56.36  0.46   Eagle_health_Vincenza\n",
      "    148.74      59.92  0.40   Eagle_education_Teresa\n",
      "    165.88     163.15  0.98   Eagle_office_Patrice\n",
      "    174.62     134.98  0.77   Eagle_office_Tia\n",
      "    182.08      65.44  0.36   Eagle_public_Alvin\n",
      "    192.41      85.60  0.44   Eagle_health_Jodi\n",
      "    216.03     259.08  1.20   Eagle_assembly_Herbert\n",
      "    226.32      55.33  0.24   Eagle_education_Will\n",
      "    272.96     138.89  0.51   Eagle_office_Nereida\n",
      "    276.26     209.64  0.76   Eagle_food_Kay\n",
      "    336.47     213.02  0.63   Eagle_office_Francis\n",
      "    390.78     235.53  0.60   Eagle_office_Norbert\n",
      "    477.70     325.22  0.68   Eagle_health_Athena\n",
      "    659.66     780.73  1.18   Eagle_health_Gregoria\n",
      "    694.98     527.53  0.76   Eagle_education_Alberto\n",
      "    712.07     671.66  0.94   Eagle_education_Norah\n",
      "   1004.16    2013.28  2.00   Eagle_education_Shante\n",
      "   1037.69     936.68  0.90   Eagle_office_Chauncey\n",
      "   1084.29     927.14  0.86   Eagle_health_Reba\n",
      "   1199.41    1084.35  0.90   Eagle_education_Roman\n",
      "   1638.69    1814.10  1.11   Eagle_education_Brooke\n",
      "   2032.67    2084.44  1.03   Eagle_education_Sherrill\n",
      "   3153.28    3042.40  0.96   Eagle_education_Peter\n"
     ]
    }
   ],
   "source": [
    "cors = []\n",
    "one_site_weather = load_weather_for_site(SITE)\n",
    "for BLDG in SITE_BUILDINGS:\n",
    "    print(\"Building\",BLDG)\n",
    "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
    "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
    "    MAX_BAD = 500\n",
    "    if count_bad<=MAX_BAD:\n",
    "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
    "        print(\" Count bad values before pseudofill:\",count_bad)\n",
    "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
    "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
    "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
    "        print(\" Count bad values after pseudofill:\",count_bad)\n",
    "        # \n",
    "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
    "        split = len(X)//2   # year 1 vs year 2\n",
    "        X_train = np.asarray(X[0:split])\n",
    "        y_train = np.asarray(y[0:split])\n",
    "        X_test = np.asarray(X[split:])\n",
    "        y_test = np.asarray(y[split:])\n",
    "        model = make_RNN()\n",
    "        print(model.summary())\n",
    "        #print(\"Example X train:\\n\",X_train[example].astype(int))\n",
    "        example=411\n",
    "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
    "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
    "        # Keep a table for reporting later.\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
    "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
    "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
    "        print(\"mean,rmse,rmse/mean,bldg:\",mean,rmse,rmse/mean,BLDG)\n",
    "        for hr in range(0,24,2):\n",
    "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n",
    "print()\n",
    "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
    "print(\"Column 1: Mean usage.\")\n",
    "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
    "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
    "print(\"Column 4: Building.\")\n",
    "for cor in sorted(cors):\n",
    "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bm8eEJdHbz9v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uY4snIvJbz9z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_227.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
