{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CR_234.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# CNN + RNN \n",
        "Compare to RNN 229.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "0f802efd-96d2-4bb6-8156-698f71e69eb0"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Eagle'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "# For our CNN, min predictive features is six.\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_HISTORY = 24\n",
        "STEPS_FORWARD = 12 \n",
        "STEPS_FUTURE =  12 \n",
        "METER_FILE='steam.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3\n",
        "SCALING=1\n",
        "CELLS = 16\n",
        "FILTERS = 16\n",
        "WIDTH = 3\n",
        "STRIDE = (1,1)\n",
        "INPUT_SHAPE = (STEPS_FORWARD,NUM_PREDICTORS,1) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 12 ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "03b58d0e-4d5b-46c0-9971-da3207eeb3ca"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvXGAH2wzBWS"
      },
      "source": [
        "def scale(df):\n",
        "    scaler=StandardScaler()\n",
        "    #scaler=MinMaxScaler()\n",
        "    scaled=scaler.fit_transform(df.values)\n",
        "    scaled = pd.DataFrame(scaled,index=df.index,columns=df.columns)\n",
        "    return scaled"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "43a01ac6-ffe2-41d0-e0b9-50f897423ec3"
      },
      "source": [
        "DATE_PARSE=True  # must be true if we use one of these as predictor\n",
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    if DATE_PARSE:\n",
        "        site_df.insert(0,'hour',0)\n",
        "        site_df.insert(1,'month',0)\n",
        "        site_df.insert(2,'doy',0)\n",
        "        L=len(site_df)\n",
        "        for i in range(0,L):\n",
        "            dt=site_df.index[i]\n",
        "            hour=dt.hour\n",
        "            month=dt.month\n",
        "            doy=dt.dayofyear\n",
        "            site_df.iat[i,0] = hour\n",
        "            site_df.iat[i,1] = month\n",
        "            site_df.iat[i,2] = doy\n",
        "    #if SCALING==1:\n",
        "    #    site_df = scale(site_df) # could break if any column is empty\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 18:00:00</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1026.2</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.2</td>\n",
              "      <td>310.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>330.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>320.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 18:00:00    18     12  365  ...          1026.2          330.0        2.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1027.0          320.0        1.5\n",
              "2017-12-31 21:00:00    21     12  365  ...          1027.2          310.0        2.6\n",
              "2017-12-31 22:00:00    22     12  365  ...          1027.4          330.0        3.1\n",
              "2017-12-31 23:00:00    23     12  365  ...          1027.4          320.0        4.6\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "046f930b-2302-4c11-c85f-accf0d7c47f2"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>92.2957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>277.5584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>280.5331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>289.3302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>164.3474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        meter\n",
              "timestamp                    \n",
              "2017-12-31 19:00:00   92.2957\n",
              "2017-12-31 20:00:00  277.5584\n",
              "2017-12-31 21:00:00  280.5331\n",
              "2017-12-31 22:00:00  289.3302\n",
              "2017-12-31 23:00:00  164.3474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "1a565930-78a8-4295-bbc3-c1b4dac7ef47"
      },
      "source": [
        "# Make X out of weather + meter features, then select wanted columns.\n",
        "# Make y out of meter features, then select meter column only.\n",
        "# Apply scaler to X only.\n",
        "# Pull out time steps for X from the past: STEPS_HISTORY.\n",
        "# Pull out proper number of time steps for X: STEPS_FORWARD.\n",
        "# Pull out proper number of time steps for y: STEPS_FUTURE.\n",
        "# Make X inefficiently with 3-deep nested for loop because\n",
        "# a) want to replace NaN with previous value\n",
        "# b) need to add the RGB dimension for CNN.\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    if SCALING==1:\n",
        "        df = scale(df) # could break if any column is empty\n",
        "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = mdf[PREDICTED_VARIABLE].values  # meter\n",
        "    #\n",
        "    X_shape = (num_samples,STEPS_FUTURE,NUM_PREDICTORS,1) # RGB = 1\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        one_sample = predictor_series[sam:sam+STEPS_FORWARD]\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = one_sample[time]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat,0] = val  # RGB dim = 0\n",
        "        for time in range (0,STEPS_FUTURE):  \n",
        "            y[sam,time]=predicted_series[sam+STEPS_HISTORY+time]\n",
        "    return X,y \n",
        "print(one_bldg_meter.head())\n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       meter\n",
            "timestamp                   \n",
            "2016-01-01 00:00:00  31.7661\n",
            "2016-01-01 01:00:00  27.4004\n",
            "2016-01-01 02:00:00  38.4989\n",
            "2016-01-01 03:00:00  59.1697\n",
            "2016-01-01 04:00:00  39.9556\n",
            "X shape: (17508, 12, 12, 1)\n",
            "y shape: (17508, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObWmpMDVuNQ",
        "outputId": "586cd392-1218-4424-e704-9e1bef1c0bc1"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
            "X example:\n",
            " [[[-1]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 1]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [-1]\n",
            "  [ 1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 3]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 2]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-2]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 2]\n",
            "  [ 1]\n",
            "  [ 0]]]\n",
            "y example:\n",
            " [ 43 119 327 322 273  92 328 363 346 168 266  27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_DNN():\n",
        "    print(\"make_DNN\")\n",
        "    print(\"input shape:\",INPUT_SHAPE)\n",
        "    dnn = Sequential()\n",
        "    dnn.add(Conv2D( input_shape=INPUT_SHAPE,\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(Conv2D(\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(MaxPooling2D())\n",
        "    #dnn.add(TimeDistributed(Flatten()))\n",
        "    dnn.add(Flatten())\n",
        "    #dnn.add(GRU(CELLS,return_sequences=True))\n",
        "    #dnn.add(GRU(CELLS,return_sequences=False))\n",
        "    dnn.add(Dense(STEPS_FUTURE))   \n",
        "    dnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    dnn.build(input_shape=INPUT_SHAPE)\n",
        "    return dnn    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "d710ed1f-ca7b-40f8-b7ac-9315ac87c057"
      },
      "source": [
        "cors = []\n",
        "overall = 0\n",
        "cnt = 0\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",BLDG)\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # Smoothing window applies to inputs\n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        # Smoothing does not apply to truth\n",
        "        one_bldg_meter = load_meter_for_building(BLDG,0)\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        X_raw,y_raw = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        y_test = np.asarray(y_raw[split:])\n",
        "        # Train and predict\n",
        "        model = make_DNN()\n",
        "        print(model.summary())\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Reporting\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        cnt += 1\n",
        "        print(\"i,mean,rmse,rmse/mean,bldg:\",cnt,mean,rmse,rmse/mean,BLDG)\n",
        "        overall += rmse/mean\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Eagle_office_Lamont\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_158 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_159 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_79 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_79 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [58 64 68 72 72 72 74 73 71 69 70 71]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 659.7018\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 63.1105\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 53.6735\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 47.7831\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45.1761\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 48.8177\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.0317\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.8011\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.5237\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45.2571\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.7536\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.1251\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.3445\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.8952\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.6275\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.4398\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.6541\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41.3264\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39.7206\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41.4588\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.2182\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.2585\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 41.5709\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 41.7949\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.7051\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.1175\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.3205\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39.8272\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.0515\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.3545\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.5222\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 42.4620\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.9773\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.4949\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39.1089\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39.6147\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.6941\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.3443\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.2576\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36.9779\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.7944\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36.9152\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.5534\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37.9134\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37.2859\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.3862\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36.3759\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.2124\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36.7233\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38.3708\n",
            "i,mean,rmse,rmse/mean,bldg: 1 36.92671326723864 8.719301324505654 0.2361244896453163 Eagle_office_Lamont\n",
            "Example prediction:\n",
            " 0 [31 33 35 38 39 41 39 37 34 32 33 34]\n",
            "Example prediction:\n",
            " 2 [31 32 34 36 36 33 30 30 28 26 27 27]\n",
            "Example prediction:\n",
            " 4 [29 30 31 29 28 27 25 24 25 24 26 26]\n",
            "Example prediction:\n",
            " 6 [27 27 27 27 26 26 26 27 26 29 28 29]\n",
            "Example prediction:\n",
            " 8 [27 28 27 28 27 27 26 30 31 33 35 34]\n",
            "Example prediction:\n",
            " 10 [32 32 31 30 30 31 32 38 39 40 40 38]\n",
            "Example prediction:\n",
            " 12 [39 38 38 39 39 43 43 45 44 42 41 39]\n",
            "Example prediction:\n",
            " 14 [44 43 44 44 47 45 43 41 39 40 42 42]\n",
            "Example prediction:\n",
            " 16 [48 45 46 46 43 41 39 40 39 41 43 44]\n",
            "Example prediction:\n",
            " 18 [46 44 41 41 39 37 38 37 39 39 40 41]\n",
            "Example prediction:\n",
            " 20 [42 39 40 40 40 40 42 41 43 44 44 44]\n",
            "Example prediction:\n",
            " 22 [39 38 39 39 40 40 41 41 41 43 43 45]\n",
            "Building Eagle_health_Athena\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_80\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_160 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_161 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_80 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_80 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1341 1348 1320 1187 1052  960  834  819  801  999 1154 1250]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 245763.1812\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 30390.3337\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 21767.1132\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 19719.3104\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 18781.1361\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 17845.1603\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 17921.5362\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 17550.0461\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 17100.7113\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 16790.6168\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 16521.0927\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 16530.5506\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15812.5049\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 16027.9197\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15728.7765\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16291.0048\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15767.5559\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15564.7894\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15713.4226\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15624.5753\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15182.0463\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15377.3107\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15594.1417\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15377.3435\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15037.4073\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15020.1916\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15185.6796\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14509.7682\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15039.3320\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14524.5045\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14669.9199\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14383.7274\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 15015.4091\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14505.5018\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14764.8671\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14309.0398\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14817.4462\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14466.1883\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14540.6264\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14642.8874\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14349.5145\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14318.0585\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14236.1998\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14206.1081\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14129.9061\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14203.6743\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14355.6819\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14451.6283\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14192.4957\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 14374.1445\n",
            "i,mean,rmse,rmse/mean,bldg: 2 477.70168061445776 175.555886287007 0.36750108574287016 Eagle_health_Athena\n",
            "Example prediction:\n",
            " 0 [568 615 672 727 773 785 733 621 499 383 316 313]\n",
            "Example prediction:\n",
            " 2 [637 716 764 768 729 642 499 366 282 237 270 359]\n",
            "Example prediction:\n",
            " 4 [735 778 761 680 570 455 351 284 288 320 396 490]\n",
            "Example prediction:\n",
            " 6 [627 610 559 485 416 369 342 341 384 445 509 564]\n",
            "Example prediction:\n",
            " 8 [594 531 456 386 342 345 391 474 581 674 733 746]\n",
            "Example prediction:\n",
            " 10 [539 438 381 336 361 450 582 708 800 842 803 770]\n",
            "Example prediction:\n",
            " 12 [464 394 391 456 551 691 787 820 815 759 687 629]\n",
            "Example prediction:\n",
            " 14 [538 541 574 631 692 748 762 740 689 651 635 633]\n",
            "Example prediction:\n",
            " 16 [683 688 696 689 688 664 630 624 630 657 687 741]\n",
            "Example prediction:\n",
            " 18 [659 643 635 611 604 611 624 641 664 681 704 738]\n",
            "Example prediction:\n",
            " 20 [682 650 614 607 612 628 650 664 689 705 717 732]\n",
            "Example prediction:\n",
            " 22 [681 674 651 644 650 668 691 705 709 706 689 664]\n",
            "Building Eagle_assembly_Herbert\n",
            "Building Eagle_public_Alvin\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_81\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_162 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_163 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_81 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_81 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [560 549 468 422 314 298 296 434 517 515 470 450]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36128.7408\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 5819.8566\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4722.5962\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4494.3039\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4452.4706\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4358.9076\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4295.4000\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4371.8766\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4313.3672\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4255.2636\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4191.9878\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4221.7091\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4084.9790\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4249.4919\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4077.2324\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4149.2583\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4172.7589\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4026.9512\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4105.4792\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3987.5931\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4015.6151\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4108.6205\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4139.4970\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4049.6433\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4041.0798\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4004.7112\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3974.8411\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4016.3907\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3954.4283\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3966.7335\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3915.3035\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4007.3114\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3935.6775\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3897.1820\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3939.8643\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3962.8296\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3948.4409\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3988.8063\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3897.1160\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3919.1681\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3912.6918\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3895.9340\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3855.4192\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3785.5356\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3832.6624\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3905.4945\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3862.6768\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3895.1675\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3815.5502\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3807.5827\n",
            "i,mean,rmse,rmse/mean,bldg: 3 182.08293646830808 73.50490759395535 0.4036891595646528 Eagle_public_Alvin\n",
            "Example prediction:\n",
            " 0 [287 292 293 305 300 307 302 280 259 237 226 220]\n",
            "Example prediction:\n",
            " 2 [275 284 286 287 268 254 231 207 198 192 199 214]\n",
            "Example prediction:\n",
            " 4 [288 291 279 267 236 211 188 174 177 184 205 229]\n",
            "Example prediction:\n",
            " 6 [276 265 246 225 203 187 175 179 196 217 241 261]\n",
            "Example prediction:\n",
            " 8 [251 229 207 192 190 191 197 216 243 268 286 292]\n",
            "Example prediction:\n",
            " 10 [230 226 222 228 236 252 266 288 305 313 307 289]\n",
            "Example prediction:\n",
            " 12 [254 253 262 265 291 300 307 314 308 299 286 281]\n",
            "Example prediction:\n",
            " 14 [268 276 289 293 299 296 296 283 271 264 267 278]\n",
            "Example prediction:\n",
            " 16 [300 299 299 295 277 268 263 269 263 275 281 290]\n",
            "Example prediction:\n",
            " 18 [310 298 282 265 254 251 263 271 287 298 316 321]\n",
            "Example prediction:\n",
            " 20 [288 266 256 246 239 242 242 262 279 295 293 280]\n",
            "Example prediction:\n",
            " 22 [262 256 255 254 257 262 267 270 271 260 251 233]\n",
            "Building Eagle_education_Raul\n",
            "Building Eagle_education_Roman\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_82\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_164 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_165 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_82 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_82 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [2589 2505 2475 2437 2673 2987 3111 2693 2817 3019 3655 3750]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1305089.1600\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 137432.1623\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 100553.9837\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 93027.8142\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 87451.6802\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 87945.4491\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 85399.2056\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 82297.9065\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 84139.8136\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 80905.6814\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 77724.7473\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 75340.6059\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 76873.7007\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 74976.2494\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 74050.1008\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 74246.0839\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 75124.0110\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 72999.4737\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 72572.3019\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 71448.9932\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 74174.4099\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 71375.0860\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 67414.1219\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 69911.1535\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 68910.7048\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 68085.1022\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 69059.4780\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 67316.4797\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 68708.1603\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 68859.4991\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 64712.8761\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66680.5728\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66902.5590\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 65333.5097\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66453.7981\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66946.0777\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66829.8609\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 65304.2662\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66284.1210\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 65336.8288\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 64796.7181\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 65462.5170\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 63817.0242\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 64848.4563\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 63195.4058\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 63409.4716\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 65961.1860\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 65537.8732\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 63986.9581\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 66584.0954\n",
            "i,mean,rmse,rmse/mean,bldg: 4 1199.4083427425896 327.18190134905706 0.27278608101133994 Eagle_education_Roman\n",
            "Example prediction:\n",
            " 0 [1141 1079 1012  980  955  932  905  896  857  848  851  909]\n",
            "Example prediction:\n",
            " 2 [1091 1028 1009  961  943  914  877  887  863  877  916 1010]\n",
            "Example prediction:\n",
            " 4 [1146 1088 1074 1010  990  966  943  981  980 1013 1068 1179]\n",
            "Example prediction:\n",
            " 6 [1162 1107 1105 1070 1074 1084 1081 1136 1174 1220 1276 1359]\n",
            "Example prediction:\n",
            " 8 [1167 1130 1155 1160 1184 1207 1243 1338 1417 1485 1536 1576]\n",
            "Example prediction:\n",
            " 10 [1358 1320 1345 1353 1390 1457 1551 1640 1685 1719 1724 1706]\n",
            "Example prediction:\n",
            " 12 [1330 1304 1335 1375 1473 1536 1597 1629 1650 1654 1629 1620]\n",
            "Example prediction:\n",
            " 14 [1312 1341 1385 1401 1458 1451 1460 1466 1463 1453 1410 1401]\n",
            "Example prediction:\n",
            " 16 [1295 1325 1359 1348 1360 1350 1392 1388 1339 1297 1237 1223]\n",
            "Example prediction:\n",
            " 18 [1377 1373 1356 1329 1316 1294 1297 1268 1243 1229 1222 1256]\n",
            "Example prediction:\n",
            " 20 [1420 1370 1327 1269 1229 1180 1173 1148 1170 1188 1198 1235]\n",
            "Example prediction:\n",
            " 22 [1355 1293 1239 1193 1146 1128 1142 1167 1175 1203 1199 1231]\n",
            "Building Eagle_office_Mandi\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_83\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_166 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_167 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_83 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_83 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [63 63 63 63 63 63 63 63 63 63 63 63]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 775.9719\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 67.0963\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 57.5556\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 55.4612\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 52.9319\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 53.1132\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 53.3857\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 52.4316\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 49.0233\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 50.1295\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 49.6038\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 48.0370\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 47.4465\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.6661\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 49.6042\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 49.3933\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 49.4445\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 46.4723\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.9556\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 49.8459\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 42.2736\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 47.5344\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 47.4480\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 44.5967\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45.3184\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 44.3164\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43.7042\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 45.6520\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 46.1925\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 47.8272\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.9472\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43.9913\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.7517\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 44.7002\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45.7089\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46.6383\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 44.0761\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40.9423\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43.4660\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.4522\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 44.5473\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 42.5246\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.7069\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42.6530\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43.5774\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 42.2484\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 44.5744\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 40.5772\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43.4125\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 40.1048\n",
            "i,mean,rmse,rmse/mean,bldg: 5 35.89375163018761 8.477640558153208 0.23618708474661881 Eagle_office_Mandi\n",
            "Example prediction:\n",
            " 0 [50 50 50 51 53 54 55 55 54 55 55 55]\n",
            "Example prediction:\n",
            " 2 [52 53 53 55 56 57 56 56 56 57 57 58]\n",
            "Example prediction:\n",
            " 4 [53 54 55 56 57 57 57 57 56 56 57 57]\n",
            "Example prediction:\n",
            " 6 [51 52 53 54 54 55 53 54 52 51 51 50]\n",
            "Example prediction:\n",
            " 8 [54 54 55 54 54 54 54 52 52 51 50 51]\n",
            "Example prediction:\n",
            " 10 [63 63 61 62 61 61 60 60 60 59 58 58]\n",
            "Example prediction:\n",
            " 12 [60 61 64 64 64 64 64 61 59 59 60 59]\n",
            "Example prediction:\n",
            " 14 [58 59 59 59 60 59 59 59 56 54 54 53]\n",
            "Example prediction:\n",
            " 16 [58 59 61 58 57 57 54 53 50 50 48 49]\n",
            "Example prediction:\n",
            " 18 [60 59 57 55 51 49 48 47 48 49 51 53]\n",
            "Example prediction:\n",
            " 20 [56 55 52 52 49 48 48 49 50 53 55 54]\n",
            "Example prediction:\n",
            " 22 [47 46 44 44 44 45 46 46 46 47 47 48]\n",
            "Building Eagle_education_Jewell\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_84\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_168 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_84 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_84 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [223 207 206 207 205 205 196 203 210 217 222 222]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2463.3853\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1029.2480\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 984.6193\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 987.6541\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 975.7759\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 947.0319\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 966.2015\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 937.0979\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 919.2583\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 919.7068\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 921.6767\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 934.8754\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 925.8828\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 939.4416\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 873.7538\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 901.5865\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 864.1342\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 861.5694\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 859.4904\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 928.6145\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 858.2401\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 893.9567\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 872.7213\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 858.7021\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 825.9518\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 870.7257\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 836.3576\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 846.4238\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 824.9957\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 842.3015\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 805.9005\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 829.7112\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 807.0463\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 830.1482\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 840.7333\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 800.0839\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 822.3253\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 800.8465\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 803.4808\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 803.8392\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 826.9542\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 819.7631\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 812.6956\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 807.6173\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 803.9488\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 812.1682\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 779.5033\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 803.9027\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 776.5080\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 788.7197\n",
            "i,mean,rmse,rmse/mean,bldg: 6 15.763073324212357 38.04007160749156 2.4132395266512745 Eagle_education_Jewell\n",
            "Example prediction:\n",
            " 0 [36 36 35 35 32 32 37 40 42 43 45 47]\n",
            "Example prediction:\n",
            " 2 [26 28 30 31 30 32 36 40 42 43 45 49]\n",
            "Example prediction:\n",
            " 4 [30 32 37 38 37 40 44 47 49 49 50 55]\n",
            "Example prediction:\n",
            " 6 [32 35 40 42 43 47 49 51 53 53 53 58]\n",
            "Example prediction:\n",
            " 8 [34 38 44 46 46 48 50 51 53 53 55 57]\n",
            "Example prediction:\n",
            " 10 [56 63 68 70 73 77 82 87 83 82 87 91]\n",
            "Example prediction:\n",
            " 12 [ 53  56  68  77  80  83  87  94  98 102 101 101]\n",
            "Example prediction:\n",
            " 14 [54 62 70 72 75 80 81 83 86 85 88 94]\n",
            "Example prediction:\n",
            " 16 [51 59 66 70 73 80 77 75 81 82 81 78]\n",
            "Example prediction:\n",
            " 18 [76 80 86 87 82 86 87 86 83 79 77 78]\n",
            "Example prediction:\n",
            " 20 [76 77 82 82 78 77 74 72 72 71 72 80]\n",
            "Example prediction:\n",
            " 22 [67 67 67 65 63 61 62 64 67 68 72 76]\n",
            "Building Eagle_office_Henriette\n",
            " Count bad values before pseudofill: 162\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_170 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_171 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_85 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_85 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 0.0207\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.6489e-04\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.7990e-04\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7.6392e-05\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2.7665e-05\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.5495e-05\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6.6497e-06\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3.4683e-06\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2.1377e-06\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.6684e-06\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8.7902e-07\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0132e-07\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.6025e-06\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.5408e-06\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2.6298e-05\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1.4453e-04\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1.1387e-05\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.3719e-06\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.4845e-06\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 7.0194e-05\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3.2999e-06\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3.4871e-07\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.5459e-06\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3.3538e-05\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.1117e-05\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8.8786e-07\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.0245e-07\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.1204e-08\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2.4511e-08\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.6685e-08\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8.9207e-06\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.2248e-05\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7.1160e-07\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.5588e-08\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2.2868e-08\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2.7747e-08\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.2919e-07\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.6820e-05\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.8931e-06\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9881e-08\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.6611e-08\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 7.7281e-09\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7.1093e-09\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6.9756e-09\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3.2144e-08\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1.7373e-06\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3.3301e-07\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 3.2469e-08\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7.0051e-09\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3.9917e-09\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i,mean,rmse,rmse/mean,bldg: 7 0.0 4.781104377111306e-05 inf Eagle_office_Henriette\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building Eagle_health_Reba\n",
            " Count bad values before pseudofill: 36\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_86\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_172 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_173 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_86 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_86 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1765 1738 1266 1758 2276 3012 2801 2550 2612 2622 2627 2592]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1140635.5386\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 120259.0463\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 72695.7729\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 59850.3341\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 54180.4434\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 50631.6734\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 48066.1703\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46687.6154\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45380.7026\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 46053.4725\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45020.3542\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41030.4434\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41460.8244\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41082.5859\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41298.9322\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40948.9183\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 40748.3200\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 42331.6997\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38396.7622\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40251.5152\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40569.2662\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40095.1387\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38175.9818\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37622.5042\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39835.5756\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37585.4385\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40732.8898\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37853.5748\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37991.5936\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40617.0234\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36404.0599\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36544.0044\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37232.5658\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37561.7729\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37940.4760\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38895.9115\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37615.4131\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36131.8216\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36444.3073\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36809.7378\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38662.2076\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37144.3280\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37712.9933\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 35812.6771\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36767.4964\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 37874.7451\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36395.2955\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 35901.6216\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 34054.3904\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36975.7681\n",
            "i,mean,rmse,rmse/mean,bldg: 8 1084.2856908630224 262.73930738076956 0.24231557198882317 Eagle_health_Reba\n",
            "Example prediction:\n",
            " 0 [742 804 854 907 939 945 919 863 824 778 785 785]\n",
            "Example prediction:\n",
            " 2 [685 767 810 834 815 770 719 659 639 623 667 707]\n",
            "Example prediction:\n",
            " 4 [675 727 732 737 687 653 620 586 607 616 673 718]\n",
            "Example prediction:\n",
            " 6 [636 645 630 630 611 603 608 615 647 668 723 747]\n",
            "Example prediction:\n",
            " 8 [827 790 746 731 733 739 789 844 902 926 965 967]\n",
            "Example prediction:\n",
            " 10 [1092 1028  983  992 1013 1060 1124 1189 1218 1230 1244 1220]\n",
            "Example prediction:\n",
            " 12 [1146 1112 1127 1132 1179 1207 1258 1306 1303 1305 1291 1270]\n",
            "Example prediction:\n",
            " 14 [1240 1221 1250 1250 1281 1279 1300 1315 1314 1310 1295 1291]\n",
            "Example prediction:\n",
            " 16 [1267 1275 1275 1266 1255 1248 1242 1251 1249 1243 1229 1226]\n",
            "Example prediction:\n",
            " 18 [1219 1205 1181 1171 1160 1163 1168 1182 1205 1212 1215 1227]\n",
            "Example prediction:\n",
            " 20 [1168 1134 1130 1123 1121 1150 1172 1218 1256 1284 1301 1316]\n",
            "Example prediction:\n",
            " 22 [1094 1083 1099 1117 1145 1189 1221 1267 1300 1314 1308 1311]\n",
            "Building Eagle_lodging_Edgardo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_174 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_175 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_87 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_87 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [203 120 203 174 176 184 226 258 235 184 243 259]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 6258.1513\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1943.3479\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1838.3063\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1745.8354\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1695.3924\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1725.1402\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1695.0671\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1673.6496\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1661.8922\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1700.4365\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1670.2898\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1709.5919\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1640.2884\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1650.6812\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1660.7696\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1650.0463\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1651.0891\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1664.1878\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1635.8897\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1623.7162\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1654.5385\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1653.1096\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1619.1607\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1612.6569\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1648.3981\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1611.6667\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1635.8089\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1618.7282\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1625.4530\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1593.9209\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1576.7196\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1639.2072\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1636.9134\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1563.9888\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1562.4964\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1590.5786\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1620.1917\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1584.6749\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1579.4283\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1597.6743\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1569.8106\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1569.8684\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1578.8282\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1599.9411\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1545.4309\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1631.3089\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1574.5950\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1564.6136\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1591.5811\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1579.3787\n",
            "i,mean,rmse,rmse/mean,bldg: 9 81.9677919573641 66.81978950986519 0.8151956752064493 Eagle_lodging_Edgardo\n",
            "Example prediction:\n",
            " 0 [ 80  83  80  88  87  94  97 103 107 106  97  92]\n",
            "Example prediction:\n",
            " 2 [62 66 67 76 80 88 86 84 82 80 72 70]\n",
            "Example prediction:\n",
            " 4 [55 57 63 72 78 82 74 71 70 72 68 65]\n",
            "Example prediction:\n",
            " 6 [58 60 62 68 74 76 69 67 66 66 65 65]\n",
            "Example prediction:\n",
            " 8 [71 74 72 74 79 80 74 73 72 75 76 78]\n",
            "Example prediction:\n",
            " 10 [ 87  87  81  85  86  89  90  91  95 101  99  97]\n",
            "Example prediction:\n",
            " 12 [102 102  98  99 105 111 105 110 105 103 102 104]\n",
            "Example prediction:\n",
            " 14 [116 117 118 126 118 120 107 102  97  97 103 101]\n",
            "Example prediction:\n",
            " 16 [132 131 128 133 125 112 104  95  94  97  97 106]\n",
            "Example prediction:\n",
            " 18 [132 129 121 114 102 102  97 101 108 121 130 137]\n",
            "Example prediction:\n",
            " 20 [114 103  98  95  99 102 107 118 129 132 135 137]\n",
            "Example prediction:\n",
            " 22 [ 94  90  91  98 105 112 118 122 130 137 139 141]\n",
            "Building Eagle_education_Cassie\n",
            "Building Eagle_education_Peter\n",
            " Count bad values before pseudofill: 34\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_176 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_177 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_88 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_88 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [7938 7923 8023 8046 8049 8002 8064 8097 8187 8160 8224 8161]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11110245.8109\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1208536.5105\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 765907.3518\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 627583.4659\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 541473.7023\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 515206.2268\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 488600.2666\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 465530.7710\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 483077.1236\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 450041.6863\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 452831.3481\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 442159.1675\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 445422.1649\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 437135.4222\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 431298.5717\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 428786.2225\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 420757.5862\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 422522.4667\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 421179.4715\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 435422.7035\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 410499.6235\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 408465.1962\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 406189.1634\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 402384.7373\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 410591.0274\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 392237.0139\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 392549.2484\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 405749.2176\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 386990.9248\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 376759.1267\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 374698.4587\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 377864.8015\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 390397.3502\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 370208.8461\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 376587.5740\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 363408.0072\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 365330.1557\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 363488.8101\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 367076.6368\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 367755.7842\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 349686.2938\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 361972.4622\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 353923.4281\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 344809.7799\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 346829.8440\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 348848.0411\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 351927.0177\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 341144.3140\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 337096.9448\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 337015.4543\n",
            "i,mean,rmse,rmse/mean,bldg: 10 3153.2853604405936 766.9760861508506 0.24323078899642775 Eagle_education_Peter\n",
            "Example prediction:\n",
            " 0 [3039 3026 2971 2938 2908 2846 2739 2670 2653 2633 2682 2771]\n",
            "Example prediction:\n",
            " 2 [2944 2917 2857 2818 2749 2653 2570 2486 2562 2600 2730 2897]\n",
            "Example prediction:\n",
            " 4 [3018 2995 2943 2887 2832 2779 2762 2764 2852 2960 3102 3271]\n",
            "Example prediction:\n",
            " 6 [3048 3018 3008 2995 2978 3020 3056 3110 3223 3304 3460 3573]\n",
            "Example prediction:\n",
            " 8 [3173 3132 3143 3176 3173 3278 3362 3451 3626 3719 3861 3941]\n",
            "Example prediction:\n",
            " 10 [3689 3649 3694 3730 3774 3922 4053 4213 4315 4423 4483 4490]\n",
            "Example prediction:\n",
            " 12 [3632 3655 3753 3849 3949 4122 4272 4391 4461 4467 4479 4469]\n",
            "Example prediction:\n",
            " 14 [3500 3549 3627 3687 3781 3849 3838 3828 3843 3840 3896 3910]\n",
            "Example prediction:\n",
            " 16 [3566 3589 3607 3613 3575 3520 3479 3470 3506 3513 3564 3590]\n",
            "Example prediction:\n",
            " 18 [3431 3440 3358 3326 3290 3249 3297 3305 3372 3459 3474 3508]\n",
            "Example prediction:\n",
            " 20 [3312 3287 3201 3179 3161 3162 3203 3230 3328 3373 3436 3479]\n",
            "Example prediction:\n",
            " 22 [3326 3296 3266 3282 3271 3320 3372 3400 3495 3542 3587 3656]\n",
            "Building Eagle_health_Gregoria\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_89\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_178 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_179 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_89 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_89 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 533781.9038\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 66242.1801\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 49295.2181\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45501.2739\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43463.5372\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 43193.7801\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38449.4522\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39649.1218\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 45747.1921\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 40683.6192\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 34812.6964\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32653.9092\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 33796.2576\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32364.7487\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 34877.5221\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 35726.4995\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 41374.3318\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 29804.9992\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 38555.9343\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 36442.0004\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35864.6828\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39113.0283\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34559.3179\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29392.1775\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34188.3250\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 39465.6689\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 34264.7302\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31275.3824\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 31541.0433\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 35926.9214\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32718.6423\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 34639.2909\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32289.0848\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34156.1839\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 28857.1323\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29728.4248\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 30862.3090\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 33762.4407\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32088.0003\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33023.8913\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32284.6738\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 30380.1638\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 30813.2630\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35466.6311\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 31862.3730\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 32390.0048\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29215.9434\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31512.9009\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 33323.6671\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33462.6379\n",
            "i,mean,rmse,rmse/mean,bldg: 11 659.65505791154 373.2433822140574 0.5658159938858675 Eagle_health_Gregoria\n",
            "Example prediction:\n",
            " 0 [678 699 671 676 652 642 593 585 569 558 565 541]\n",
            "Example prediction:\n",
            " 2 [344 370 342 318 300 289 255 240 230 213 218 183]\n",
            "Example prediction:\n",
            " 4 [122 141 140 107 105  97  85  63  74  47  61  28]\n",
            "Example prediction:\n",
            " 6 [132 137 147 130 141 128 141 134 148 135 138 122]\n",
            "Example prediction:\n",
            " 8 [513 496 519 510 531 510 552 554 575 578 565 568]\n",
            "Example prediction:\n",
            " 10 [ 911  888  904  914  944  935  967  968 1002 1013  998 1012]\n",
            "Example prediction:\n",
            " 12 [1080 1089 1082 1098 1130 1112 1140 1147 1157 1194 1187 1207]\n",
            "Example prediction:\n",
            " 14 [1097 1104 1107 1117 1116 1095 1103 1111 1124 1122 1116 1115]\n",
            "Example prediction:\n",
            " 16 [1051 1061 1036 1030 1025 1011 1010 1006 1015  996  982  951]\n",
            "Example prediction:\n",
            " 18 [931 939 926 919 914 899 893 880 868 839 844 831]\n",
            "Example prediction:\n",
            " 20 [903 896 892 881 876 869 860 848 841 821 838 841]\n",
            "Example prediction:\n",
            " 22 [957 951 950 950 951 934 934 938 943 932 945 936]\n",
            "Building Eagle_lodging_Dawn\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_180 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_181 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_90 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_90 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [305 286 264 281 322 322 322 293 305 300 312 293]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8313.8582\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1551.1593\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1434.3637\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1408.9539\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1379.9279\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1346.0148\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1327.5565\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1357.3575\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1303.1778\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1314.3048\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1303.9959\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1298.9592\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1242.5947\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1302.9200\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1276.3640\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1212.8173\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1262.7580\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1236.5884\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1227.9606\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1241.4258\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1216.2026\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1210.2193\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1228.6872\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1240.5888\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1225.9520\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1210.3096\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1208.4577\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1154.8623\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1196.7012\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1188.4006\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1199.2633\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1203.3477\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1197.0702\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1196.1090\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1223.7194\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1190.7020\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1203.3762\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1213.6241\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1166.8890\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1201.4939\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1155.4963\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 1169.7644\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1153.2177\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1148.3147\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1156.5799\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1169.5048\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1161.1811\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1165.0560\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1160.5562\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1162.7518\n",
            "i,mean,rmse,rmse/mean,bldg: 12 92.82705527815749 54.941874841085905 0.5918735079600647 Eagle_lodging_Dawn\n",
            "Example prediction:\n",
            " 0 [66 65 71 78 84 83 84 78 68 62 52 54]\n",
            "Example prediction:\n",
            " 2 [57 63 65 68 63 61 55 45 38 40 40 46]\n",
            "Example prediction:\n",
            " 4 [55 56 56 54 50 42 35 32 26 34 42 54]\n",
            "Example prediction:\n",
            " 6 [55 57 60 62 61 55 54 48 46 54 64 72]\n",
            "Example prediction:\n",
            " 8 [ 75  80  79  79  78  80  82  76  80  87  92 100]\n",
            "Example prediction:\n",
            " 10 [106 102 102 108 114 114 113 116 121 124 132 126]\n",
            "Example prediction:\n",
            " 12 [116 114 117 123 122 127 129 138 135 137 130 129]\n",
            "Example prediction:\n",
            " 14 [135 128 125 115 115 136 130 126 120 121 115 115]\n",
            "Example prediction:\n",
            " 16 [135 134 134 132 126 125 120 124 119 118 119 115]\n",
            "Example prediction:\n",
            " 18 [133 134 120 112  94  95  93 101 100 116 112 115]\n",
            "Example prediction:\n",
            " 20 [125 110  99  93  85  91  93 102 106 114 116 122]\n",
            "Example prediction:\n",
            " 22 [112 102  98  98 103 113 117 126 129 136 136 138]\n",
            "Building Eagle_office_Nereida\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_182 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_183 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_91 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_91 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [433 474 507 535 536 537 550 540 527 515 523 530]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 50417.1794\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5172.1454\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3134.2182\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3057.2553\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2864.0725\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2861.3953\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2651.2243\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2644.3777\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2697.9861\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2749.4490\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2630.8825\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2474.7365\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2420.7057\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2553.3506\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2521.5141\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2520.3846\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2350.9516\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2406.0919\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2337.9710\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2443.3674\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2434.4087\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2397.6653\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2384.7494\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2293.4392\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2265.1135\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2319.1906\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2371.1513\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2286.8085\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2286.9673\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2270.0740\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2377.7597\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2293.6767\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2447.3584\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2114.8677\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2331.1768\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2403.2086\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2311.7785\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2244.5256\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2209.6965\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2196.3120\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2255.1962\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2260.6612\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2220.6685\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2127.3725\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 2214.1348\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2214.0511\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2219.7478\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2296.8184\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2290.4001\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2077.7201\n",
            "i,mean,rmse,rmse/mean,bldg: 13 272.9589435921015 58.6104222833502 0.2147224835795642 Eagle_office_Nereida\n",
            "Example prediction:\n",
            " 0 [226 233 247 258 267 266 262 245 231 216 226 234]\n",
            "Example prediction:\n",
            " 2 [196 211 228 227 224 211 207 191 185 181 181 186]\n",
            "Example prediction:\n",
            " 4 [192 201 192 189 173 158 154 156 158 165 168 178]\n",
            "Example prediction:\n",
            " 6 [155 156 147 141 139 132 138 145 162 172 178 190]\n",
            "Example prediction:\n",
            " 8 [187 177 175 169 163 168 178 197 219 235 238 243]\n",
            "Example prediction:\n",
            " 10 [234 229 225 231 224 245 259 282 296 304 305 307]\n",
            "Example prediction:\n",
            " 12 [256 267 261 278 290 301 308 322 322 316 307 303]\n",
            "Example prediction:\n",
            " 14 [323 317 328 329 328 326 326 322 317 314 321 327]\n",
            "Example prediction:\n",
            " 16 [369 363 353 350 342 328 322 321 328 326 333 341]\n",
            "Example prediction:\n",
            " 18 [340 325 308 302 302 293 301 304 313 319 322 335]\n",
            "Example prediction:\n",
            " 20 [291 282 274 275 278 286 297 305 314 317 321 329]\n",
            "Example prediction:\n",
            " 22 [276 273 275 278 282 289 297 301 301 308 319 332]\n",
            "Building Eagle_lodging_Tressa\n",
            "Building Eagle_education_Eileen\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_184 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_185 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_92 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_92 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 95  89  84  64  47  38  35  69  92 112  99  88]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1551.6106\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 362.1432\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 286.7381\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 262.1705\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 254.8712\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 250.8405\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 247.8493\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 246.0881\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 242.0679\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 236.7487\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 232.9635\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 230.1989\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 232.5453\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 225.4559\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 221.5233\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 220.7165\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 227.5327\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 223.6470\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 217.4946\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 220.6131\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 222.9473\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 222.4018\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 209.6109\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 210.6486\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.1948\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 214.1764\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 208.0945\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 214.3615\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 212.2998\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 212.4360\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 200.7020\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 203.8951\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 209.6596\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 209.1654\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.8566\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 215.3275\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 208.7929\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 203.8060\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 206.3790\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 208.9928\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 197.9807\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 198.6023\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 201.5426\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207.1917\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 202.2542\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 200.8353\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205.6801\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.7853\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 192.5152\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 198.7784\n",
            "i,mean,rmse,rmse/mean,bldg: 14 46.463674127906486 21.126565254244618 0.45468994113738886 Eagle_education_Eileen\n",
            "Example prediction:\n",
            " 0 [46 53 54 54 53 52 48 41 26 11  3  5]\n",
            "Example prediction:\n",
            " 2 [36 43 42 36 33 28 22 11  2  0  9 24]\n",
            "Example prediction:\n",
            " 4 [29 33 30 21 13  7  4  3  7 10 18 26]\n",
            "Example prediction:\n",
            " 6 [35 29 22 11  7  4  4  2 10 22 38 47]\n",
            "Example prediction:\n",
            " 8 [30 24 24 19 11  3  2 11 38 66 81 78]\n",
            "Example prediction:\n",
            " 10 [35 22 16  6  9 19 41 67 92 96 86 67]\n",
            "Example prediction:\n",
            " 12 [ 26  10   5  18  44  72  95 100  88  81  71  66]\n",
            "Example prediction:\n",
            " 14 [14 22 45 75 97 98 91 84 77 68 57 45]\n",
            "Example prediction:\n",
            " 16 [46 65 81 92 84 78 63 54 45 44 42 50]\n",
            "Example prediction:\n",
            " 18 [73 70 63 56 51 48 41 41 42 42 45 47]\n",
            "Example prediction:\n",
            " 20 [69 64 52 41 34 39 43 48 54 58 58 59]\n",
            "Example prediction:\n",
            " 22 [48 46 42 42 40 44 47 52 55 57 46 32]\n",
            "Building Eagle_education_Wesley\n",
            " Count bad values before pseudofill: 112\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_186 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_187 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_93 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_93 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 0.0310\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 0.0014\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7.1929e-04\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.6940e-04\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.1197e-04\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9587e-04\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0376e-04\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8275e-04\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0274e-04\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0365e-04\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.1549e-04\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0480e-04\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0975e-04\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.1131e-04\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9202e-04\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0787e-04\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5.0615e-04\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8809e-04\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9596e-04\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8129e-04\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8199e-04\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8903e-04\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9031e-04\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8910e-04\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.8667e-04\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.9161e-04\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.6734e-04\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.5871e-04\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.6857e-04\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.6505e-04\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.6746e-04\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.6700e-04\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3908e-04\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.5579e-04\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4.5252e-04\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3659e-04\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.5043e-04\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.2712e-04\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.5499e-04\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3728e-04\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.2581e-04\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3750e-04\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.4717e-04\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.4439e-04\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3787e-04\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 2ms/step - loss: 4.2642e-04\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.2635e-04\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.2775e-04\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3643e-04\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4.3864e-04\n",
            "i,mean,rmse,rmse/mean,bldg: 15 0.10542525466510344 0.036180537263671105 0.34318662429228297 Eagle_education_Wesley\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building Eagle_health_Vincenza\n",
            " Count bad values before pseudofill: 75\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_188 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_189 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_94 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_94 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [249 254 202 138  82  79  84  86 160 230 306 296]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10497.6901\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2107.2664\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1289.9958\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1118.8726\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 949.2512\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 909.4769\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 875.4009\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 852.4652\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 854.9547\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 807.4597\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 775.5734\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 761.0276\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 766.3324\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 718.3040\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 761.0825\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 729.4096\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 693.6765\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 722.7498\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 709.9421\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 684.8255\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 683.8765\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 663.6235\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 648.7976\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 663.6715\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 675.4335\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 659.2469\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 691.9441\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 659.5261\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 675.3678\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 671.4854\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 657.5456\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 651.5036\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 649.6407\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 645.7910\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 648.2618\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 645.2024\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 651.6245\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 637.5134\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 644.4450\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 646.7314\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 647.7174\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 629.3812\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 635.9822\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 637.1647\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 627.9836\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 634.0159\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 610.2291\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 627.0013\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 621.6894\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 652.2380\n",
            "i,mean,rmse,rmse/mean,bldg: 16 122.34687520459563 40.28483818068513 0.3292674055901996 Eagle_health_Vincenza\n",
            "Example prediction:\n",
            " 0 [176 174 174 175 189 195 178 136  86  51  28  18]\n",
            "Example prediction:\n",
            " 2 [148 164 184 188 170 134  89  46  17   5  21  60]\n",
            "Example prediction:\n",
            " 4 [149 164 151 130  86  55  32  23  16  27  44  72]\n",
            "Example prediction:\n",
            " 6 [140 131  94  58  35  34  33  30  40  78 108 121]\n",
            "Example prediction:\n",
            " 8 [108  82  56  39  37  34  38  69 116 169 191 183]\n",
            "Example prediction:\n",
            " 10 [ 91  40  20  14  30  49 104 172 209 213 195 182]\n",
            "Example prediction:\n",
            " 12 [ 38  11  11  45  95 159 201 212 198 183 177 173]\n",
            "Example prediction:\n",
            " 14 [ 24  52 110 175 210 219 205 185 172 166 168 169]\n",
            "Example prediction:\n",
            " 16 [139 163 185 200 193 172 162 150 153 157 162 175]\n",
            "Example prediction:\n",
            " 18 [183 175 161 148 135 128 133 147 161 168 179 191]\n",
            "Example prediction:\n",
            " 20 [156 145 138 130 137 153 169 176 183 185 180 154]\n",
            "Example prediction:\n",
            " 22 [148 146 153 161 174 181 185 188 187 169 138  96]\n",
            "Building Eagle_office_Dallas\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_190 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_191 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_95 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_95 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [113 154 154 154 117 109 105 137 178 215 219 219]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3403.0210\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2011.3946\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1757.3349\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2019.9742\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1921.4251\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1840.7980\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1842.2277\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2096.9522\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1863.9514\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1712.7774\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1959.6055\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1899.9960\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1780.5987\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1861.5648\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1835.6683\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1862.3727\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1784.7134\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1788.6160\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1863.9321\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1571.4794\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1877.3086\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1804.1846\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1838.2880\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1715.9264\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1626.7156\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1710.8171\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1797.8896\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1698.5790\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1593.1773\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1659.3267\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1788.0167\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1819.5199\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1805.2057\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1691.2398\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1874.1327\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1781.8369\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1653.7839\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1715.7074\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1718.9727\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1892.2178\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1675.1753\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1794.2632\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1663.1384\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1659.2042\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1622.1317\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1701.1730\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1569.5292\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1813.4725\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1633.8824\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1686.7607\n",
            "i,mean,rmse,rmse/mean,bldg: 17 56.508133937526274 75.17846177349837 1.3304007146407182 Eagle_office_Dallas\n",
            "Example prediction:\n",
            " 0 [11 17 24 29 31 35 37 36 32 30 29 26]\n",
            "Example prediction:\n",
            " 2 [13 16 20 21 24 27 26 27 22 21 21 14]\n",
            "Example prediction:\n",
            " 4 [10 13 15 17 16 19 21 20 16 14 14 10]\n",
            "Example prediction:\n",
            " 6 [14 14 15 16 16 22 25 22 19 17 20 16]\n",
            "Example prediction:\n",
            " 8 [22 22 22 24 27 31 30 25 24 20 23 18]\n",
            "Example prediction:\n",
            " 10 [48 51 47 49 41 42 42 40 43 34 34 37]\n",
            "Example prediction:\n",
            " 12 [64 62 54 45 41 55 64 56 52 47 48 52]\n",
            "Example prediction:\n",
            " 14 [50 43 47 45 53 55 54 47 41 42 44 30]\n",
            "Example prediction:\n",
            " 16 [60 62 66 61 54 47 47 49 44 42 46 39]\n",
            "Example prediction:\n",
            " 18 [73 64 55 53 41 49 51 56 53 60 65 64]\n",
            "Example prediction:\n",
            " 20 [39 35 32 34 36 47 55 58 64 66 71 77]\n",
            "Example prediction:\n",
            " 22 [24 30 34 38 44 50 57 57 60 63 70 75]\n",
            "Building Eagle_education_Shante\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_192 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_193 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_96 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_96 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 171740.9917\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 92529.3514\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 62736.0862\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 45431.7787\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37911.2976\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 28819.0345\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33146.0215\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29582.8407\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24519.9888\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21370.1916\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20202.1798\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17600.2734\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15664.5003\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17253.2058\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18766.4361\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21033.7469\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19067.0786\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18010.4964\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15250.0806\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23121.0958\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23703.0894\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20365.6607\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17195.8203\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15851.4960\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14918.8271\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19955.3324\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17571.5862\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18080.1182\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16810.4443\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14602.6669\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18583.6991\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18466.9985\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17731.8102\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16631.6902\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17309.0287\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14268.4559\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17669.6500\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16885.1544\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19966.4671\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18947.4781\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15987.2521\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17590.6245\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12265.8731\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18171.2539\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15060.7395\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18446.6479\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17192.6517\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18448.3194\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16943.0685\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14874.2949\n",
            "i,mean,rmse,rmse/mean,bldg: 18 1004.1571291019169 707.0140056666777 0.7040870250047484 Eagle_education_Shante\n",
            "Example prediction:\n",
            " 0 [1248 1261 1281 1292 1317 1308 1292 1272 1262 1229 1216 1196]\n",
            "Example prediction:\n",
            " 2 [ 997 1004 1015 1017 1024 1007  984  954  944  911  907  888]\n",
            "Example prediction:\n",
            " 4 [628 627 619 615 615 597 578 552 549 520 524 511]\n",
            "Example prediction:\n",
            " 6 [409 399 373 365 371 359 354 349 347 330 330 328]\n",
            "Example prediction:\n",
            " 8 [494 492 470 468 483 482 495 513 511 512 494 500]\n",
            "Example prediction:\n",
            " 10 [751 760 795 796 839 845 868 897 909 912 870 867]\n",
            "Example prediction:\n",
            " 12 [1055 1081 1145 1161 1217 1222 1235 1256 1267 1252 1220 1205]\n",
            "Example prediction:\n",
            " 14 [1346 1370 1431 1452 1491 1491 1490 1500 1501 1450 1442 1412]\n",
            "Example prediction:\n",
            " 16 [1488 1514 1537 1544 1579 1578 1558 1555 1548 1489 1497 1444]\n",
            "Example prediction:\n",
            " 18 [1414 1414 1436 1444 1463 1442 1443 1417 1412 1370 1362 1328]\n",
            "Example prediction:\n",
            " 20 [1333 1320 1351 1357 1384 1356 1362 1348 1350 1315 1296 1273]\n",
            "Example prediction:\n",
            " 22 [1259 1255 1287 1289 1327 1315 1314 1301 1304 1269 1254 1234]\n",
            "Building Eagle_office_Chauncey\n",
            " Count bad values before pseudofill: 116\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_97\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_194 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_195 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_97 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_97 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1735 1706 1698 1718 1726 1771 1713 1731 1755 1796 1823 1753]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 858880.2755\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 136301.7292\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 48446.4023\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36505.8149\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33012.7867\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31503.4868\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 30392.0252\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29651.9070\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 30024.0807\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29407.4037\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25734.1294\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 27824.5356\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24620.2505\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24876.2306\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23148.9915\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24609.4581\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22868.1422\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22833.5218\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23136.8074\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22424.7319\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22518.5733\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22333.5895\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22905.3403\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23340.8756\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21888.3511\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23001.2549\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23402.7035\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22555.6718\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22471.5495\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22166.1439\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23264.3215\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22668.2155\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21064.3206\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20818.8122\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21425.6783\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22169.1586\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21432.9557\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21787.1186\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22835.2787\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20670.5793\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21234.4204\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19920.6848\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22607.6178\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21080.6539\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21632.7313\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25137.5480\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23345.1226\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22192.1159\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20984.4872\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20193.5380\n",
            "i,mean,rmse,rmse/mean,bldg: 19 1037.6877345457663 181.6882030469145 0.17508947730450536 Eagle_office_Chauncey\n",
            "Example prediction:\n",
            " 0 [1007 1052 1077 1098 1089 1068 1035  996  950  927  922  921]\n",
            "Example prediction:\n",
            " 2 [886 927 946 931 873 840 795 755 714 691 710 709]\n",
            "Example prediction:\n",
            " 4 [810 825 807 776 710 677 649 641 628 637 647 675]\n",
            "Example prediction:\n",
            " 6 [637 624 610 588 574 562 563 585 607 631 652 679]\n",
            "Example prediction:\n",
            " 8 [666 636 619 614 626 651 691 750 797 811 851 869]\n",
            "Example prediction:\n",
            " 10 [ 849  801  796  805  846  890  957 1016 1056 1067 1093 1099]\n",
            "Example prediction:\n",
            " 12 [ 953  929  934  967 1028 1056 1122 1136 1140 1148 1161 1176]\n",
            "Example prediction:\n",
            " 14 [1165 1159 1165 1161 1203 1194 1204 1197 1183 1179 1197 1206]\n",
            "Example prediction:\n",
            " 16 [1348 1334 1323 1296 1295 1261 1229 1233 1205 1216 1231 1235]\n",
            "Example prediction:\n",
            " 18 [1316 1287 1261 1244 1235 1214 1219 1222 1240 1237 1245 1251]\n",
            "Example prediction:\n",
            " 20 [1259 1241 1225 1221 1232 1241 1262 1268 1281 1289 1290 1298]\n",
            "Example prediction:\n",
            " 22 [1204 1199 1214 1222 1240 1265 1283 1294 1302 1307 1317 1318]\n",
            "Building Eagle_office_Phyllis\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_98\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_196 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_197 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_98 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_98 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [154 154 154 154 154 154 154 154 155 155 154 153]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5325.3343\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 579.9727\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 411.6895\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 367.7961\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 344.6435\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 328.2073\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 342.0074\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 305.2433\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 319.3018\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 309.1716\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 301.4457\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 301.5296\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 313.9044\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 293.7785\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 297.6895\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 290.5704\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 296.5554\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 297.5412\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 289.8178\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 274.5520\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 301.2605\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 277.2711\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 279.5245\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 286.7959\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 265.1407\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 280.1354\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 264.5823\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 268.3142\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 273.5794\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 270.9851\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 266.1560\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 268.0173\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 274.0545\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 263.2576\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 263.6763\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 257.0982\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 265.3479\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 248.3864\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 248.7602\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 264.7464\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 262.7273\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 265.8968\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 248.8637\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 247.4597\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 252.5740\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 235.8832\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 235.2385\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 256.8945\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 252.9210\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 252.5791\n",
            "i,mean,rmse,rmse/mean,bldg: 20 87.211947406521 21.13865400004854 0.24238254767451692 Eagle_office_Phyllis\n",
            "Example prediction:\n",
            " 0 [117 117 117 122 126 130 130 129 128 128 127 128]\n",
            "Example prediction:\n",
            " 2 [120 122 123 128 131 132 133 133 132 131 133 133]\n",
            "Example prediction:\n",
            " 4 [128 131 131 135 138 138 138 137 137 135 136 135]\n",
            "Example prediction:\n",
            " 6 [127 129 132 134 136 136 135 133 131 128 127 127]\n",
            "Example prediction:\n",
            " 8 [132 135 135 138 136 136 135 131 129 127 125 126]\n",
            "Example prediction:\n",
            " 10 [157 155 153 155 152 151 152 155 150 150 150 148]\n",
            "Example prediction:\n",
            " 12 [147 146 148 152 151 151 146 152 149 147 146 141]\n",
            "Example prediction:\n",
            " 14 [139 141 143 145 145 145 143 141 135 127 127 130]\n",
            "Example prediction:\n",
            " 16 [139 143 144 140 137 133 131 126 124 119 124 122]\n",
            "Example prediction:\n",
            " 18 [133 131 133 128 124 122 118 116 114 115 122 126]\n",
            "Example prediction:\n",
            " 20 [130 127 126 121 116 116 116 117 117 120 126 131]\n",
            "Example prediction:\n",
            " 22 [113 109 109 109 109 111 112 114 114 115 117 117]\n",
            "Building Eagle_office_Freida\n",
            " Count bad values before pseudofill: 63\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_99\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_198 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_199 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_99 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_99 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [417 437 390 223 148  54  65  75 200 350 504 409]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13707.5014\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3731.4413\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3086.6049\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2878.6970\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2803.5746\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2764.8177\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2711.9701\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2692.3739\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2639.8117\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2675.2965\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2606.6433\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2627.3692\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2651.8285\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2558.2032\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2543.7489\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2604.5747\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2607.6343\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2553.9215\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2611.8032\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2580.6899\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2424.1449\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2515.2874\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2534.4240\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2546.0294\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2452.9358\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2515.8690\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2555.1146\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2413.9664\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2478.6471\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2451.0777\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2534.2618\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2463.9832\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2438.9637\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2410.0888\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2416.5094\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2410.9486\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2491.9013\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2460.8413\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2387.7221\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2401.4825\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2391.8673\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2425.8391\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2365.5005\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2368.4006\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2447.6069\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2377.9718\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2461.3128\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2361.0746\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2430.7881\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2345.0170\n",
            "i,mean,rmse,rmse/mean,bldg: 21 101.60147001775249 65.71116731814783 0.6467541001785342 Eagle_office_Freida\n",
            "Example prediction:\n",
            " 0 [176 172 170 175 180 185 172 139 102  68  56  63]\n",
            "Example prediction:\n",
            " 2 [153 151 152 149 136 112  87  64  56  54  67  85]\n",
            "Example prediction:\n",
            " 4 [169 166 149 123  97  72  60  53  62  72  94 111]\n",
            "Example prediction:\n",
            " 6 [157 136 112  84  71  61  61  66  76  87 105 120]\n",
            "Example prediction:\n",
            " 8 [127  99  83  66  66  66  72  82  94 103 111 124]\n",
            "Example prediction:\n",
            " 10 [116  88  80  71  71  74  85 104 114 118 120 128]\n",
            "Example prediction:\n",
            " 12 [130 108  90  83  86  99 122 121 132 133 133 131]\n",
            "Example prediction:\n",
            " 14 [132 111 110 108 123 134 140 140 135 127 131 142]\n",
            "Example prediction:\n",
            " 16 [145 137 131 121 118 123 120 128 128 129 136 152]\n",
            "Example prediction:\n",
            " 18 [165 131 110 101 100 104 117 138 141 144 149 138]\n",
            "Example prediction:\n",
            " 20 [149 109 100  95 110 121 140 158 160 144 124  93]\n",
            "Example prediction:\n",
            " 22 [132 118 112 117 131 144 162 160 140 110  73  52]\n",
            "Building Eagle_office_Francis\n",
            " Count bad values before pseudofill: 20\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_100\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_200 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_201 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_100 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_100 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [396 408 439 451 463 432 451 481 487 475 475 469]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 57130.1612\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6037.5031\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4314.3364\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3831.7210\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3571.4315\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3584.5393\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3606.4736\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3610.5467\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3362.6588\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3336.9706\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3363.9043\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3350.3707\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3256.3294\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3292.6615\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3359.0687\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3414.3481\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3203.7611\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3118.6949\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3196.3891\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3180.5771\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2987.3104\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3029.4508\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3053.8832\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3033.1566\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3024.7522\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3097.0233\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3148.5913\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3044.0306\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2975.6103\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3036.9820\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3086.8531\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3142.7582\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2888.5984\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2947.6839\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3043.3421\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2859.1681\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3094.4946\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3068.3202\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2958.0522\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2851.7414\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3021.4274\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3098.8326\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3062.8868\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3024.8029\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2831.4434\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2882.6172\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2945.7767\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2908.5128\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3032.8626\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2942.8680\n",
            "i,mean,rmse,rmse/mean,bldg: 22 336.4746029213393 103.267531665772 0.3069103307327887 Eagle_office_Francis\n",
            "Example prediction:\n",
            " 0 [233 256 282 315 322 322 318 293 253 218 214 213]\n",
            "Example prediction:\n",
            " 2 [236 266 284 297 290 267 238 205 175 176 187 203]\n",
            "Example prediction:\n",
            " 4 [237 254 260 251 218 191 164 156 158 179 204 226]\n",
            "Example prediction:\n",
            " 6 [199 196 191 172 153 143 143 166 183 215 243 254]\n",
            "Example prediction:\n",
            " 8 [269 243 212 190 179 181 207 247 290 338 359 356]\n",
            "Example prediction:\n",
            " 10 [304 275 248 246 266 296 330 376 416 433 418 399]\n",
            "Example prediction:\n",
            " 12 [355 336 337 356 382 413 449 463 460 454 429 410]\n",
            "Example prediction:\n",
            " 14 [447 441 429 442 451 456 462 464 442 435 428 430]\n",
            "Example prediction:\n",
            " 16 [490 492 488 483 476 455 457 448 438 432 422 423]\n",
            "Example prediction:\n",
            " 18 [479 481 470 461 451 445 437 432 423 425 423 423]\n",
            "Example prediction:\n",
            " 20 [432 425 417 414 414 414 407 407 408 412 416 415]\n",
            "Example prediction:\n",
            " 22 [380 382 379 387 392 392 400 402 401 400 401 402]\n",
            "Building Eagle_office_Sheree\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_101\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_202 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_203 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_101 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_101 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [165 214 171 103  35  49  79  81 125 157 170 169]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3029.5556\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1268.2235\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1117.9184\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1053.2830\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1013.1075\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1020.0038\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1004.2257\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 987.2325\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 973.2302\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 975.3784\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 948.9108\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 973.5000\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 945.2807\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 935.9933\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 940.6463\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 935.2997\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 938.5348\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 931.6399\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 908.5981\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 904.8024\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 936.8274\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 909.4713\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 908.2998\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 924.3813\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 891.6949\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 898.0900\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 892.4135\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 905.8427\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 888.5664\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 894.8123\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 893.3546\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 887.3796\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 883.6919\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 894.6068\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 885.4787\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 864.3421\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 879.7800\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 882.1152\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 865.1255\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 845.0768\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 855.0672\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 865.3874\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 871.7943\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 853.7882\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 839.4015\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 849.3587\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 845.6894\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 861.8740\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 842.9506\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 846.8490\n",
            "i,mean,rmse,rmse/mean,bldg: 23 62.01927753077944 53.51112935089088 0.8628144583647227 Eagle_office_Sheree\n",
            "Example prediction:\n",
            " 0 [88 91 92 91 88 90 89 80 57 30 10  9]\n",
            "Example prediction:\n",
            " 2 [82 78 70 58 57 62 63 44 27 19 26 41]\n",
            "Example prediction:\n",
            " 4 [62 57 51 41 35 32 31 31 38 44 55 62]\n",
            "Example prediction:\n",
            " 6 [63 62 60 52 41 37 32 37 46 59 72 78]\n",
            "Example prediction:\n",
            " 8 [ 88  93  87  69  49  42  41  49  70  89 104 102]\n",
            "Example prediction:\n",
            " 10 [ 95  78  59  38  30  45  70 101 123 135 129 120]\n",
            "Example prediction:\n",
            " 12 [ 64  40  37  49  82 109 132 144 142 135 132 115]\n",
            "Example prediction:\n",
            " 14 [ 52  59  80 115 146 162 166 151 133 110 104  93]\n",
            "Example prediction:\n",
            " 16 [113 125 131 144 156 142 130 129 126 123 114 105]\n",
            "Example prediction:\n",
            " 18 [147 140 130 123 114 108 115 124 129 131 129 130]\n",
            "Example prediction:\n",
            " 20 [122 115 112 105 109 118 120 108 107 107 114 110]\n",
            "Example prediction:\n",
            " 22 [112 112 110 107 112 114 112 105 100  98  84  58]\n",
            "Building Eagle_education_Sherrill\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_102\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_204 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_205 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_102 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_102 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4351 4417 4462 4503 4501 4299 4100 3898 3908 3882 3982 3918]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4481942.6036\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 620905.0505\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 427711.8014\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 353312.9630\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 311568.6346\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 291232.1101\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 284173.4843\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 270810.7043\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 264859.3494\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 261637.1751\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 255689.2915\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 253448.4464\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 257572.5453\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 248803.2151\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 254768.3825\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 248076.9851\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 249334.1230\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 245689.8124\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 235441.5594\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 237760.0385\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 239193.1920\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 235236.2953\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 238088.5266\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 233563.1877\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 231804.4853\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 230541.8289\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 228917.9196\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 222802.0051\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 225231.5231\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 225564.3511\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 219449.2328\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 229825.0378\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 216898.6060\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 222530.9346\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205909.6216\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 211971.2656\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 210830.7949\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 218397.4199\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 217920.4449\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 216875.7678\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204740.8101\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207467.1072\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207700.5778\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207028.3588\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 199159.3210\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205918.8947\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205788.2630\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 197215.5427\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 203520.2891\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207869.7682\n",
            "i,mean,rmse,rmse/mean,bldg: 24 2032.6741548164603 533.7749014258014 0.2625973770370483 Eagle_education_Sherrill\n",
            "Example prediction:\n",
            " 0 [1995 2157 2202 2271 2291 2215 2081 1818 1646 1466 1427 1485]\n",
            "Example prediction:\n",
            " 2 [1765 1903 1905 1852 1734 1523 1308 1079  949  960  966 1097]\n",
            "Example prediction:\n",
            " 4 [1721 1724 1650 1504 1341 1164  977  923  860  951  998 1183]\n",
            "Example prediction:\n",
            " 6 [1433 1404 1347 1267 1180 1125 1084 1122 1172 1287 1369 1489]\n",
            "Example prediction:\n",
            " 8 [1867 1715 1697 1651 1605 1671 1739 1977 2102 2362 2487 2542]\n",
            "Example prediction:\n",
            " 10 [2346 2209 2242 2270 2312 2540 2751 2985 3115 3220 3240 3227]\n",
            "Example prediction:\n",
            " 12 [2522 2537 2619 2784 2974 3153 3295 3277 3281 3204 3241 3133]\n",
            "Example prediction:\n",
            " 14 [2788 2888 2976 3048 3127 3071 2980 2831 2736 2682 2731 2713]\n",
            "Example prediction:\n",
            " 16 [3065 3126 3072 2978 2795 2678 2522 2404 2334 2321 2320 2328]\n",
            "Example prediction:\n",
            " 18 [2944 2855 2662 2490 2325 2256 2194 2157 2107 2146 2178 2215]\n",
            "Example prediction:\n",
            " 20 [2673 2516 2334 2184 2133 2105 2126 2180 2216 2325 2418 2493]\n",
            "Example prediction:\n",
            " 22 [2368 2280 2201 2165 2196 2272 2353 2464 2502 2579 2658 2725]\n",
            "Building Eagle_education_Brooke\n",
            " Count bad values before pseudofill: 56\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_103\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_206 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_207 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_103 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_103 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [3417 3599 4324 4893 5456 4802 4206 3638 4293 4751 4707 4611]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3890719.3491\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 693204.6250\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 533319.7823\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 487513.2178\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 466634.0324\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 462040.7585\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 461085.6706\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 451231.0286\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 444831.4770\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 449774.5557\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 446165.1189\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 443400.8036\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 445764.0769\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442887.4277\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 440742.9107\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 439803.7015\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 445097.6930\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442959.6856\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 436120.5076\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 439999.4530\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442489.2241\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 437082.5528\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 438081.5349\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 426686.7653\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 434749.3472\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442842.6200\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 440360.0191\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 435000.0727\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 425699.4375\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 432370.9936\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 430529.3836\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 431518.8777\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 440349.2185\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 424793.7369\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 430596.7505\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 430339.9822\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 423376.6626\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 427346.2634\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 429218.7778\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 426182.1356\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 426476.3991\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 433810.6320\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 428232.1342\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 425533.7972\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 409747.7708\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 430143.5167\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 420260.5348\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 414928.5842\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 418122.4323\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 424866.0812\n",
            "i,mean,rmse,rmse/mean,bldg: 25 1638.6896927614282 1152.7077374546545 0.7034325916288495 Eagle_education_Brooke\n",
            "Example prediction:\n",
            " 0 [2316 2347 2365 2316 2301 2246 2155 2077 2061 1999 2052 2081]\n",
            "Example prediction:\n",
            " 2 [2155 2152 2153 2071 2051 2007 1940 1897 1901 1864 1926 1955]\n",
            "Example prediction:\n",
            " 4 [1932 1913 1899 1832 1835 1816 1788 1764 1795 1785 1849 1881]\n",
            "Example prediction:\n",
            " 6 [1818 1777 1764 1703 1729 1729 1748 1738 1815 1857 1909 1933]\n",
            "Example prediction:\n",
            " 8 [2055 1972 1935 1888 1925 1962 2020 2038 2111 2176 2204 2208]\n",
            "Example prediction:\n",
            " 10 [2501 2399 2391 2393 2433 2452 2512 2504 2537 2557 2556 2565]\n",
            "Example prediction:\n",
            " 12 [2759 2751 2744 2727 2758 2748 2747 2703 2732 2755 2778 2817]\n",
            "Example prediction:\n",
            " 14 [3117 3093 3002 2945 2948 2864 2874 2866 2862 2845 2922 2977]\n",
            "Example prediction:\n",
            " 16 [3082 3049 2985 2906 2851 2832 2804 2756 2792 2761 2841 2815]\n",
            "Example prediction:\n",
            " 18 [2781 2768 2761 2709 2726 2680 2659 2613 2651 2596 2609 2540]\n",
            "Example prediction:\n",
            " 20 [2560 2563 2530 2481 2496 2473 2464 2452 2488 2412 2469 2474]\n",
            "Example prediction:\n",
            " 22 [2224 2237 2237 2190 2218 2212 2236 2238 2300 2299 2408 2460]\n",
            "Building Eagle_education_Alberto\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_104\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_208 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_209 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_104 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_104 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1231 1231 1231 1234 1234 1234 1233 1234 1238 1238 1230 1225]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 456747.5976\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 64622.8662\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29682.0630\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25101.8624\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23902.1989\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24264.2289\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23206.4334\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22582.9969\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23256.7272\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20178.7935\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21890.9034\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20649.6075\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22307.5951\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19464.0526\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21333.2867\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21456.9757\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20335.1915\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19655.3859\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19353.3124\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19176.7127\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19452.9443\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19411.0231\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19365.2855\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18481.1904\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20165.3457\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 19391.0614\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18708.8718\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18212.9883\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 20007.7234\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18661.8365\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18500.7404\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18490.7673\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18345.7688\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17763.8694\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17341.9583\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17665.3820\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17944.1355\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17859.4166\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17539.2176\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18656.7963\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16748.5785\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16873.9169\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17246.1807\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18018.0858\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17328.3937\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15267.2249\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17209.4310\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16054.4191\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16598.9238\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 16671.1396\n",
            "i,mean,rmse,rmse/mean,bldg: 26 694.9820078317341 168.25100071734096 0.24209403814965685 Eagle_education_Alberto\n",
            "Example prediction:\n",
            " 0 [ 981  987  993 1006 1026 1049 1068 1092 1097 1094 1088 1096]\n",
            "Example prediction:\n",
            " 2 [1012 1027 1045 1056 1067 1085 1099 1114 1112 1109 1107 1107]\n",
            "Example prediction:\n",
            " 4 [1057 1068 1080 1091 1099 1113 1122 1133 1133 1129 1125 1120]\n",
            "Example prediction:\n",
            " 6 [ 997 1019 1035 1054 1052 1072 1072 1072 1068 1060 1052 1042]\n",
            "Example prediction:\n",
            " 8 [1001 1018 1032 1036 1035 1035 1035 1033 1030 1020 1015 1009]\n",
            "Example prediction:\n",
            " 10 [1158 1165 1156 1153 1159 1151 1160 1159 1176 1161 1158 1153]\n",
            "Example prediction:\n",
            " 12 [1106 1105 1123 1118 1123 1132 1146 1152 1135 1127 1136 1123]\n",
            "Example prediction:\n",
            " 14 [1027 1040 1061 1085 1069 1078 1062 1056 1037 1027 1020 1007]\n",
            "Example prediction:\n",
            " 16 [1005 1036 1057 1038 1022 1004  987  972  953  939  939  948]\n",
            "Example prediction:\n",
            " 18 [1020 1017  993  979  943  915  902  907  918  922  935  951]\n",
            "Example prediction:\n",
            " 20 [1001  969  924  896  866  866  880  889  910  959  978  990]\n",
            "Example prediction:\n",
            " 22 [856 834 812 808 808 829 845 870 885 909 917 928]\n",
            "Building Eagle_food_Kay\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_105\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_210 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_211 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_105 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_105 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [769 710 591 484 385 366 332 292 254 420 593 804]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 89259.4702\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 21842.0757\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 17034.1254\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 15449.1308\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 14334.4685\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13522.4159\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 13092.4395\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12658.9845\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12419.8958\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12325.3228\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 12048.3671\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11737.5518\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11498.1538\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11814.5958\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11440.8318\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11617.3048\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11698.5111\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11418.6535\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11619.1905\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11430.9690\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11252.6645\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11443.0358\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11269.4452\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11131.5832\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11106.8757\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11260.0096\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11100.4233\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11164.9994\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11066.7049\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11106.7522\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11161.8317\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11021.2406\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10951.5403\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10997.5838\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10581.2493\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10939.4843\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10774.5693\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10909.4518\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11037.4910\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10887.8292\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10996.4710\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10925.7308\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10878.1165\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10884.7912\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10665.6567\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10677.2474\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10536.1246\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10594.9705\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10671.3728\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 10376.1413\n",
            "i,mean,rmse,rmse/mean,bldg: 27 276.26271949384346 170.34862762276984 0.6166182246192146 Eagle_food_Kay\n",
            "Example prediction:\n",
            " 0 [520 513 487 472 484 494 460 379 284 180 109  71]\n",
            "Example prediction:\n",
            " 2 [382 434 484 496 458 376 276 173  81  29  28  81]\n",
            "Example prediction:\n",
            " 4 [522 527 477 386 278 173 100  50  29  48 100 189]\n",
            "Example prediction:\n",
            " 6 [473 410 299 191 103  48  33  40  86 174 267 342]\n",
            "Example prediction:\n",
            " 8 [254 190 125  83  60  47  83 145 251 390 503 565]\n",
            "Example prediction:\n",
            " 10 [244 166 114  93  99 170 281 442 567 646 690 675]\n",
            "Example prediction:\n",
            " 12 [224 162 130 194 292 414 558 654 717 724 727 699]\n",
            "Example prediction:\n",
            " 14 [122 170 295 443 594 664 724 712 662 586 523 494]\n",
            "Example prediction:\n",
            " 16 [354 421 520 581 645 637 625 550 489 411 369 373]\n",
            "Example prediction:\n",
            " 18 [525 560 583 580 566 511 453 393 342 316 330 369]\n",
            "Example prediction:\n",
            " 20 [602 576 549 499 449 401 379 366 357 358 363 355]\n",
            "Example prediction:\n",
            " 22 [621 574 513 443 400 380 374 351 322 282 245 210]\n",
            "Building Eagle_health_Jodi\n",
            " Count bad values before pseudofill: 41\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_106\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_212 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_213 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_106 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_106 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [355 365 365 365 365 365 365 365 365 365 365 353]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23221.3166\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3095.8823\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2167.4377\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1936.4949\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1838.3503\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1775.2115\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1704.3002\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1640.3322\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1559.8337\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1563.9087\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1528.9978\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1530.2111\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1522.6538\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1500.7155\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1447.1582\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1503.8086\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1531.2127\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1461.4227\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1451.0321\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1424.1286\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1435.4956\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1400.4197\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1497.6274\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1436.7752\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1408.5096\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1461.1713\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1415.6307\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1393.8305\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1370.9875\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1413.0179\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1389.7283\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1396.3482\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1361.2854\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1387.8527\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1361.1492\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1360.6331\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1378.5410\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1404.0621\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1383.2272\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1348.4945\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1436.5007\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1340.5733\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1326.2601\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1398.0670\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1374.3766\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1377.8454\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1345.3215\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1360.0961\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1343.5542\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1349.0290\n",
            "i,mean,rmse,rmse/mean,bldg: 28 192.41198593089595 42.31850151409412 0.2199369301728046 Eagle_health_Jodi\n",
            "Example prediction:\n",
            " 0 [ 97 110 122 136 147 152 142 128 112  98  87  82]\n",
            "Example prediction:\n",
            " 2 [ 96 113 129 136 136 121 101  88  75  68  62  60]\n",
            "Example prediction:\n",
            " 4 [ 97 105 112 106  97  83  69  64  62  67  67  67]\n",
            "Example prediction:\n",
            " 6 [69 70 68 67 62 61 58 63 70 80 86 88]\n",
            "Example prediction:\n",
            " 8 [ 83  74  68  64  64  71  80  95 112 127 140 149]\n",
            "Example prediction:\n",
            " 10 [118 106  97  96  96 112 131 152 174 187 196 207]\n",
            "Example prediction:\n",
            " 12 [140 121 119 120 126 152 169 180 183 183 195 201]\n",
            "Example prediction:\n",
            " 14 [178 169 161 162 173 179 186 184 180 178 179 191]\n",
            "Example prediction:\n",
            " 16 [237 223 208 193 187 180 172 165 167 167 173 180]\n",
            "Example prediction:\n",
            " 18 [212 193 181 166 161 157 153 153 156 163 173 186]\n",
            "Example prediction:\n",
            " 20 [168 156 149 142 147 148 151 162 171 185 199 212]\n",
            "Example prediction:\n",
            " 22 [149 148 143 146 155 160 171 180 191 202 208 216]\n",
            "Building Eagle_education_Norah\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_107\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_214 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_215 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_107 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_107 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1834 1841 2051 2083 2227 2267 2373 2379 2382 2320 2133 2034]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 459744.0953\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 68377.8403\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 54861.0893\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 48315.6686\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 43890.8768\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42615.1950\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 42172.4692\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 40925.1446\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 39857.0209\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 39446.2570\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38334.2439\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 38972.7679\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36960.8988\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36895.6253\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37285.0475\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 37574.8187\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 36401.6386\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35418.3619\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35596.6115\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35933.5910\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34320.6910\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 35228.9267\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33869.8559\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34728.6792\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34025.2680\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33261.7584\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34269.6893\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33955.6640\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33827.1607\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33175.4741\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31164.2649\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31742.8784\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32130.8406\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 33869.2568\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32883.3110\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31345.8128\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32807.5797\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32226.6335\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31902.9866\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31949.2550\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 30941.6019\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31001.1212\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31868.0497\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 32291.5952\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31262.1657\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29971.1659\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31361.2410\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 29780.5088\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 30973.4098\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 31716.1409\n",
            "i,mean,rmse,rmse/mean,bldg: 29 712.0697702804412 263.73041017502317 0.3703715860191 Eagle_education_Norah\n",
            "Example prediction:\n",
            " 0 [620 635 662 686 705 685 643 580 490 436 402 389]\n",
            "Example prediction:\n",
            " 2 [591 614 621 613 573 496 402 329 242 198 171 174]\n",
            "Example prediction:\n",
            " 4 [559 561 549 493 435 351 276 230 182 182 175 177]\n",
            "Example prediction:\n",
            " 6 [340 336 325 294 272 245 232 234 233 263 271 288]\n",
            "Example prediction:\n",
            " 8 [357 349 329 327 337 355 398 462 527 579 616 651]\n",
            "Example prediction:\n",
            " 10 [550 543 547 560 587 658 734 807 904 941 986 996]\n",
            "Example prediction:\n",
            " 12 [ 651  671  681  734  789  852  937  981 1025 1052 1073 1052]\n",
            "Example prediction:\n",
            " 14 [842 842 836 868 888 889 890 902 888 891 909 925]\n",
            "Example prediction:\n",
            " 16 [915 905 890 868 835 801 758 738 723 742 743 744]\n",
            "Example prediction:\n",
            " 18 [843 809 778 714 673 652 649 646 645 666 676 679]\n",
            "Example prediction:\n",
            " 20 [745 704 678 652 637 644 647 683 679 720 749 760]\n",
            "Example prediction:\n",
            " 22 [647 633 630 647 663 692 719 751 766 805 837 856]\n",
            "Building Eagle_education_Will\n",
            " Count bad values before pseudofill: 15\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_108\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_216 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_217 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_108 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_108 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [440 418 442 391 383 361 362 370 360 362 363 373]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 51219.0926\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5713.4059\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4108.9361\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3801.6735\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3583.3028\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3744.6758\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3560.0580\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3687.8356\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3543.5335\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3320.0650\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3248.4326\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3565.6255\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3362.4205\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3280.7223\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2900.5068\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3251.5035\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3176.2576\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3134.9576\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3299.5926\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2891.0413\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2738.0122\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2874.2395\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3453.5781\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2953.5602\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2794.5669\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2934.2921\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3011.4365\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3144.4212\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3165.9360\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2808.4148\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2967.8006\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3219.1100\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2866.3256\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2840.5491\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2865.7268\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3088.5546\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2764.5149\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2994.3397\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3318.9400\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2768.4681\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2834.5456\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3144.4897\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2718.7536\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3057.0133\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2832.7229\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2879.6418\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2576.3156\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3023.6387\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2917.1921\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2850.7016\n",
            "i,mean,rmse,rmse/mean,bldg: 30 226.31931160799405 44.15179289332085 0.19508628132360104 Eagle_education_Will\n",
            "Example prediction:\n",
            " 0 [ 98 115 136 142 146 141 132 128 118 114 118 120]\n",
            "Example prediction:\n",
            " 2 [ 80  97 109 115 111  99  90  93  90  92  97 104]\n",
            "Example prediction:\n",
            " 4 [40 52 62 65 60 58 56 65 67 71 78 87]\n",
            "Example prediction:\n",
            " 6 [35 39 48 47 48 50 51 63 69 75 80 85]\n",
            "Example prediction:\n",
            " 8 [ 94  87  86  81  81  81  89 102 113 121 126 128]\n",
            "Example prediction:\n",
            " 10 [144 129 128 123 124 133 156 173 182 185 193 189]\n",
            "Example prediction:\n",
            " 12 [170 153 162 155 167 179 185 189 199 197 196 187]\n",
            "Example prediction:\n",
            " 14 [203 199 199 201 213 212 215 198 196 186 190 192]\n",
            "Example prediction:\n",
            " 16 [238 243 246 236 223 217 215 204 194 200 201 205]\n",
            "Example prediction:\n",
            " 18 [231 233 225 216 208 201 192 193 206 207 216 218]\n",
            "Example prediction:\n",
            " 20 [205 201 196 194 189 190 196 205 208 214 220 230]\n",
            "Example prediction:\n",
            " 22 [183 184 193 191 194 200 210 213 219 222 229 234]\n",
            "Building Eagle_lodging_Blake\n",
            " Count bad values before pseudofill: 8\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_109\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_218 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_219 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_109 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_109 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [  8   8   8   8   8   8   8   8 296 296 297   6]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 27227.1410\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24895.5053\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25733.4119\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25558.0768\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25411.9721\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 26622.4337\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24084.8768\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24064.6013\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24779.0140\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24401.3368\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24334.8973\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24145.9700\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24954.4421\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25099.7728\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24359.2425\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25177.9453\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24694.3462\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24894.9887\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25609.1576\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23970.6967\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23964.1230\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24939.7324\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24708.2002\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24714.9032\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24571.6170\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25007.4518\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24519.4804\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25576.1775\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24051.6946\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23807.2247\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24420.7479\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25506.1689\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25052.4873\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25540.3033\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23831.2804\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23481.8801\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24592.4258\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24810.2057\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24878.7613\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24130.6249\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23640.5962\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24118.7039\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24912.1943\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23250.5394\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24197.3105\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23525.0906\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 23867.3226\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 25003.3004\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24135.3375\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 24974.7876\n",
            "i,mean,rmse,rmse/mean,bldg: 31 43.440914326264924 67.55004493178137 1.5549867211459627 Eagle_lodging_Blake\n",
            "Example prediction:\n",
            " 0 [27 34 38 45 51 52 58 58 66 70 67 50]\n",
            "Example prediction:\n",
            " 2 [48 51 45 47 46 45 49 50 54 51 48 34]\n",
            "Example prediction:\n",
            " 4 [65 72 67 63 55 48 51 53 61 61 60 54]\n",
            "Example prediction:\n",
            " 6 [72 80 74 68 57 54 58 55 64 66 70 66]\n",
            "Example prediction:\n",
            " 8 [74 81 76 75 64 58 62 61 73 78 86 87]\n",
            "Example prediction:\n",
            " 10 [ 85  88  90  81  55  56  66  87  99 108 116 112]\n",
            "Example prediction:\n",
            " 12 [115 123 124  97  73  83  85  92  98 100 108 111]\n",
            "Example prediction:\n",
            " 14 [120 105  87  96  95  77  53  34  51  81 126 144]\n",
            "Example prediction:\n",
            " 16 [ 89 109 105  83  56  44  69  98 135 154 163 164]\n",
            "Example prediction:\n",
            " 18 [ 63  53  38  30  31  64 120 156 155 167 157 142]\n",
            "Example prediction:\n",
            " 20 [-11   3  30  68 118 138 173 180 184 182 170 151]\n",
            "Example prediction:\n",
            " 22 [ 51  73 102 124 150 156 173 180 189 182 174 160]\n",
            "Building Eagle_education_Petra\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_110\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_220 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_221 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_110 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_110 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [166 175 180 184 182 180 184 190 193 193 192 190]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2395.9392\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 320.2374\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 293.4279\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 286.2993\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 265.5386\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 259.3099\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 249.0039\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 244.7087\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 245.8197\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 237.0901\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 230.5850\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 228.3392\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 230.7799\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 216.3639\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 223.7029\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 218.9017\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 218.8287\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 218.5132\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 217.0530\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 210.4663\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 212.1733\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207.8399\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 208.1854\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205.9069\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 215.5912\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.0831\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207.4213\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207.6911\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205.7370\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 203.7166\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 213.7557\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 207.0135\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 200.1037\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 202.9178\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.4900\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 204.3774\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 205.8075\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 202.4681\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 203.6330\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 200.7267\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 200.8288\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 201.1155\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 199.4915\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 191.7106\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 201.5744\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 197.3363\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 198.1549\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 200.0377\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 195.9964\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 196.0039\n",
            "i,mean,rmse,rmse/mean,bldg: 32 57.05018764819895 33.81778775729138 0.5927725946464786 Eagle_education_Petra\n",
            "Example prediction:\n",
            " 0 [57 61 65 67 68 65 60 56 55 53 52 55]\n",
            "Example prediction:\n",
            " 2 [50 54 53 52 52 48 44 42 47 46 51 54]\n",
            "Example prediction:\n",
            " 4 [47 49 44 41 42 39 40 43 49 49 53 54]\n",
            "Example prediction:\n",
            " 6 [47 49 45 43 45 45 48 51 55 54 57 58]\n",
            "Example prediction:\n",
            " 8 [60 60 60 60 64 63 65 66 69 70 71 74]\n",
            "Example prediction:\n",
            " 10 [73 73 78 80 82 79 78 80 81 84 80 77]\n",
            "Example prediction:\n",
            " 12 [85 90 92 94 94 94 93 91 91 88 86 87]\n",
            "Example prediction:\n",
            " 14 [ 99 102 102 103 104 103 100  94  91  92  91  98]\n",
            "Example prediction:\n",
            " 16 [115 116 115 112 112 104 103  99 100 105 105 111]\n",
            "Example prediction:\n",
            " 18 [111 108 103 102  98  98  97 101 108 114 119 120]\n",
            "Example prediction:\n",
            " 20 [ 99 100  98 100 104 105 106 111 112 114 110 114]\n",
            "Example prediction:\n",
            " 22 [85 88 91 92 98 97 94 94 92 91 93 97]\n",
            "Building Eagle_lodging_Trina\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_111\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_222 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_223 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_111 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_111 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [316 317 318 331 326 327 320 325 335 365 381 365]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7984.0176\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2398.7879\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2273.2595\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2180.3737\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2201.0725\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2256.8268\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2210.9001\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2162.5196\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2205.1327\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2188.0378\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2116.5967\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2145.7932\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2127.7319\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2132.8254\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2140.5346\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2096.1022\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2081.6854\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2124.0505\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2056.3735\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2165.6090\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2056.3005\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2073.7913\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2007.6394\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2058.8315\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2034.0722\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2049.2635\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2004.9606\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2072.7772\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2017.6053\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2054.0287\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2026.8447\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2029.6945\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1978.1003\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1937.3034\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2080.0759\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1943.1696\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2057.7319\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2006.1021\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1989.4790\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1999.9251\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1972.7305\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1978.5844\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1988.6436\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2004.6486\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2029.1255\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1986.0701\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1994.0113\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1968.1509\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1995.1240\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2050.3040\n",
            "i,mean,rmse,rmse/mean,bldg: 33 91.28035335157311 74.88068481660613 0.8203373679788198 Eagle_lodging_Trina\n",
            "Example prediction:\n",
            " 0 [62 59 61 65 66 73 67 64 66 65 63 66]\n",
            "Example prediction:\n",
            " 2 [58 59 63 64 63 61 54 53 59 61 60 64]\n",
            "Example prediction:\n",
            " 4 [59 57 59 58 55 55 52 54 61 63 61 60]\n",
            "Example prediction:\n",
            " 6 [63 61 65 67 65 64 66 73 81 84 85 84]\n",
            "Example prediction:\n",
            " 8 [ 96  95 100 100 100 104 110 115 120 124 124 121]\n",
            "Example prediction:\n",
            " 10 [122 117 117 124 130 140 142 148 148 153 143 134]\n",
            "Example prediction:\n",
            " 12 [126 128 136 146 150 154 154 158 162 148 125 131]\n",
            "Example prediction:\n",
            " 14 [151 162 177 176 171 171 185 173 155 144 151 157]\n",
            "Example prediction:\n",
            " 16 [215 208 211 207 204 200 172 171 171 181 186 188]\n",
            "Example prediction:\n",
            " 18 [196 189 189 168 163 150 147 158 163 169 171 169]\n",
            "Example prediction:\n",
            " 20 [165 153 145 137 136 144 161 170 164 159 167 181]\n",
            "Example prediction:\n",
            " 22 [133 126 124 126 133 143 147 147 144 150 155 158]\n",
            "Building Eagle_health_Reuben\n",
            "Building Eagle_education_Teresa\n",
            " Count bad values before pseudofill: 35\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_112\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_224 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_225 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_112 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_112 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [319 328 253 169 146 189 234 284 352 432 447 463]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 18994.0004\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2544.0384\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2074.6266\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1770.2541\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1584.6067\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1477.6660\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1429.5085\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1344.4647\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1310.6292\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1282.8066\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1291.3901\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1295.6170\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1273.8410\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1251.6675\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1272.8268\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1216.3394\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1208.6615\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1210.6535\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1196.2987\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1216.4002\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1172.5407\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1164.8423\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1182.8056\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1158.4366\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1136.9480\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1145.3804\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1162.6418\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1151.7682\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1095.4987\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1157.2231\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1125.4911\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1112.6030\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1082.5448\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1104.4990\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1062.5330\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1087.9859\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1084.3941\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1086.1060\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1047.8856\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1089.9825\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1068.7454\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1040.1562\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1052.0454\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1063.7444\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1095.5020\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1077.0031\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1095.4247\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1080.6559\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1081.5742\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 1006.0276\n",
            "i,mean,rmse,rmse/mean,bldg: 34 148.73671883114207 48.08738121885654 0.3233053787709894 Eagle_education_Teresa\n",
            "Example prediction:\n",
            " 0 [140 152 168 175 182 183 175 136  71  21  16  54]\n",
            "Example prediction:\n",
            " 2 [128 158 183 186 175 131  75  23  -4  13  65 125]\n",
            "Example prediction:\n",
            " 4 [147 173 176 142  85  36  13  19  33  55  83 124]\n",
            "Example prediction:\n",
            " 6 [137 136 114  76  43  28  33  55  74  96 109 130]\n",
            "Example prediction:\n",
            " 8 [115  93  84  75  65  57  64  92 135 162 166 164]\n",
            "Example prediction:\n",
            " 10 [149 102  71  56  56  87 125 172 198 202 191 193]\n",
            "Example prediction:\n",
            " 12 [ 97  53  53  83 127 172 196 206 208 205 183 168]\n",
            "Example prediction:\n",
            " 14 [ 74  63  99 150 180 192 180 180 176 164 145 147]\n",
            "Example prediction:\n",
            " 16 [142 142 160 162 171 164 156 148 154 153 148 155]\n",
            "Example prediction:\n",
            " 18 [170 150 144 139 136 134 126 137 153 159 172 177]\n",
            "Example prediction:\n",
            " 20 [162 151 148 142 135 142 147 162 177 190 189 175]\n",
            "Example prediction:\n",
            " 22 [164 164 168 170 173 175 178 190 196 184 153 131]\n",
            "Building Eagle_office_Norbert\n",
            " Count bad values before pseudofill: 52\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_113\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_226 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_227 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_113 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_113 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [643 656 671 690 692 704 696 696 695 699 709 696]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 107122.5358\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 11627.9415\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6174.7317\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5361.6482\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4751.7106\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4801.0988\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4455.0900\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4222.4815\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4309.4010\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4120.8011\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4119.7237\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4114.3591\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4064.1359\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3845.9814\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4058.9604\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4233.7867\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3957.7519\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3920.0859\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4074.7645\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3842.5420\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3839.4013\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3925.4243\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3936.8760\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3796.8794\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4016.2886\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3650.3115\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3736.6818\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3531.0160\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3642.1785\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3984.1456\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3753.0693\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3777.8088\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3702.9030\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3493.2330\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3621.9024\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3764.6482\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3591.0726\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3548.6944\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3377.4116\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3680.1547\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3593.5125\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3536.5388\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3610.8780\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3264.7135\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3533.9259\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3477.0395\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3477.0702\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3572.0785\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3319.9602\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3389.5950\n",
            "i,mean,rmse,rmse/mean,bldg: 35 390.7765009878811 73.45788571491973 0.18797928107042913 Eagle_office_Norbert\n",
            "Example prediction:\n",
            " 0 [407 422 427 427 423 413 395 383 360 353 349 361]\n",
            "Example prediction:\n",
            " 2 [392 401 395 383 366 342 327 310 292 287 292 303]\n",
            "Example prediction:\n",
            " 4 [370 372 352 331 318 301 290 278 275 280 289 294]\n",
            "Example prediction:\n",
            " 6 [283 283 276 270 265 260 260 265 269 283 288 295]\n",
            "Example prediction:\n",
            " 8 [273 269 259 276 284 290 303 324 333 356 360 365]\n",
            "Example prediction:\n",
            " 10 [346 343 338 349 362 385 422 440 453 472 470 470]\n",
            "Example prediction:\n",
            " 12 [382 380 389 401 425 455 473 486 489 493 491 486]\n",
            "Example prediction:\n",
            " 14 [451 452 463 477 486 498 496 503 491 488 494 490]\n",
            "Example prediction:\n",
            " 16 [524 525 519 517 502 503 483 481 484 472 477 481]\n",
            "Example prediction:\n",
            " 18 [508 507 497 482 477 474 471 461 466 471 472 476]\n",
            "Example prediction:\n",
            " 20 [472 458 453 452 449 451 462 465 468 470 475 470]\n",
            "Example prediction:\n",
            " 22 [428 425 426 432 441 449 456 463 465 465 477 475]\n",
            "Building Eagle_lodging_Casey\n",
            "Building Eagle_office_Tia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_114\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_228 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_229 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_114 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_114 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [417 393 372 281 208 166 153 304 405 493 436 388]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 34479.1153\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 8600.7506\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6869.7289\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 6060.5860\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5664.5392\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5274.8872\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5196.4072\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5223.8242\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4950.7341\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4858.1212\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4778.1234\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4820.3090\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4820.3015\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4539.2667\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4578.7912\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4697.1803\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4653.0556\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4448.9806\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4308.2213\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4534.4943\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4513.9621\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4548.5238\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4446.3467\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4600.1051\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4476.3404\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4213.9829\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4416.1948\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4313.6461\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4232.7845\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4187.7757\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4293.9492\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4242.5984\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4344.1713\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4378.0513\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4177.8444\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4197.0229\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4055.2376\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4021.2858\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4042.6547\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4162.7117\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4186.9292\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4155.8452\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4023.7148\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4147.2089\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4068.3154\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4039.2280\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4137.5702\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4027.6868\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3945.8529\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4081.7934\n",
            "i,mean,rmse,rmse/mean,bldg: 36 174.62244338235027 80.79663795209342 0.4626933192956332 Eagle_office_Tia\n",
            "Example prediction:\n",
            " 0 [197 226 236 233 222 204 188 153  96  29  -9  15]\n",
            "Example prediction:\n",
            " 2 [171 200 212 198 166 121  73  30  -6 -15  26 108]\n",
            "Example prediction:\n",
            " 4 [162 171 158 124  83  43  14   1   0  15  56 119]\n",
            "Example prediction:\n",
            " 6 [162 132  95  64  41  27  11   6  29  83 140 187]\n",
            "Example prediction:\n",
            " 8 [156 104  74  57  34  17   7  43 130 251 323 324]\n",
            "Example prediction:\n",
            " 10 [198 121  59  24  23  67 172 292 376 402 361 283]\n",
            "Example prediction:\n",
            " 12 [195  94  30  59 169 296 388 405 388 350 314 272]\n",
            "Example prediction:\n",
            " 14 [ 86  76 157 280 388 417 392 334 283 242 204 177]\n",
            "Example prediction:\n",
            " 16 [223 262 309 365 358 320 254 201 176 157 183 211]\n",
            "Example prediction:\n",
            " 18 [300 332 315 281 222 177 162 155 168 173 190 208]\n",
            "Example prediction:\n",
            " 20 [281 260 229 202 174 164 171 178 194 211 233 235]\n",
            "Example prediction:\n",
            " 22 [177 181 184 182 174 171 172 193 214 213 185 130]\n",
            "Building Eagle_office_Remedios\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_115\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_230 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_231 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_115 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_115 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [192 211 225 238 238 238 244 240 234 229 232 236]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 9073.5069\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 834.2869\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 624.7235\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 578.2643\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 550.8253\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 510.7461\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 489.9087\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 500.5949\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 497.3424\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 499.0063\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 479.1179\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 500.3504\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 490.1372\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 463.1294\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 472.3193\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 493.8950\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 448.4004\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 470.2742\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 461.1563\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 483.0252\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 429.0792\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 469.5738\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 452.1439\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 461.2762\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442.5801\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 436.4944\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 449.2405\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 427.5016\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 430.2083\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 466.0982\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 468.7527\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 447.7851\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 431.3704\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 439.3567\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 422.8914\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 431.8506\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 419.9508\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 411.6678\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 427.8118\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 414.3267\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 409.6464\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 415.9803\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 406.7416\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 391.8978\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 427.2914\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 434.1557\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 426.6773\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 410.3531\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 404.7271\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 397.2476\n",
            "i,mean,rmse,rmse/mean,bldg: 37 121.45535634240541 27.741824709956838 0.22841170241802622 Eagle_office_Remedios\n",
            "Example prediction:\n",
            " 0 [106 111 117 121 126 130 128 121 114 114 112 111]\n",
            "Example prediction:\n",
            " 2 [106 109 115 116 111 109 103  97  91  99  98 101]\n",
            "Example prediction:\n",
            " 4 [107 106 111 104  95  93  86  86  88  92  91  96]\n",
            "Example prediction:\n",
            " 6 [ 89  90  91  88  85  85  84  86  90  97  98 102]\n",
            "Example prediction:\n",
            " 8 [105 105 102  97  96  97  99 105 114 128 130 131]\n",
            "Example prediction:\n",
            " 10 [118 117 117 113 117 121 130 141 150 155 159 152]\n",
            "Example prediction:\n",
            " 12 [136 135 138 143 153 159 164 165 164 164 163 154]\n",
            "Example prediction:\n",
            " 14 [152 157 160 164 162 162 162 154 154 153 151 150]\n",
            "Example prediction:\n",
            " 16 [164 162 160 158 156 152 146 142 145 147 143 146]\n",
            "Example prediction:\n",
            " 18 [152 149 139 134 135 133 135 138 138 140 143 144]\n",
            "Example prediction:\n",
            " 20 [130 127 123 124 124 130 132 136 136 139 141 146]\n",
            "Example prediction:\n",
            " 22 [119 120 123 125 126 130 129 131 135 137 140 147]\n",
            "Building Eagle_office_Patrice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_116\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_232 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_233 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_116 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_116 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [498 460 394 367 320 292 239 212 212 221 295 459]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 22567.6894\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 5353.9290\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4668.0524\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 4038.9283\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 4168.6878\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3989.8634\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3825.4512\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3777.7316\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3608.8403\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3720.3157\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 3231.7945\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3398.7434\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3203.0190\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3519.8569\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3243.6060\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3345.0327\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3024.5846\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3141.0429\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3268.3239\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3113.0819\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3045.1208\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3280.1565\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2916.7954\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3163.3018\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3092.9100\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3029.6447\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3190.3852\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3018.7352\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3097.5381\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3056.0802\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3254.8397\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2983.0769\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2808.9891\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2888.7857\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3071.7341\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3260.5059\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2860.0198\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3174.3005\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2881.5512\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2953.9381\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2877.6604\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 2968.0674\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2979.3618\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2696.2256\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2831.9183\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2854.6903\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3103.2303\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2876.4021\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 2817.1613\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 3000.1402\n",
            "i,mean,rmse,rmse/mean,bldg: 38 165.8827588691298 76.21600004495183 0.459457031969676 Eagle_office_Patrice\n",
            "Example prediction:\n",
            " 0 [257 278 295 284 266 230 192 152 110  83  75  83]\n",
            "Example prediction:\n",
            " 2 [225 235 240 209 171 129  89  58  43  41  50  66]\n",
            "Example prediction:\n",
            " 4 [170 170 161 125  86  57  34  20  22  32  42  59]\n",
            "Example prediction:\n",
            " 6 [101  92  82  64  44  31  25  28  55  86 109 128]\n",
            "Example prediction:\n",
            " 8 [103  83  66  54  44  49  65 101 160 217 257 270]\n",
            "Example prediction:\n",
            " 10 [125 108  88  89 117 147 205 262 311 343 355 344]\n",
            "Example prediction:\n",
            " 12 [130 129 145 184 235 271 322 343 358 350 344 329]\n",
            "Example prediction:\n",
            " 14 [280 282 299 312 339 359 366 356 342 338 350 352]\n",
            "Example prediction:\n",
            " 16 [381 382 372 357 353 354 347 344 338 351 364 372]\n",
            "Example prediction:\n",
            " 18 [350 337 333 322 319 322 333 327 329 319 315 319]\n",
            "Example prediction:\n",
            " 20 [342 323 326 326 326 319 311 304 291 265 245 230]\n",
            "Example prediction:\n",
            " 22 [338 343 340 330 316 296 272 247 220 202 186 183]\n",
            "Building Eagle_education_Shana\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_117\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_234 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_235 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_117 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_117 (Flatten)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 12)                3084      \n",
            "=================================================================\n",
            "Total params: 5,564\n",
            "Trainable params: 5,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [182 182 182 183 183 183 183 183 183 183 182 181]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 7475.4665\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 743.0385\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 510.5787\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 511.1845\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 491.9067\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 480.8265\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 432.1496\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442.0796\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 457.8935\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 438.7523\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 433.2799\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 457.4466\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 406.1124\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 396.9456\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 419.7385\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 403.9543\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 402.8500\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 431.3218\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 455.2638\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 409.7661\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 419.1395\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 399.1486\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 405.2135\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 442.0115\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 375.6212\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 410.0481\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 406.3861\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 387.7105\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 394.8920\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 399.6308\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 386.2900\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 366.3869\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 397.0907\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 378.2897\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 386.5485\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 378.1282\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 383.1920\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 360.7521\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 391.0671\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 370.3726\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 373.1931\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 384.2934\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 364.7957\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 340.6606\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 362.5529\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 369.0985\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 374.0481\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 395.2287\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 358.7661\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 3ms/step - loss: 368.8687\n",
            "i,mean,rmse,rmse/mean,bldg: 39 103.1846223894125 23.935786204810356 0.23197047826058959 Eagle_education_Shana\n",
            "Example prediction:\n",
            " 0 [130 128 129 131 133 137 139 143 142 145 146 146]\n",
            "Example prediction:\n",
            " 2 [136 136 138 140 140 143 146 147 148 149 154 152]\n",
            "Example prediction:\n",
            " 4 [143 145 145 147 147 149 153 152 153 153 156 155]\n",
            "Example prediction:\n",
            " 6 [135 138 140 141 142 145 147 144 143 140 142 142]\n",
            "Example prediction:\n",
            " 8 [140 141 143 144 141 141 141 141 137 138 140 139]\n",
            "Example prediction:\n",
            " 10 [159 157 159 159 156 156 157 160 160 158 162 160]\n",
            "Example prediction:\n",
            " 12 [154 160 159 159 159 164 164 161 161 153 157 159]\n",
            "Example prediction:\n",
            " 14 [145 147 155 156 158 155 155 152 147 144 143 136]\n",
            "Example prediction:\n",
            " 16 [144 145 146 144 145 141 142 136 128 133 132 135]\n",
            "Example prediction:\n",
            " 18 [148 147 143 138 137 133 131 131 130 133 138 143]\n",
            "Example prediction:\n",
            " 20 [143 142 135 131 128 129 130 133 134 135 143 143]\n",
            "Example prediction:\n",
            " 22 [121 120 118 116 115 116 121 123 126 124 130 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5cv9fQ8jieQ",
        "outputId": "4b690489-a9c6-459b-9994-e2bfa3979adc"
      },
      "source": [
        "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))  \n",
        "overall = overall/cnt\n",
        "print (\"overall = \",overall)  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "History 24 Future 12\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      0.00       0.00   inf   Eagle_office_Henriette\n",
            "      0.11       0.04  0.34   Eagle_education_Wesley\n",
            "     15.76      38.04  2.41   Eagle_education_Jewell\n",
            "     35.89       8.48  0.24   Eagle_office_Mandi\n",
            "     36.93       8.72  0.24   Eagle_office_Lamont\n",
            "     43.44      67.55  1.55   Eagle_lodging_Blake\n",
            "     46.46      21.13  0.45   Eagle_education_Eileen\n",
            "     56.51      75.18  1.33   Eagle_office_Dallas\n",
            "     57.05      33.82  0.59   Eagle_education_Petra\n",
            "     62.02      53.51  0.86   Eagle_office_Sheree\n",
            "     81.97      66.82  0.82   Eagle_lodging_Edgardo\n",
            "     87.21      21.14  0.24   Eagle_office_Phyllis\n",
            "     91.28      74.88  0.82   Eagle_lodging_Trina\n",
            "     92.83      54.94  0.59   Eagle_lodging_Dawn\n",
            "    101.60      65.71  0.65   Eagle_office_Freida\n",
            "    103.18      23.94  0.23   Eagle_education_Shana\n",
            "    121.46      27.74  0.23   Eagle_office_Remedios\n",
            "    122.35      40.28  0.33   Eagle_health_Vincenza\n",
            "    148.74      48.09  0.32   Eagle_education_Teresa\n",
            "    165.88      76.22  0.46   Eagle_office_Patrice\n",
            "    174.62      80.80  0.46   Eagle_office_Tia\n",
            "    182.08      73.50  0.40   Eagle_public_Alvin\n",
            "    192.41      42.32  0.22   Eagle_health_Jodi\n",
            "    226.32      44.15  0.20   Eagle_education_Will\n",
            "    272.96      58.61  0.21   Eagle_office_Nereida\n",
            "    276.26     170.35  0.62   Eagle_food_Kay\n",
            "    336.47     103.27  0.31   Eagle_office_Francis\n",
            "    390.78      73.46  0.19   Eagle_office_Norbert\n",
            "    477.70     175.56  0.37   Eagle_health_Athena\n",
            "    659.66     373.24  0.57   Eagle_health_Gregoria\n",
            "    694.98     168.25  0.24   Eagle_education_Alberto\n",
            "    712.07     263.73  0.37   Eagle_education_Norah\n",
            "   1004.16     707.01  0.70   Eagle_education_Shante\n",
            "   1037.69     181.69  0.18   Eagle_office_Chauncey\n",
            "   1084.29     262.74  0.24   Eagle_health_Reba\n",
            "   1199.41     327.18  0.27   Eagle_education_Roman\n",
            "   1638.69    1152.71  0.70   Eagle_education_Brooke\n",
            "   2032.67     533.77  0.26   Eagle_education_Sherrill\n",
            "   3153.29     766.98  0.24   Eagle_education_Peter\n",
            "overall =  inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}