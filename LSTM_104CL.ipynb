{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LSTM_104CL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# LSTM \n",
        "We previously used linear regression\n",
        "to predict future air temp based on past air temp.\n",
        "Here, use LSTM for the same task.\n",
        "Where LinReg viewed each vector as one point,\n",
        "LSTM will view each vector as a time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "1dcf2d4d-add7-414f-8caa-44f5812ac31e"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "STEAM_FILE='steam.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n",
        "def get_site_timeseries(panda,site):\n",
        "    # Assume the panda dataframe has a datetime column.\n",
        "    # (If not, call fix_date_type() before this.)\n",
        "    # Extract the timeseries for one site.\n",
        "    # Convert the datetime column to a DatetimeIndex.\n",
        "    site_df = panda[panda['site_id']==site]\n",
        "    temp_col = site_df['date']\n",
        "    temp_val = temp_col.values\n",
        "    temp_ndx = pd.DatetimeIndex(temp_val)\n",
        "    dropped = site_df.drop('date',axis=1)\n",
        "    panda = dropped.set_index(temp_ndx)\n",
        "    return panda"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZgkgsP6w9pg"
      },
      "source": [
        "SITE = 'Eagle'\n",
        "METER = 'steam'\n",
        "BLDG = 'Eagle_education_Peter'   # one example\n",
        "PREDICTOR_VARIABLE = 'airTemperature'  # for starters\n",
        "PREDICTED_VARIABLE = 'steam'  # for starters"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YVYM_bqw9pi"
      },
      "source": [
        "wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "wet_df = fix_date_type(wet_df)\n",
        "stm_df = read_zip_to_panda(ZIP_PATH,STEAM_FILE)\n",
        "stm_df = fix_date_type(stm_df)\n",
        "site_specific_weather = wet_df.loc[wet_df['site_id'] == SITE]\n",
        "all_buildings = [x for x in stm_df.columns if x.startswith(SITE)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VynRgLt9w9pk"
      },
      "source": [
        "DOWNSAMPLE = False   # if true, use 1 time per day, else 24 times per day\n",
        "STEPS_HISTORY = 24 \n",
        "STEPS_FUTURE =  1    \n",
        "def smooth(df):\n",
        "    # For smoothing the 24 hour cycle, we do not want exponential smoothing.\n",
        "    smoothed = None\n",
        "    if DOWNSAMPLE:\n",
        "        # This alternate method samples down to 1/24 time steps.\n",
        "        smoothed = df.resample(\"24H\").mean() \n",
        "    else:\n",
        "        # This method does not reduce the number of time steps.\n",
        "        # Note the first 23 measurements get set to Nan.\n",
        "        smoothed=df.rolling(window=24).mean()\n",
        "        smoothed=smoothed[24:]\n",
        "    return smoothed\n",
        "\n",
        "# Correlation is low when buildings have many NaN and 0 meter readings.\n",
        "# We will ignore buildings that have >max bad meter readings.\n",
        "def is_usable_column(df,column_name):\n",
        "    MAX_BAD = 500 \n",
        "    bad = df[column_name].isin([0]).sum()\n",
        "    return bad<=MAX_BAD\n",
        "\n",
        "def prepare_for_learning(df):\n",
        "    # This is very slow. Is there a faster way? See...\n",
        "    # https://stackoverflow.com/questions/27852343/split-python-sequence-time-series-array-into-subsequences-with-overlap\n",
        "    # X = df.drop(METER,axis=1) # this would use all predictors, just drop the predicted\n",
        "    X=[]\n",
        "    y=[]\n",
        "    predictor_series = df[PREDICTOR_VARIABLE].values\n",
        "    predicted_series = df[PREDICTED_VARIABLE].values\n",
        "    for i in range(STEPS_HISTORY,len(df)-STEPS_FUTURE):\n",
        "        one_predictor = [[p] for p in predictor_series[i-STEPS_HISTORY:i]]\n",
        "        one_predicted = [[p] for p in predicted_series[i:i+STEPS_FUTURE]]\n",
        "        X.append(one_predictor)\n",
        "        y.append(one_predicted)\n",
        "    # Return two lists of lists of lists.\n",
        "    # At this point, each data point is a scalar (1D) but RNN expects a vector.\n",
        "    # 1000 samples * 100 time steps * 1D vector.\n",
        "    return X,y \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "TIMESTEP_VECTOR_DIMENSION = 1 # we are univariate so far\n",
        "def make_RNN():\n",
        "    rnn = Sequential([\n",
        "        SimpleRNN(20,return_sequences=True, \n",
        "                  input_shape=(STEPS_HISTORY,TIMESTEP_VECTOR_DIMENSION)), \n",
        "        SimpleRNN(10,return_sequences=False),\n",
        "        Dense(STEPS_FUTURE)  \n",
        "    ])\n",
        "    rnn = Sequential([\n",
        "        LSTM(20,return_sequences=True, \n",
        "                  input_shape=(STEPS_HISTORY,TIMESTEP_VECTOR_DIMENSION)), \n",
        "        LSTM(10,return_sequences=False),\n",
        "        Dense(STEPS_FUTURE)   \n",
        "    ])\n",
        "    rnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    return rnn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "outputId": "89650343-d508-4190-c09a-e9721e919b65"
      },
      "source": [
        "cors = []\n",
        "EPOCHS=50\n",
        "# Test on only Peter just during code development\n",
        "for BLDG in all_buildings:\n",
        "    print(\"Building\",BLDG)\n",
        "    # Get steam usage for one building.\n",
        "    bldg_specific_steam = stm_df[[BLDG]]\n",
        "    # Concatenate steam usage with weather.\n",
        "    one_bldg_df = pd.concat([bldg_specific_steam,site_specific_weather],axis=1)\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    one_bldg_df = one_bldg_df.drop(['site_id'],axis=1)\n",
        "    # The original steam table used column name = building name.\n",
        "    # We are processing one building, so rename to the column 'steam'.\n",
        "    one_bldg_df = one_bldg_df.rename(columns={BLDG : METER})\n",
        "    # In order to filter bad buildings, count sum of NaN + zero.\n",
        "    one_bldg_df = one_bldg_df.fillna(0)\n",
        "    \n",
        "    if is_usable_column(one_bldg_df,METER):\n",
        "        one_bldg_df = smooth(one_bldg_df) \n",
        "        X,y = prepare_for_learning(one_bldg_df)\n",
        "        # Ideally, split Year1 = train, Year2 = test.\n",
        "        # Some data is incomplete, so split 1st half and 2nd half.\n",
        "        split = len(X)//2 \n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        y_test = np.asarray(y[split:])\n",
        "        print(\"Train on\",len(X_train),\"samples...\")\n",
        "        model = make_RNN()\n",
        "        print(model.summary())\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Compare. Solve the problem that predict.shape != truth.shape \n",
        "        ##print(\" before ytestshape\",y_test.shape,\"ypredshape\",y_pred.shape)\n",
        "        nsamples, nsteps, ndim = y_test.shape\n",
        "        y_test = y_test.reshape(nsamples,nsteps*ndim)\n",
        "        #nsamples, nsteps, ndim = y_pred.shape\n",
        "        #y_pred = y_pred.reshape(nsamples,nsteps*ndim)\n",
        "        ##print(\" after ytestshape\",y_test.shape,\"ypredshape\",y_pred.shape)\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        # Keep a table for reporting later.\n",
        "        mean = one_bldg_df[METER].mean()\n",
        "        cor = one_bldg_df.corr().loc[PREDICTED_VARIABLE][PREDICTOR_VARIABLE] \n",
        "        cors.append([cor,mean,rmse,rmse/mean,BLDG])\n",
        "        print(\"cor,mean,rmse,rmse/mean,bldg:\",cor,mean,rmse,rmse/mean,BLDG)\n",
        "\n",
        "        ## break   ## REMOVE THIS LINE TO LOOP OVER BUILDINGS!\n",
        "        \n",
        "if True:\n",
        "    print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
        "    print(\"Column 1: Correlation of\",PREDICTED_VARIABLE,\"and\",PREDICTOR_VARIABLE)\n",
        "    print(\"          Using one weather feature as leading correlate.\")\n",
        "    print(\"Column 2: Mean usage.\")\n",
        "    print(\"          Using mean to help understand the RMSE.\")\n",
        "    print(\"Column 3: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "    print(\"Column 4: RMSE/mean normalized to help understand RMSE.\")\n",
        "    print(\"Column 5: Building.\")\n",
        "    for cor in sorted(cors):\n",
        "        print(\"%7.4f %10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3],cor[4]))    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Eagle_office_Lamont\n",
            "Building Eagle_health_Athena\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 7s 16ms/step - loss: 342525.8408\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 333752.9839\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 328243.0137\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 333557.6094\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 324072.9744\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 324192.5945\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 325955.7873\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 314056.8228\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 314520.0128\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 308403.0542\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 305649.1491\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 301040.4349\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 300352.5698\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 309634.5482\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 296219.5514\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 297601.2520\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 282516.9172\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 288930.7633\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 281239.7272\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 281495.6335\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 273749.7102\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 281252.7643\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 276141.3889\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 270670.8586\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 267401.5077\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 263833.6385\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 261793.2791\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 265454.5164\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 264028.6340\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 255141.1698\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 250696.1763\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 250771.1665\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 242278.4934\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 247764.5694\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 234151.4822\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 241782.1249\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 233313.1276\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 235014.8034\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 236854.4077\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 226111.1712\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 227538.4444\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 225550.1457\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 222671.3527\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 217083.3621\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 219831.5578\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 214011.4703\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 209738.8063\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 213540.7322\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 210165.5824\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 202616.8520\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.849214848008229 477.4110643545472 352.24684936620224 0.7378271591640526 Eagle_health_Athena\n",
            "Building Eagle_assembly_Herbert\n",
            "Building Eagle_public_Alvin\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 7s 16ms/step - loss: 51540.0273\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 49907.6516\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 48353.9580\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 47531.1371\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 46294.5668\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 45524.6082\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 43218.3149\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 43682.5761\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 41946.4707\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 41403.5982\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 40570.6135\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 38363.1661\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 37732.2363\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 37305.7674\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 35300.9876\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 34995.4242\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 33493.6143\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 32927.3677\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 31843.6119\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 31305.4219\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 30890.8114\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 30230.8545\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 29577.2244\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 28888.2141\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 28275.4680\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 27065.5879\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 26038.7283\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 25378.4677\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 24394.4920\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 25223.4496\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 23140.8020\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 22792.8030\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 21549.4141\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 21246.9307\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 20774.4004\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 20634.5205\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 19734.1582\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 19014.3654\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 18833.0936\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 18136.6034\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 17590.7332\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 17761.6034\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 16348.4854\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 16313.8916\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 15873.2657\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 16125.7208\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 15243.8797\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 14171.8725\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 14454.4033\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 13827.1024\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.755524810622147 181.93965687500068 56.00845082921678 0.3078408071732084 Eagle_public_Alvin\n",
            "Building Eagle_education_Raul\n",
            "Building Eagle_education_Roman\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 16ms/step - loss: 1636101.0927\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1633549.4577\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1654817.8986\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1636742.0309\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1621519.8400\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1615467.8773\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1599266.5850\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1606984.6077\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1592516.4414\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1596392.9745\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1587789.0555\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1561840.5709\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1543551.4727\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1541735.3368\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1559065.4041\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1547947.6286\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1560450.3400\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1517043.7132\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1502134.0964\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1517940.1209\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1506185.9914\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1525092.8936\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1473582.0664\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1506918.9609\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1489149.8859\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1497708.2795\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1471890.3577\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1460566.9545\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1432984.8214\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1450102.4627\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1453575.5918\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1451887.8168\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1427534.1005\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1433418.0750\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1422929.3514\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1388395.2409\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1424157.4491\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1366803.6759\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1390599.2423\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1371677.4255\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1388993.2714\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1389376.0155\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1365352.0470\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1359214.0268\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1340909.3450\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1363190.9718\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1354312.9055\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1340673.0627\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1290056.5475\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1339544.6482\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8040385849969112 1197.0160150732597 1120.1324850955439 0.9357706755719468 Eagle_education_Roman\n",
            "Building Eagle_office_Mandi\n",
            "Building Eagle_education_Jewell\n",
            "Building Eagle_office_Henriette\n",
            "Building Eagle_health_Reba\n",
            "Building Eagle_lodging_Edgardo\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_6 (LSTM)                (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 8498.0133\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 7557.0424\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 7198.7329\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6809.3010\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6308.2623\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5994.2013\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5714.9795\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5224.3039\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4933.6951\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4754.3172\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4364.8219\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4143.0324\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3984.2383\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3738.6834\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3611.9100\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3370.2032\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3316.2980\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3023.3491\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2931.2887\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2843.9031\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2656.7567\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2519.1823\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2560.5302\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2244.5215\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2260.0306\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2217.7056\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2029.7152\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2008.8001\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1850.1411\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1703.1588\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1666.8425\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1621.0368\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1517.9401\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1517.8199\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1433.8620\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1384.3739\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1374.6943\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 1211.0885\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1171.1878\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1177.8557\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1145.5781\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1065.5193\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1081.6958\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 1029.4745\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 920.0980\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 946.0946\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 912.3919\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 931.4578\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 883.3520\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 871.3240\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.7661481928706323 81.87169456264219 29.702629235653728 0.3627948510694067 Eagle_lodging_Edgardo\n",
            "Building Eagle_education_Cassie\n",
            "Building Eagle_education_Peter\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 7s 16ms/step - loss: 12940814.2436\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12959522.3636\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12727783.4327\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12573916.3382\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12698414.8836\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12662169.4836\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12508130.5527\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12700667.2509\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12671733.3491\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12678088.8764\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 12436657.3673\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12671505.6400\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12749814.8073\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12435984.2982\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12551456.3091\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12398290.5164\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12598850.5455\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12352609.2109\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12511163.2000\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12338051.7127\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12521822.2873\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12079489.7927\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12418828.8400\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12378605.0182\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12367230.4400\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12329374.7164\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12256619.2655\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12175287.2364\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11980796.5818\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12273477.0291\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12473985.4909\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12399750.3782\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11828484.1273\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12149420.9636\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12032695.9673\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12178667.1491\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12042834.7382\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12071476.7745\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12008645.2436\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12088548.1273\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12035314.4764\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11982639.4327\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12151530.4582\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11814864.5673\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11766917.2073\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 12003573.4436\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11761170.2618\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12039503.5782\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12014676.2873\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11768425.1236\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8267485430759834 3147.4307034315702 3076.128045842916 0.9773457577601583 Eagle_education_Peter\n",
            "Building Eagle_health_Gregoria\n",
            "Building Eagle_lodging_Dawn\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_10 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 13156.9235\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 11991.5698\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 11226.6365\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 10595.9125\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 10185.7920\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 9516.2524\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9435.2356\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9030.3359\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 8238.3012\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 8009.1898\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 7430.0105\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 7446.5732\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 7087.5232\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6701.7345\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6065.6157\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6077.0923\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5680.0517\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5398.2333\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5207.6891\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4812.5736\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4718.4801\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4488.6850\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4369.5677\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4138.1938\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3874.8655\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3809.6158\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3547.1539\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3458.8209\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3214.0642\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3265.8416\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3325.1207\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2994.7602\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2958.1338\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2795.4711\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 2734.6554\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 2648.2748\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 2513.0817\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 2341.3095\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2334.8773\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2214.9750\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2227.2459\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2115.2644\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 16ms/step - loss: 2048.1072\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 4s 16ms/step - loss: 2018.9100\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1781.4209\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1847.6052\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1914.3933\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1767.9160\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1664.7451\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1695.1328\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.7248890899079364 92.73484781606884 40.434889325162004 0.43602691196906845 Eagle_lodging_Dawn\n",
            "Building Eagle_office_Nereida\n",
            "Building Eagle_lodging_Tressa\n",
            "Building Eagle_education_Eileen\n",
            "Building Eagle_education_Wesley\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_12 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 0.0019\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0114e-04\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0888e-04\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9871e-04\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0249e-04\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9517e-04\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9675e-04\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0317e-04\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0140e-04\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0479e-04\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0183e-04\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0204e-04\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0101e-04\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.9819e-04\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9634e-04\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9000e-04\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8873e-04\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9447e-04\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9374e-04\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9800e-04\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9240e-04\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9442e-04\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8569e-04\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9026e-04\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8454e-04\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9534e-04\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0142e-04\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8965e-04\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9365e-04\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8961e-04\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9080e-04\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9557e-04\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9345e-04\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8200e-04\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8875e-04\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9037e-04\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8547e-04\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9082e-04\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8968e-04\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8472e-04\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8070e-04\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8963e-04\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8984e-04\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8786e-04\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8909e-04\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.8182e-04\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9723e-04\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.8161e-04\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9148e-04\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9087e-04\n",
            "cor,mean,rmse,rmse/mean,bldg: 0.7078518941773995 0.10523124548135511 0.022420871375351866 0.21306287189506276 Eagle_education_Wesley\n",
            "Building Eagle_health_Vincenza\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_14 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 15419.4376\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 14019.1770\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 13473.8916\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 12801.3800\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 12073.2520\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 11673.0667\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 10917.9993\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 10595.1659\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9965.4888\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9599.5830\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9060.1950\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8471.8765\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 8012.3340\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 7605.5078\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 7311.4717\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6819.4072\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6447.1121\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 6136.5754\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5819.6256\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5424.3926\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5130.2856\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4919.2044\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4471.8548\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4133.1011\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4065.5700\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3817.0289\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3552.9411\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3205.7141\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3200.0225\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2922.3929\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2769.2717\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2630.1038\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2482.6319\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2270.6518\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2267.6277\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2115.1604\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1999.1106\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1867.9681\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1702.7612\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1640.0522\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1553.4839\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1381.1167\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1323.9533\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1253.6592\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1207.4362\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1110.5783\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1032.8090\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 991.6526\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 950.4484\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 899.5433\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8217409529348659 121.90840096508708 28.918718065631833 0.237216777815942 Eagle_health_Vincenza\n",
            "Building Eagle_office_Dallas\n",
            "Building Eagle_education_Shante\n",
            "Building Eagle_office_Chauncey\n",
            "Building Eagle_office_Phyllis\n",
            "Building Eagle_office_Freida\n",
            "Building Eagle_office_Francis\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_16 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 86506.4210\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 84138.4741\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 81165.1360\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 79969.6990\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 78867.9173\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 76634.8222\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 75627.1666\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 74013.4263\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 73612.6935\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 71685.8522\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 70577.2959\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 68259.6817\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 67220.5752\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 66156.4550\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 64857.8231\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 64126.9374\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 61825.0912\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 61153.2073\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 58989.8941\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 59139.0080\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 57417.3248\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 56721.1319\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 55519.8278\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 53837.0668\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 53353.1447\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 51979.6181\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 49995.8956\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 49178.2200\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 48488.5881\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 47655.8291\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 46215.4721\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 45401.5334\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 44103.9225\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 43156.3445\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 42218.4860\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 40807.8806\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 39792.2446\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 39796.2790\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 39086.8487\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 37539.3638\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 36798.3964\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 36205.6625\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 34611.1358\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 33610.6165\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 33328.1672\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 32187.3133\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 31970.6002\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 31202.9529\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 29701.6171\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 29143.6203\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.612183884934729 335.95636354975403 268.9234146790647 0.8004712631056862 Eagle_office_Francis\n",
            "Building Eagle_office_Sheree\n",
            "Building Eagle_education_Sherrill\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_18 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_19 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 5471415.5364\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5348164.6182\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5336600.3082\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5301053.4764\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5242416.7291\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5357893.8764\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5330282.7418\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5298758.9491\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5322709.6545\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5316583.3273\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5243342.3345\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5207562.4582\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5372113.0145\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5230399.6891\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5311441.2855\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5312796.2964\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5280072.7818\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5157258.7200\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5202837.2418\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5140344.6473\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5209115.2636\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5140948.6091\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5112757.0491\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5128854.6527\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5106266.6818\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5096647.4782\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5045559.8945\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5064241.1855\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5089388.2673\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5066209.7509\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5005599.5345\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 5099615.1036\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4983888.7218\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4860452.1473\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4993898.2164\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4853142.5818\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4895233.9382\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4926538.5845\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4892290.9127\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4919317.4182\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4850211.4291\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4989388.0164\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4922012.1091\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4854297.1882\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4947990.7582\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 4814461.6073\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4872720.4309\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4737329.8400\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4863096.4691\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4723702.7691\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.9191698832911114 2030.3587884358317 2108.6841897742797 1.0385771233067576 Eagle_education_Sherrill\n",
            "Building Eagle_education_Brooke\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 4443279.8927\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4289236.9845\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4346334.4282\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4332658.6782\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4302351.3745\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4283582.6555\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4339665.5073\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4233663.2018\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4288217.6164\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4311542.1527\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4280611.8664\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4240483.2273\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4195887.7973\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4174433.6718\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4191052.8682\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4210852.6427\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4290228.2318\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4196509.0891\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4142797.9518\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4099822.3500\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4193608.9545\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4041493.7900\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4127353.1155\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4154783.9545\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4164013.2364\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4030612.2082\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4097144.5745\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4100589.7073\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4038767.1800\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4048802.8636\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3955797.3418\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4012471.6764\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 4009876.9836\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3981306.9382\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3971950.1836\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3932012.1545\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3966412.9318\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3961885.7045\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3997333.5436\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3996454.7936\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3945824.1518\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3897409.1191\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3941728.6809\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3932220.3982\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3849335.4282\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3905000.7127\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3847705.9836\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3880449.4673\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3834136.8600\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 3830441.1673\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8668943529930063 1634.2804902547134 1572.8550887123854 0.962414406885103 Eagle_education_Brooke\n",
            "Building Eagle_education_Alberto\n",
            "Building Eagle_food_Kay\n",
            "Building Eagle_health_Jodi\n",
            "Building Eagle_education_Norah\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_22 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 17ms/step - loss: 603660.0305\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 602522.0361\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 600771.4791\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 585395.7037\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 578569.7334\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 586597.2470\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 578787.7447\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 567408.9547\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 569645.3373\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 581617.6675\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 569786.0382\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 556353.4405\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 550532.9788\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 557431.9537\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 548139.3552\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 546730.7481\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 544142.9889\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 532042.8157\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 537680.9673\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 535007.2352\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 533040.1840\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 527926.0797\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 523086.7858\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 517404.7963\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 509290.0968\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 513979.5394\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 515360.0086\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 505287.0539\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 486863.1518\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 496657.4209\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 487417.2999\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 496549.1659\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 474265.9309\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 473557.4698\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 469977.6552\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 478319.5053\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 482284.7242\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 467099.7919\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 465471.2501\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 453726.3744\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 459634.3949\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 446231.9574\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 445783.2586\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 454145.7105\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 444123.5577\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 434934.6653\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 431847.7391\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 429980.3552\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 430963.6082\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 425112.0076\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8006961414633348 711.3309670245861 703.1040872082858 0.9884345259834358 Eagle_education_Norah\n",
            "Building Eagle_education_Will\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_24 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_25 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 75898.1371\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 72719.7650\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 71656.6711\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 69703.9032\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 67685.6184\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 67167.6983\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 65263.6588\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 64582.7826\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 63325.1256\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 60281.9582\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 59232.1952\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 58685.6759\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 57708.7738\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 56020.8591\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 55540.7417\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 53770.6150\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 52006.5722\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 50192.7418\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 49842.7151\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 49234.8546\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 47117.3643\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 46892.4967\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 44834.2161\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 44680.9858\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 43048.4566\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 42094.6016\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 41288.8114\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 39426.2963\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 38074.0629\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 37435.2662\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 37454.5451\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 35489.3041\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 34968.1696\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 34043.7123\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 33238.6391\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 34376.8963\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 32173.9961\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 30781.9538\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 29914.7057\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 28519.9843\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 28739.4452\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 27289.4811\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 26687.1292\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 25553.3681\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 25380.8093\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 23909.0635\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 23982.3828\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 23386.7145\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 22973.9920\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 22131.6090\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.3432541996930441 226.06948909769736 59.33718085616434 0.2624731939413611 Eagle_education_Will\n",
            "Building Eagle_lodging_Blake\n",
            "Building Eagle_education_Petra\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_26 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 4128.7547\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3578.7939\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3284.9801\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2999.1558\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2799.1780\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2625.7838\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2394.7266\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2267.9215\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2121.9217\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1928.1837\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1858.9782\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1666.3470\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1638.5518\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1503.4484\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1409.5756\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1277.0888\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1233.1936\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1176.1105\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1131.2940\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 998.5969\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 920.8106\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 920.1815\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 880.0513\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 795.6387\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 745.0694\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 674.5429\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 625.3098\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 601.9493\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 574.1254\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 504.7133\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 476.3374\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 451.4522\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 413.0124\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 377.0733\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 373.2237\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 358.4424\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 343.1570\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 324.4959\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 306.6827\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 297.9999\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 275.0321\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 265.8861\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 268.6559\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 251.7613\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 239.3703\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 231.5781\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 217.7120\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 210.5774\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 206.1804\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 195.7874\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.8318744067272149 56.9627497750187 16.666163061862857 0.2925800304179116 Eagle_education_Petra\n",
            "Building Eagle_lodging_Trina\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_28 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 11360.9901\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 10405.6193\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 9998.6126\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 9669.1329\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8764.9791\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8912.6144\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8353.2819\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8099.6167\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7102.6136\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7327.0638\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6993.2013\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6693.7667\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6434.0669\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6389.8011\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5944.0600\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 5661.7268\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5662.6852\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5259.1554\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 5215.8872\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4934.9994\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4820.7142\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4927.2448\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4567.0729\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4563.8243\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4067.1499\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4210.4947\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3895.7058\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 4034.7195\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3689.0051\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3836.6746\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3612.0398\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 3416.2420\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3334.2084\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3194.5895\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 3192.2464\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2946.9386\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2841.7684\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2973.3502\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2824.4870\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2595.8282\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2695.1340\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2586.8434\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2474.7981\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2513.7861\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2371.5716\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2454.5578\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2337.6860\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2303.7796\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2198.4481\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2118.4005\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.7102555423632535 91.19691350551751 34.78718121969397 0.38145130007705036 Eagle_lodging_Trina\n",
            "Building Eagle_health_Reuben\n",
            "Building Eagle_education_Teresa\n",
            "Train on 8747 samples...\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_30 (LSTM)               (None, 24, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_31 (LSTM)               (None, 10)                1240      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,011\n",
            "Trainable params: 3,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 29360.3079\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 27015.5331\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 26362.5720\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 25777.2519\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 25148.2503\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 24027.7006\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 23004.7993\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 22346.9596\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 21900.4352\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 20777.4302\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 20199.8310\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 19831.7479\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 18587.8009\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 18143.6708\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 17801.1552\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 17700.1305\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 16941.4832\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 15554.1068\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 15101.8214\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 15238.5473\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 14283.1592\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 13970.4350\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 13948.0371\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 13175.3188\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 12951.3234\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 12308.0739\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 11819.3335\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 11557.9630\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 11455.5178\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 10623.2847\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 10691.3624\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 10530.2260\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 9770.3527\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 9476.5389\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 9614.8121\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 9051.5628\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8790.3355\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8729.1479\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8698.7794\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8197.4617\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7991.4000\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 8109.6206\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7713.9798\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7950.2540\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 7348.0964\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6881.0943\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 6461.6520\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6512.1862\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 6212.0051\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 6321.4484\n",
            "cor,mean,rmse,rmse/mean,bldg: -0.730241336157007 148.50737262033783 43.6507211264624 0.2939296572033253 Eagle_education_Teresa\n",
            "Building Eagle_office_Norbert\n",
            "Building Eagle_lodging_Casey\n",
            "Building Eagle_office_Tia\n",
            "Building Eagle_office_Remedios\n",
            "Building Eagle_office_Patrice\n",
            "Building Eagle_education_Shana\n",
            "History 24 Future 1\n",
            "Column 1: Correlation of steam and airTemperature\n",
            "          Using one weather feature as leading correlate.\n",
            "Column 2: Mean usage.\n",
            "          Using mean to help understand the RMSE.\n",
            "Column 3: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 4: RMSE/mean normalized to help understand RMSE.\n",
            "Column 5: Building.\n",
            "-0.9192    2030.36    2108.68  1.04   Eagle_education_Sherrill\n",
            "-0.8669    1634.28    1572.86  0.96   Eagle_education_Brooke\n",
            "-0.8492     477.41     352.25  0.74   Eagle_health_Athena\n",
            "-0.8319      56.96      16.67  0.29   Eagle_education_Petra\n",
            "-0.8267    3147.43    3076.13  0.98   Eagle_education_Peter\n",
            "-0.8217     121.91      28.92  0.24   Eagle_health_Vincenza\n",
            "-0.8040    1197.02    1120.13  0.94   Eagle_education_Roman\n",
            "-0.8007     711.33     703.10  0.99   Eagle_education_Norah\n",
            "-0.7661      81.87      29.70  0.36   Eagle_lodging_Edgardo\n",
            "-0.7555     181.94      56.01  0.31   Eagle_public_Alvin\n",
            "-0.7302     148.51      43.65  0.29   Eagle_education_Teresa\n",
            "-0.7249      92.73      40.43  0.44   Eagle_lodging_Dawn\n",
            "-0.7103      91.20      34.79  0.38   Eagle_lodging_Trina\n",
            "-0.6122     335.96     268.92  0.80   Eagle_office_Francis\n",
            "-0.3433     226.07      59.34  0.26   Eagle_education_Will\n",
            " 0.7079       0.11       0.02  0.21   Eagle_education_Wesley\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW9PKIwow9p8"
      },
      "source": [
        "## Useful Links\n",
        "\n",
        "Jason Brownlee  \n",
        "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
        "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
        "https://machinelearningmastery.com/suitability-long-short-term-memory-networks-time-series-forecasting/\n",
        "https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1WeQ4Vw9p-"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}