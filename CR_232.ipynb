{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CR_232.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# CNN + RNN \n",
        "Site = Bull \n",
        "Meter = Chilled Water.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "904f2b14-3217-48e4-84b2-68014a05f250"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Bull'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "# For our CNN, min predictive features is six.\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_HISTORY = 24\n",
        "STEPS_FORWARD = 12 \n",
        "STEPS_FUTURE =  12 \n",
        "METER_FILE='electricity.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3\n",
        "SCALING=1\n",
        "CELLS = 16\n",
        "FILTERS = 16\n",
        "WIDTH = 3\n",
        "STRIDE = (1,1)\n",
        "INPUT_SHAPE = (STEPS_FORWARD,NUM_PREDICTORS,1) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 12 ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "571e2f70-5731-4456-bb92-fa15244abbaa"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvXGAH2wzBWS"
      },
      "source": [
        "def scale(df):\n",
        "    scaler=StandardScaler()\n",
        "    #scaler=MinMaxScaler()\n",
        "    scaled=scaler.fit_transform(df.values)\n",
        "    scaled = pd.DataFrame(scaled,index=df.index,columns=df.columns)\n",
        "    return scaled"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "7b8b534b-d99e-45b1-9868-7f0167b4ce0b"
      },
      "source": [
        "DATE_PARSE=True  # must be true if we use one of these as predictor\n",
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    if DATE_PARSE:\n",
        "        site_df.insert(0,'hour',0)\n",
        "        site_df.insert(1,'month',0)\n",
        "        site_df.insert(2,'doy',0)\n",
        "        L=len(site_df)\n",
        "        for i in range(0,L):\n",
        "            dt=site_df.index[i]\n",
        "            hour=dt.hour\n",
        "            month=dt.month\n",
        "            doy=dt.dayofyear\n",
        "            site_df.iat[i,0] = hour\n",
        "            site_df.iat[i,1] = month\n",
        "            site_df.iat[i,2] = doy\n",
        "    #if SCALING==1:\n",
        "    #    site_df = scale(site_df) # could break if any column is empty\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>19</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-3.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1034.4</td>\n",
              "      <td>340.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-1.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-3.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1035.4</td>\n",
              "      <td>350.0</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-1.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-3.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1036.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-2.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1037.2</td>\n",
              "      <td>360.0</td>\n",
              "      <td>5.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-2.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-5.6</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1037.7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 19:00:00    19     12  365  ...          1034.4          340.0        4.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1035.4          350.0        4.1\n",
              "2017-12-31 21:00:00    21     12  365  ...          1036.4           10.0        4.1\n",
              "2017-12-31 22:00:00    22     12  365  ...          1037.2          360.0        5.7\n",
              "2017-12-31 23:00:00    23     12  365  ...          1037.7           10.0        4.1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "4021e261-57eb-4410-e77a-1dadd8b4a3f7"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>37.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>38.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>38.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>36.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>39.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     meter\n",
              "timestamp                 \n",
              "2017-12-31 19:00:00  37.63\n",
              "2017-12-31 20:00:00  38.88\n",
              "2017-12-31 21:00:00  38.66\n",
              "2017-12-31 22:00:00  36.79\n",
              "2017-12-31 23:00:00  39.31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "13c3455a-3299-45f3-a131-89bc6d25ed2b"
      },
      "source": [
        "# Make X out of weather + meter features, then select wanted columns.\n",
        "# Make y out of meter features, then select meter column only.\n",
        "# Apply scaler to X only.\n",
        "# Pull out time steps for X from the past: STEPS_HISTORY.\n",
        "# Pull out proper number of time steps for X: STEPS_FORWARD.\n",
        "# Pull out proper number of time steps for y: STEPS_FUTURE.\n",
        "# Make X inefficiently with 3-deep nested for loop because\n",
        "# a) want to replace NaN with previous value\n",
        "# b) need to add the RGB dimension for CNN.\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    if SCALING==1:\n",
        "        df = scale(df) # could break if any column is empty\n",
        "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = mdf[PREDICTED_VARIABLE].values  # meter\n",
        "    #\n",
        "    X_shape = (num_samples,STEPS_FUTURE,NUM_PREDICTORS,1) # RGB = 1\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        one_sample = predictor_series[sam:sam+STEPS_FORWARD]\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = one_sample[time]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat,0] = val  # RGB dim = 0\n",
        "        for time in range (0,STEPS_FUTURE):  \n",
        "            y[sam,time]=predicted_series[sam+STEPS_HISTORY+time]\n",
        "    return X,y \n",
        "print(one_bldg_meter.head())\n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     meter\n",
            "timestamp                 \n",
            "2016-01-01 00:00:00  17.95\n",
            "2016-01-01 01:00:00  17.91\n",
            "2016-01-01 02:00:00  16.94\n",
            "2016-01-01 03:00:00  17.69\n",
            "2016-01-01 04:00:00  17.93\n",
            "X shape: (17508, 12, 12, 1)\n",
            "y shape: (17508, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mObWmpMDVuNQ",
        "outputId": "a7199a3e-2656-4d1d-e00c-d23fb9980711"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['hour', 'month', 'doy', 'meter', 'cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
            "X example:\n",
            " [[[-1]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [-1]\n",
            "  [-1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [-1]\n",
            "  [-1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 0]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [-1]\n",
            "  [-1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [-1]\n",
            "  [-1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-2]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 0]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [-1]\n",
            "  [-1]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 1]\n",
            "  [ 0]]\n",
            "\n",
            " [[ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [-1]\n",
            "  [-1]\n",
            "  [ 0]\n",
            "  [ 0]\n",
            "  [ 1]\n",
            "  [ 0]\n",
            "  [ 0]]]\n",
            "y example:\n",
            " [51 42 41 38 39 38 39 40 40 42 43 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_DNN():\n",
        "    print(\"make_DNN\")\n",
        "    print(\"input shape:\",INPUT_SHAPE)\n",
        "    dnn = Sequential()\n",
        "    dnn.add(Conv2D( input_shape=INPUT_SHAPE,\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(Conv2D(\n",
        "            filters=FILTERS,kernel_size=WIDTH,strides=STRIDE,\n",
        "            activation=None, padding=\"valid\"))\n",
        "    dnn.add(MaxPooling2D())\n",
        "    dnn.add(TimeDistributed(Flatten()))\n",
        "    dnn.add(GRU(CELLS,return_sequences=True))\n",
        "    dnn.add(GRU(CELLS,return_sequences=False))\n",
        "    dnn.add(Dense(STEPS_FUTURE))   \n",
        "    dnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    dnn.build(input_shape=INPUT_SHAPE)\n",
        "    return dnn    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "12c83ef3-af1e-4656-bb5e-30f8d1329c1e"
      },
      "source": [
        "cors = []\n",
        "overall = 0\n",
        "cnt = 0\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",BLDG)\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # Smoothing window applies to inputs\n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        # Smoothing does not apply to truth\n",
        "        one_bldg_meter = load_meter_for_building(BLDG,0)\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        X_raw,y_raw = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        y_test = np.asarray(y_raw[split:])\n",
        "        # Train and predict\n",
        "        model = make_DNN()\n",
        "        print(model.summary())\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        y_pred = model.predict(X_test)\n",
        "        # Reporting\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        cnt += 1\n",
        "        print(\"i,mean,rmse,rmse/mean,bldg:\",cnt,mean,rmse,rmse/mean,BLDG)\n",
        "        overall += rmse/mean\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 180154.6229\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 175553.1080\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 173112.4674\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 168620.3453\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 165986.3145\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 162678.7975\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 159404.2772\n",
            "i,mean,rmse,rmse/mean,bldg: 90 618.8234112588725 414.6573276359369 0.6700737562471973 Bull_education_Carl\n",
            "Example prediction:\n",
            " 0 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 2 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 4 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 6 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 8 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 10 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 12 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 14 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 16 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 18 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 20 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Example prediction:\n",
            " 22 [227 228 228 227 227 229 227 228 228 227 228 227]\n",
            "Building Bull_education_Nichole\n",
            " Count bad values before pseudofill: 77\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_180 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_181 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_90 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_90 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_180 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_181 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [150 150 151 155 165 182 211 240 264 277 281 282]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 38937.7555\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37374.6101\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35603.7343\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33733.2533\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32477.7988\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30930.9350\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29547.4079\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27805.5125\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26721.0549\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25261.8171\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23985.8391\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23231.9578\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21677.3013\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20693.1140\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19506.6518\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18205.7473\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17391.8849\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16310.6250\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15470.0193\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 14570.7991\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13653.9266\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12802.1455\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12032.1161\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11495.6503\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10676.9126\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10103.2782\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9286.5737\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8783.5819\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8209.8983\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7690.4171\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7109.3496\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6623.4561\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6309.7646\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5837.5877\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5445.1596\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5214.6650\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4788.8977\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4619.5251\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4327.3656\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4046.7047\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3872.5610\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3780.3342\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3567.4915\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3434.4862\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3312.1199\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3287.7300\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3198.2223\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3085.5709\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3064.4457\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3038.8185\n",
            "i,mean,rmse,rmse/mean,bldg: 91 196.62348060498738 51.90724657787446 0.26399312237868006 Bull_education_Nichole\n",
            "Example prediction:\n",
            " 0 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 2 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 4 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 6 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 8 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 10 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 12 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 14 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 16 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 18 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 20 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Example prediction:\n",
            " 22 [185 186 186 186 186 186 186 185 185 185 186 186]\n",
            "Building Bull_lodging_Jeremiah\n",
            " Count bad values before pseudofill: 87\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_182 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_183 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_91 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_91 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_182 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_183 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [45 44 43 46 57 68 77 81 83 84 85 85]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 3402.0409\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2802.9805\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2391.1971\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2029.6401\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1723.4366\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1459.4745\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1236.2723\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1040.1601\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 875.3427\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 760.1328\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 653.1742\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 572.6411\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 511.7028\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 459.0746\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 417.3208\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 407.1979\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 393.9427\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 390.7986\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 376.7103\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 374.0651\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 376.3139\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 373.1679\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 377.0804\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 374.0228\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 366.5708\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 368.5178\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 377.0900\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 375.6806\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 377.5075\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 378.0565\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 375.1812\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 380.1476\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 375.5183\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 380.0576\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 373.7406\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 370.5632\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 284.2554\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 241.8870\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 221.7744\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 210.5755\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 201.8166\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 197.1118\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 184.9006\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 186.6918\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 178.7788\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 179.5034\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 174.1149\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 170.0751\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 163.7006\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 165.4641\n",
            "i,mean,rmse,rmse/mean,bldg: 92 58.154081282501245 16.518314838933875 0.2840439479852 Bull_lodging_Jeremiah\n",
            "Example prediction:\n",
            " 0 [48 47 46 45 45 44 44 44 45 45 46 47]\n",
            "Example prediction:\n",
            " 2 [54 53 53 53 52 52 51 52 52 53 54 54]\n",
            "Example prediction:\n",
            " 4 [61 61 61 61 61 61 61 61 61 61 62 62]\n",
            "Example prediction:\n",
            " 6 [63 63 63 63 63 63 63 63 63 63 63 63]\n",
            "Example prediction:\n",
            " 8 [72 74 75 77 77 78 78 77 77 76 74 72]\n",
            "Example prediction:\n",
            " 10 [72 74 75 77 77 78 78 77 77 76 74 72]\n",
            "Example prediction:\n",
            " 12 [69 70 71 71 71 72 72 72 71 71 70 69]\n",
            "Example prediction:\n",
            " 14 [65 65 65 65 65 65 64 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 16 [65 65 65 65 65 65 64 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 18 [65 65 65 65 64 64 64 64 64 64 65 65]\n",
            "Example prediction:\n",
            " 20 [63 62 62 62 61 61 61 61 61 61 62 62]\n",
            "Example prediction:\n",
            " 22 [64 63 63 62 62 62 62 62 62 62 63 63]\n",
            "Building Bull_assembly_Lance\n",
            " Count bad values before pseudofill: 40\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_184 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_185 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_92 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_92 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_184 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_185 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [383 376 369 369 392 421 444 446 448 450 461 467]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 6ms/step - loss: 223707.3424\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 215594.2372\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 212101.3641\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 212659.0725\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 203737.9371\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 205998.8615\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 200133.2740\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 194383.0120\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 191252.4570\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 190878.3928\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 186242.8908\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183602.5798\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 180019.2340\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 174701.3342\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 169546.6071\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 168301.0385\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 166949.8926\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 159430.7115\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 157261.2939\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 152585.5543\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 151713.4311\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 144211.7647\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 144628.3572\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 141438.0915\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138514.8828\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 131983.7245\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 133528.4918\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 131193.7972\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 128871.4986\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 120752.8524\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 121233.0806\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 120580.3512\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 115490.9364\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 112509.9875\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 112157.3299\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105913.1063\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 101260.2752\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 101049.5352\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 102240.2530\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 98769.1769\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 95159.8493\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97125.7720\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 95111.4747\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 90050.1701\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 89256.3268\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 86553.8253\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 83963.7949\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 78235.5616\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78787.7867\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78635.6201\n",
            "i,mean,rmse,rmse/mean,bldg: 93 444.1377044496098 297.5602662505101 0.6699729909651707 Bull_assembly_Lance\n",
            "Example prediction:\n",
            " 0 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 2 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 4 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 6 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 8 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 10 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 12 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 14 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 16 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 18 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 20 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Example prediction:\n",
            " 22 [222 223 224 224 223 224 223 224 223 224 222 223]\n",
            "Building Bull_education_Annette\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_186 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_187 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_93 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_93 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_186 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_187 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [164 164 164 164 167 170 178 185 192 195 198 198]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 18956.5544\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17048.6357\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 7ms/step - loss: 16630.8376\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15171.9040\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14065.3003\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 16713.5094\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14096.2332\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13606.4443\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 13190.3281\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13628.8326\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13019.8812\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 14819.2842\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11205.5853\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11932.6759\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11535.6994\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12041.2327\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11696.3625\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11628.1754\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10927.8808\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9672.1946\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8622.4876\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7951.9496\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7397.8955\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7244.4129\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8463.3528\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6884.0880\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5977.1912\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6033.4754\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6890.2838\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6397.0003\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7806.1486\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5660.5567\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6252.2008\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6678.4491\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5590.0183\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5167.9641\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4599.7487\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6079.2838\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4459.3962\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6338.2163\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 5271.6731\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 3534.5017\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 4846.4487\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4042.5039\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3165.3293\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4577.0771\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4889.6584\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 4405.0054\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5495.7400\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 4746.3124\n",
            "i,mean,rmse,rmse/mean,bldg: 94 108.01345054880792 146.3580920252522 1.3549987643355355 Bull_education_Annette\n",
            "Example prediction:\n",
            " 0 [98 98 97 96 98 97 98 96 97 95 97 97]\n",
            "Example prediction:\n",
            " 2 [95 94 94 93 95 94 94 93 93 92 94 94]\n",
            "Example prediction:\n",
            " 4 [102 101 102 101 102 102 102 101 101 100 102 102]\n",
            "Example prediction:\n",
            " 6 [128 127 128 127 128 128 127 127 126 126 128 127]\n",
            "Example prediction:\n",
            " 8 [165 164 164 165 165 165 164 165 164 164 164 164]\n",
            "Example prediction:\n",
            " 10 [169 169 168 169 169 168 168 169 168 168 168 168]\n",
            "Example prediction:\n",
            " 12 [168 168 168 169 169 168 168 169 168 168 168 168]\n",
            "Example prediction:\n",
            " 14 [168 168 168 169 169 168 168 169 168 168 168 168]\n",
            "Example prediction:\n",
            " 16 [100  99 100  99 100 100 100  99  99  98 100 100]\n",
            "Example prediction:\n",
            " 18 [106 104 105 104 105 105 104 104 104 103 105 104]\n",
            "Example prediction:\n",
            " 20 [138 137 138 138 138 138 137 138 137 137 138 137]\n",
            "Example prediction:\n",
            " 22 [165 165 165 166 165 165 165 166 165 165 165 165]\n",
            "Building Bull_education_Violeta\n",
            " Count bad values before pseudofill: 95\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_188 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_189 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_94 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_94 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_188 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_189 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [370 372 373 375 409 458 506 518 512 507 495 489]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 44797.1428\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44462.9364\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 40684.2074\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40079.7637\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37622.6368\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 37224.0076\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 35947.7745\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 35184.2584\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33762.8529\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 32641.1140\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31195.2795\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 30156.7054\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28915.7462\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 28496.0137\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27957.1605\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27011.2950\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25955.3545\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25715.0615\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25028.1730\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23667.8686\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23250.5785\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22405.9412\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22165.9927\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21411.6698\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21478.0968\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20892.5864\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21225.8399\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19947.8838\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19858.1431\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18955.2965\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18534.6575\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18795.0860\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18550.8213\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17606.7671\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17520.5404\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17218.0823\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17349.9306\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17626.6396\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17069.1906\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17396.0081\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17170.5186\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17083.1253\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16648.2711\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16163.5661\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13704.1835\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13111.4479\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12918.9325\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11914.7430\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11444.8669\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11264.7027\n",
            "i,mean,rmse,rmse/mean,bldg: 95 134.30027460556857 50.60567855333682 0.3768099410218073 Bull_education_Violeta\n",
            "Example prediction:\n",
            " 0 [187 187 186 187 187 187 187 187 186 187 187 186]\n",
            "Example prediction:\n",
            " 2 [188 189 188 188 188 189 189 189 187 188 189 188]\n",
            "Example prediction:\n",
            " 4 [189 190 189 189 189 190 190 190 188 189 190 189]\n",
            "Example prediction:\n",
            " 6 [190 190 189 190 190 190 190 190 189 190 190 189]\n",
            "Example prediction:\n",
            " 8 [190 191 190 190 190 190 191 191 189 190 191 190]\n",
            "Example prediction:\n",
            " 10 [190 191 190 190 190 191 191 191 189 190 191 190]\n",
            "Example prediction:\n",
            " 12 [190 190 189 190 189 190 190 190 188 189 190 189]\n",
            "Example prediction:\n",
            " 14 [190 190 189 190 189 190 190 190 188 189 190 189]\n",
            "Example prediction:\n",
            " 16 [189 189 188 189 189 189 189 189 188 189 189 188]\n",
            "Example prediction:\n",
            " 18 [184 184 183 184 184 184 184 184 183 184 184 183]\n",
            "Example prediction:\n",
            " 20 [171 171 170 171 171 171 171 171 170 170 171 170]\n",
            "Example prediction:\n",
            " 22 [161 162 161 161 161 162 162 162 161 161 162 161]\n",
            "Building Bull_education_Stewart\n",
            " Count bad values before pseudofill: 71\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_190 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_191 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_95 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_95 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_190 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_191 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [123 122 123 123 123 125 131 141 151 157 163 164]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 16581.0586\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15149.6643\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14077.8975\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13059.3116\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12072.6401\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11147.5590\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10305.2975\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9480.5539\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8720.9969\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7934.7172\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7264.6867\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6666.8070\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6065.0731\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5514.3075\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4935.6123\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4511.4926\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3998.8292\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3596.2615\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3225.8264\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2873.1755\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2517.2617\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2231.3238\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1989.2916\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1731.3691\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1553.9661\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1360.1161\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1208.1025\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1077.2288\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 929.4334\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 855.3719\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 788.3741\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 734.1747\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 691.6635\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 666.9887\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 612.3618\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 614.2038\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 583.6170\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 573.4178\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 616.2191\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 605.6672\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 582.4292\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 629.5699\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 591.9357\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 568.1776\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 608.7774\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 578.7655\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 607.2505\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 611.3909\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 601.5133\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 569.6144\n",
            "i,mean,rmse,rmse/mean,bldg: 96 123.85313315398817 23.493373787831672 0.1896873594519572 Bull_education_Stewart\n",
            "Example prediction:\n",
            " 0 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 2 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 4 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 6 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 8 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 10 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 12 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 14 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 16 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 18 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 20 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Example prediction:\n",
            " 22 [128 128 128 128 128 128 128 128 128 128 128 128]\n",
            "Building Bull_assembly_Nathanial\n",
            " Count bad values before pseudofill: 124\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_192 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_193 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_96 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_96 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_192 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_193 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [165 165 165 165 171 186 205 228 243 254 253 250]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 45863.0327\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43554.8058\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41681.0135\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 39668.0151\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38153.9110\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36142.7569\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 34660.6804\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33212.3332\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31675.2292\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30277.4647\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28979.0732\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27742.0807\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26204.0263\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24959.4297\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23828.2109\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22372.1072\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21239.6129\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19988.3817\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19051.3874\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17968.0759\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17054.6509\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15942.1075\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15125.2294\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14055.1142\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13331.4139\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12502.3464\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11798.0264\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11037.7757\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10319.4069\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9618.1424\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8841.5107\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8299.0800\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7947.6312\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7385.8991\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6735.6361\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6267.2226\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5914.2391\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5434.0873\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5156.7118\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4778.8111\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4505.1756\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4204.5475\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3910.0052\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3601.1544\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3552.7800\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3317.7888\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3157.1054\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2976.7367\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3016.4937\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2894.2010\n",
            "i,mean,rmse,rmse/mean,bldg: 97 210.59561489838532 62.629876782792614 0.29739402130007414 Bull_assembly_Nathanial\n",
            "Example prediction:\n",
            " 0 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 2 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 4 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 6 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 8 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 10 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 12 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 14 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 16 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 18 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 20 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Example prediction:\n",
            " 22 [196 194 196 195 195 195 195 195 194 195 195 194]\n",
            "Building Bull_lodging_Terence\n",
            " Count bad values before pseudofill: 138\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_97\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_194 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_195 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_97 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_97 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_194 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_195 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [30 29 28 28 28 28 27 26 25 25 25 26]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 583.7133\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 375.8923\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 241.8419\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 158.9819\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 111.0517\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 81.6057\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66.2405\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.8936\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58.4229\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.5149\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.6593\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.6717\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.3150\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.1954\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.9838\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.6457\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58.4877\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.6769\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.8937\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.5269\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.5891\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.1927\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.3774\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.9269\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 37.6380\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.9091\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35.2982\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.2741\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34.7423\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34.5358\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.1133\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.1424\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34.9942\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.5468\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.6422\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31.3534\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.7982\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.4915\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.4448\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.4638\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.5857\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31.9173\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.1883\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.8335\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.0807\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.7388\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.8143\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35.5655\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.2670\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.1204\n",
            "i,mean,rmse,rmse/mean,bldg: 98 23.846826607094574 15.043417829784211 0.6308352082917693 Bull_lodging_Terence\n",
            "Example prediction:\n",
            " 0 [30 31 30 30 29 27 26 25 24 23 23 23]\n",
            "Example prediction:\n",
            " 2 [26 25 25 24 24 23 22 22 22 22 22 22]\n",
            "Example prediction:\n",
            " 4 [23 23 23 23 22 22 22 22 23 23 23 23]\n",
            "Example prediction:\n",
            " 6 [22 22 22 22 23 23 24 24 25 26 26 26]\n",
            "Example prediction:\n",
            " 8 [26 26 26 26 25 25 24 24 24 23 23 23]\n",
            "Example prediction:\n",
            " 10 [26 26 26 25 25 24 23 23 22 22 22 22]\n",
            "Example prediction:\n",
            " 12 [23 23 22 22 22 22 22 22 23 23 23 24]\n",
            "Example prediction:\n",
            " 14 [20 20 20 20 21 22 22 23 24 25 25 25]\n",
            "Example prediction:\n",
            " 16 [20 20 21 22 23 24 25 26 27 28 29 29]\n",
            "Example prediction:\n",
            " 18 [23 24 24 25 26 27 29 29 30 31 31 30]\n",
            "Example prediction:\n",
            " 20 [26 27 28 28 29 30 30 31 31 31 30 29]\n",
            "Example prediction:\n",
            " 22 [30 31 31 31 30 29 28 27 26 25 25 24]\n",
            "Building Bull_lodging_Perry\n",
            " Count bad values before pseudofill: 83\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_98\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_196 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_197 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_98 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_98 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_196 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_197 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [53 51 50 49 49 49 50 50 51 52 51 51]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 2674.7470\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2176.7517\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1797.5195\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1463.8735\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1196.3686\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 961.9846\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 775.1296\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 628.1275\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 489.0209\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 389.5618\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 315.0693\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 253.7146\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 209.2702\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 183.0996\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 158.8648\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 147.1094\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 142.4134\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 134.0044\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 135.7458\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 140.0628\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138.8132\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 137.4547\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138.1863\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 140.2177\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138.9770\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138.8930\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 133.4324\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 136.3589\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 133.7345\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 135.2096\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 136.6724\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 136.7344\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 137.6283\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 138.2138\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 140.8187\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 91.8972\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 72.3386\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71.8426\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 69.4708\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 60.5898\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58.0097\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.7643\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.3803\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.8600\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.2106\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.5804\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.3229\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.3010\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.1739\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.3815\n",
            "i,mean,rmse,rmse/mean,bldg: 99 52.119120667274075 8.65709639632478 0.1661021192508458 Bull_lodging_Perry\n",
            "Example prediction:\n",
            " 0 [43 43 43 43 42 42 42 42 42 42 42 42]\n",
            "Example prediction:\n",
            " 2 [51 51 51 51 51 51 51 51 51 51 51 51]\n",
            "Example prediction:\n",
            " 4 [51 51 50 50 50 50 50 51 50 51 51 51]\n",
            "Example prediction:\n",
            " 6 [54 54 53 53 53 53 53 53 53 54 54 54]\n",
            "Example prediction:\n",
            " 8 [55 54 54 54 54 54 54 54 54 54 55 55]\n",
            "Example prediction:\n",
            " 10 [54 54 54 54 54 54 53 54 54 54 54 54]\n",
            "Example prediction:\n",
            " 12 [54 54 54 54 54 54 54 54 54 54 54 54]\n",
            "Example prediction:\n",
            " 14 [53 53 53 53 53 53 53 53 53 53 53 53]\n",
            "Example prediction:\n",
            " 16 [57 57 57 57 57 57 57 57 57 57 57 57]\n",
            "Example prediction:\n",
            " 18 [57 58 58 58 58 58 58 58 58 58 57 57]\n",
            "Example prediction:\n",
            " 20 [57 57 57 57 57 57 57 57 57 57 57 57]\n",
            "Example prediction:\n",
            " 22 [56 56 56 56 56 56 56 56 56 56 56 56]\n",
            "Building Bull_assembly_Nick\n",
            " Count bad values before pseudofill: 65\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_99\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_198 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_199 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_99 (MaxPooling (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_99 (TimeDis (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_198 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_199 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [799 777 785 812 862 922 971 995 970 934 906 894]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 931866.7627\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 921370.6484\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 916894.7191\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 897719.6798\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 892226.0468\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 890458.7575\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 879878.0998\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 871974.4870\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 865068.8157\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 848894.5405\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 844084.7711\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 838576.8105\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 830428.2355\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 823361.6911\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 818124.0698\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 805278.2902\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 798753.9239\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 786586.5939\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 783572.2557\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 771500.5418\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 764575.8098\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 757164.7916\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 750656.8809\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 743699.8570\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 734130.2641\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 729951.6243\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 718689.2077\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 717489.8614\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 699925.0414\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 701760.9377\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 689884.1466\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 682422.0411\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 676176.1295\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 671110.7082\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 661353.6355\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 655582.6855\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 649039.8559\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 643904.5316\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 636137.7368\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 631121.9720\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 622735.6845\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 617805.2886\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 608012.8048\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 601498.2098\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 599018.4075\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 586811.6516\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 584028.6839\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 573493.9050\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 573013.4616\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 564038.6905\n",
            "i,mean,rmse,rmse/mean,bldg: 100 926.213532070678 711.8259673519538 0.7685333270402223 Bull_assembly_Nick\n",
            "Example prediction:\n",
            " 0 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 2 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 4 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 6 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 8 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 10 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 12 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 14 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 16 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 18 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 20 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Example prediction:\n",
            " 22 [230 230 229 230 228 230 229 230 229 231 231 229]\n",
            "Building Bull_lodging_Elena\n",
            " Count bad values before pseudofill: 95\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_100\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_200 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_201 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_100 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_100 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_200 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_201 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [8 8 8 7 8 8 8 8 7 7 6 6]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 6ms/step - loss: 28.3545\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4.9958\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 3.4857\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3.4146\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2.8170\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.7816\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.6887\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.5970\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.5964\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.5348\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.3871\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.3690\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.3935\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2773\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2238\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2493\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2078\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2028\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.1461\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.1449\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1.0753\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.0497\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.0303\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.0092\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9623\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9807\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9472\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9044\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9448\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9205\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.9031\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.8863\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8854\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8390\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8435\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8535\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8405\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8185\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8742\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8577\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8020\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.7923\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7963\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.8459\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7924\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8115\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8134\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.7471\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.7615\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7772\n",
            "i,mean,rmse,rmse/mean,bldg: 101 6.3330231661774405 1.5631967481436746 0.24683262750279927 Bull_lodging_Elena\n",
            "Example prediction:\n",
            " 0 [8 8 8 7 7 7 6 6 6 5 5 5]\n",
            "Example prediction:\n",
            " 2 [8 8 7 7 7 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 4 [6 6 6 6 6 6 6 7 7 7 7 7]\n",
            "Example prediction:\n",
            " 6 [7 7 7 7 7 7 7 7 7 7 7 7]\n",
            "Example prediction:\n",
            " 8 [7 7 6 6 6 6 6 7 7 7 7 7]\n",
            "Example prediction:\n",
            " 10 [7 7 6 6 6 6 6 6 7 7 7 7]\n",
            "Example prediction:\n",
            " 12 [7 7 7 7 6 6 6 6 7 7 7 7]\n",
            "Example prediction:\n",
            " 14 [6 7 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 16 [6 6 6 5 5 5 5 5 5 5 6 6]\n",
            "Example prediction:\n",
            " 18 [6 6 6 6 5 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 20 [6 7 7 7 7 7 8 7 8 8 8 8]\n",
            "Example prediction:\n",
            " 22 [7 7 7 7 7 7 8 8 8 8 8 7]\n",
            "Building Bull_education_Brady\n",
            " Count bad values before pseudofill: 81\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_101\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_202 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_203 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_101 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_101 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_202 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_203 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [18 18 18 19 25 35 48 57 63 67 68 69]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 1316.2799\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 993.5806\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 805.5125\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 626.1757\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 500.9289\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 423.0206\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 354.2170\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 313.4269\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 286.4955\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 271.8897\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 265.9424\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 253.4906\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 251.9301\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 258.1130\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 252.8115\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 250.4436\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 251.5737\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 255.4220\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 255.6241\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 253.7453\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 251.5235\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 256.1556\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 256.0647\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 252.9958\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 253.2009\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 254.2808\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 247.9983\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 208.3988\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 187.1829\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 176.4974\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 165.9019\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 156.3447\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 142.2731\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 132.9548\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 123.5931\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 112.0065\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105.6918\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97.3896\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 89.3818\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83.2293\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79.0170\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 76.0678\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71.6200\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 69.3854\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65.6254\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65.4532\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 61.4686\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 60.7798\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58.3476\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.4214\n",
            "i,mean,rmse,rmse/mean,bldg: 102 33.180581057327146 12.130929748917628 0.365603294528165 Bull_education_Brady\n",
            "Example prediction:\n",
            " 0 [31 28 26 24 22 21 21 22 23 24 26 29]\n",
            "Example prediction:\n",
            " 2 [26 24 23 22 22 24 25 28 32 34 38 40]\n",
            "Example prediction:\n",
            " 4 [21 22 23 25 29 33 38 43 47 51 53 54]\n",
            "Example prediction:\n",
            " 6 [21 22 25 29 34 39 45 50 54 56 58 58]\n",
            "Example prediction:\n",
            " 8 [24 28 32 38 43 49 53 57 60 60 60 57]\n",
            "Example prediction:\n",
            " 10 [32 37 41 46 51 56 59 61 61 60 57 54]\n",
            "Example prediction:\n",
            " 12 [52 55 57 58 58 57 54 51 48 44 40 37]\n",
            "Example prediction:\n",
            " 14 [53 55 57 57 56 54 51 47 43 39 36 33]\n",
            "Example prediction:\n",
            " 16 [55 56 57 56 53 50 46 42 38 34 31 28]\n",
            "Example prediction:\n",
            " 18 [44 43 41 38 35 32 29 25 23 22 21 21]\n",
            "Example prediction:\n",
            " 20 [47 44 42 38 35 31 28 26 24 23 24 25]\n",
            "Example prediction:\n",
            " 22 [33 31 28 26 23 22 21 21 21 23 24 26]\n",
            "Building Bull_education_Nolan\n",
            " Count bad values before pseudofill: 74\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_102\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_204 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_205 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_102 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_102 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_204 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_205 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [51 51 51 51 52 55 60 66 72 76 76 76]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 4250.1395\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3587.6851\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3081.6805\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2633.7980\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2274.6169\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1927.7815\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1629.8429\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1363.1391\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1138.7388\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 947.9970\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 801.8633\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 652.1463\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 539.7528\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 453.1799\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 386.4798\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 336.1126\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 301.7404\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 277.8053\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 254.1282\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240.4469\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240.8296\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 231.3537\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 236.0394\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 231.5817\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239.5078\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 236.7192\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234.1305\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 228.9214\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 242.2409\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 226.7166\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 226.1664\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 229.2321\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 237.3435\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 232.1394\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 232.8098\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 235.0534\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 226.7540\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 227.7809\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234.6241\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 228.8229\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 227.0599\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234.7531\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 241.0661\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234.7931\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234.6938\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 232.9930\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240.1334\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 235.5786\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 229.0077\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 237.8578\n",
            "i,mean,rmse,rmse/mean,bldg: 103 65.70081030708366 14.137459930363242 0.21517938461162306 Bull_education_Nolan\n",
            "Example prediction:\n",
            " 0 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 2 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 4 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 6 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 8 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 10 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 12 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 14 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 16 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 18 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 20 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Example prediction:\n",
            " 22 [65 65 65 65 65 65 65 65 65 65 65 65]\n",
            "Building Bull_education_Mervin\n",
            " Count bad values before pseudofill: 83\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_103\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_206 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_207 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_103 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_103 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_206 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_207 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [190 187 185 185 185 193 207 224 239 250 256 258]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 35837.3020\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33808.3330\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32130.8366\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30410.1436\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28926.3775\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27496.6362\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26140.7640\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24734.1800\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23527.3366\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22064.3081\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21004.7995\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19956.2760\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 18630.0491\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17663.7768\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16570.9003\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15789.7621\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14671.4482\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 13812.3050\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 12822.7708\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12001.2950\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11180.9542\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10491.4544\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9841.4150\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8968.8757\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8371.9397\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7733.9706\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7146.8037\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6515.8269\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6150.6836\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5599.7319\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5122.7402\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4709.7504\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4330.3704\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3972.5724\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3626.0402\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3378.0616\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3094.4298\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2868.7211\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2560.3432\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2409.9044\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2272.1033\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2158.5944\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1988.7161\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1906.7620\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1845.4918\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1721.9981\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1789.7843\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1733.3808\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1666.3089\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1691.4669\n",
            "i,mean,rmse,rmse/mean,bldg: 104 183.6804657793423 38.77678909208905 0.21111003245534066 Bull_education_Mervin\n",
            "Example prediction:\n",
            " 0 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 2 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 4 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 6 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 8 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 10 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 12 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 14 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 16 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 18 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 20 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Example prediction:\n",
            " 22 [182 182 181 182 182 182 182 182 182 182 182 182]\n",
            "Building Bull_education_Lenny\n",
            " Count bad values before pseudofill: 101\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_104\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_208 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_209 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_104 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_104 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_208 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_209 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [27 27 27 27 30 32 35 37 41 45 46 47]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 1212.2818\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 893.1044\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 673.4396\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 506.3796\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 368.8774\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 273.7071\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 205.4300\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 161.9225\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 127.0961\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.8636\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 101.5768\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88.8004\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.7309\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 90.0834\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 89.9465\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88.1284\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88.1113\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 90.7298\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88.4880\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88.7992\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87.7858\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87.3372\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 85.8829\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 89.5534\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 73.8143\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 68.8997\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.6990\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.2222\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.6551\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.8391\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.3783\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.6885\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49.1633\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 45.3880\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43.8240\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.3245\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39.4667\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37.3006\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 36.5946\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34.4586\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34.1225\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32.2634\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.7827\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32.9287\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 29.6452\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.0192\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.7123\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.7568\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.8065\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.0673\n",
            "i,mean,rmse,rmse/mean,bldg: 105 35.09778660528385 6.921409909944867 0.197203601121185 Bull_education_Lenny\n",
            "Example prediction:\n",
            " 0 [32 32 30 30 29 29 29 29 30 31 32 33]\n",
            "Example prediction:\n",
            " 2 [30 29 28 28 27 28 28 29 30 31 33 34]\n",
            "Example prediction:\n",
            " 4 [30 29 28 28 28 29 29 30 31 32 33 34]\n",
            "Example prediction:\n",
            " 6 [28 29 29 31 32 34 36 37 38 40 40 40]\n",
            "Example prediction:\n",
            " 8 [31 32 33 35 36 38 40 42 43 43 43 43]\n",
            "Example prediction:\n",
            " 10 [37 37 38 39 40 40 41 40 40 39 39 38]\n",
            "Example prediction:\n",
            " 12 [42 43 43 43 43 42 41 39 38 37 36 35]\n",
            "Example prediction:\n",
            " 14 [38 39 38 38 38 37 36 35 34 33 32 32]\n",
            "Example prediction:\n",
            " 16 [39 39 38 37 35 34 32 31 30 29 28 28]\n",
            "Example prediction:\n",
            " 18 [41 41 40 39 37 36 33 31 30 29 28 27]\n",
            "Example prediction:\n",
            " 20 [32 31 30 29 28 27 26 25 25 25 25 26]\n",
            "Example prediction:\n",
            " 22 [29 29 27 27 26 26 26 26 26 27 27 28]\n",
            "Building Bull_assembly_Vanessa\n",
            " Count bad values before pseudofill: 81\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_105\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_210 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_211 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_105 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_105 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_210 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_211 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [102  98 102 109 112 117 131 149 161 162 162 150]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 16040.4162\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14663.3396\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13665.5772\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12640.6534\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11775.1480\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10954.4107\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10014.4084\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9315.5958\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8541.8425\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7879.3633\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 7149.5673\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6486.8482\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5983.6098\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5470.8919\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4997.4197\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4537.3704\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4092.0169\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3692.1712\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3358.3951\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3039.2036\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2699.9513\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2428.2517\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2227.3531\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2024.0738\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1829.2475\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1678.4791\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1528.5963\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1400.0602\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1288.9997\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1235.3234\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1157.0888\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1110.8655\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1113.4040\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1064.5133\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1091.4605\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1063.5843\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1007.0299\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1066.7079\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1062.1232\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1052.4808\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1040.8962\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1014.9523\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1031.0694\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1041.1820\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1060.6551\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1056.8252\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1033.1221\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1043.9909\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1078.7775\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1049.1451\n",
            "i,mean,rmse,rmse/mean,bldg: 106 133.2031369646655 39.41284778161441 0.2958852822818232 Bull_assembly_Vanessa\n",
            "Example prediction:\n",
            " 0 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 2 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 4 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 6 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 8 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 10 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 12 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 14 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 16 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 18 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 20 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Example prediction:\n",
            " 22 [124 124 124 124 124 124 124 124 124 124 124 124]\n",
            "Building Bull_services_Nadine\n",
            " Count bad values before pseudofill: 80\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_106\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_212 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_213 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_106 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_106 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_212 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_213 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [145 146 145 146 146 144 140 130 123 121 120 119]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 15987.0498\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14656.5024\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13479.9906\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12562.1509\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11577.1949\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10605.0760\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9817.6379\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8960.0835\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8292.1207\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7566.8710\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6932.0033\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6303.9009\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5725.0718\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5204.2504\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4686.5395\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4179.3738\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3765.2416\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3356.2974\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3011.0281\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 2654.1077\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2338.0933\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2064.9539\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1825.1521\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1604.7313\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1382.2012\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1246.0532\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1088.0845\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 990.8701\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 870.6802\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 803.6134\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 694.6549\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 665.8945\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 629.3610\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 647.8065\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 585.3323\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 575.2218\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 549.8254\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 526.5809\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 549.7998\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 592.3764\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 539.4525\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 565.5889\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 560.6969\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 581.6751\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 572.1089\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 536.7888\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 570.5610\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 565.9396\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 527.6240\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 524.9192\n",
            "i,mean,rmse,rmse/mean,bldg: 107 117.87434442691203 22.236659566622876 0.1886471536680377 Bull_services_Nadine\n",
            "Example prediction:\n",
            " 0 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 2 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 4 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 6 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 8 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 10 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 12 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 14 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 16 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 18 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 20 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Example prediction:\n",
            " 22 [125 125 125 125 125 125 125 125 125 125 125 125]\n",
            "Building Bull_education_Dottie\n",
            " Count bad values before pseudofill: 29\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_107\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_214 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_215 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_107 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_107 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_214 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_215 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1160 1159 1161 1165 1197 1249 1315 1365 1401 1421 1435 1441]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 2387856.2205\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2374006.5155\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2427029.1768\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2456964.4986\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2443065.4955\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2493518.5555\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2245412.8250\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2367153.8855\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2344136.8364\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2307913.4455\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2407097.0555\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2296947.0773\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2454254.3400\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2288953.9605\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2436396.6891\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2156999.9209\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2323758.6350\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2204305.8927\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2240797.0786\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2241295.2286\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2180786.5686\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2347088.7886\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2254950.0009\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2251091.5345\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2101077.9773\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2118200.8695\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2218988.3805\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2085460.6486\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2283087.1309\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2029410.8518\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2103564.4414\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2047191.8327\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2152308.8141\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1978187.9023\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2249466.0932\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1902869.4309\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1966424.6564\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2188149.1959\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2051830.2273\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1976612.3600\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1925414.3409\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2038197.0509\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2015308.0482\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2017525.9132\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1981020.7314\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1889463.5141\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1838347.8809\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1959532.7773\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1868621.0391\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1911183.9536\n",
            "i,mean,rmse,rmse/mean,bldg: 108 1328.9222557038272 3209.523810141431 2.41513286150934 Bull_education_Dottie\n",
            "Example prediction:\n",
            " 0 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 2 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 4 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 6 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 8 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 10 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 12 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 14 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 16 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 18 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 20 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Example prediction:\n",
            " 22 [231 231 232 230 229 229 232 230 230 233 230 233]\n",
            "Building Bull_education_Nina\n",
            " Count bad values before pseudofill: 83\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_108\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_216 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_217 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_108 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_108 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_216 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_217 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [268 268 267 267 268 276 290 310 326 338 341 343]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 59553.0336\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56956.2714\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54688.1501\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52648.6317\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50470.8378\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48840.2473\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 46558.3756\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44986.4197\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42893.6947\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41274.7268\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39707.0146\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37789.1544\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36489.0442\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34750.5668\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33240.2589\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31665.8462\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30198.4356\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29008.5306\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27517.1025\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26225.6641\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24851.1845\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23781.6533\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22454.9794\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21152.9637\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20151.2854\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18979.1116\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18096.7869\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16844.9299\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15833.9838\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14990.7825\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14061.8878\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13290.2891\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12307.4815\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11458.7983\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10834.2981\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9942.9289\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9543.6993\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8825.6581\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8134.3088\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7556.7015\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7079.1375\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6540.1512\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5958.5526\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5567.8330\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5152.4274\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4612.3150\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4397.7604\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4102.7740\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3761.0366\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3530.3637\n",
            "i,mean,rmse,rmse/mean,bldg: 109 246.32049508130592 62.03635377833819 0.2518521804605058 Bull_education_Nina\n",
            "Example prediction:\n",
            " 0 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 2 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 4 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 6 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 8 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 10 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 12 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 14 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 16 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 18 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 20 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Example prediction:\n",
            " 22 [206 206 207 206 205 206 204 206 206 206 206 207]\n",
            "Building Bull_assembly_Daryl\n",
            "Building Bull_education_Gregory\n",
            " Count bad values before pseudofill: 83\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_109\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_218 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_219 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_109 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_109 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_218 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_219 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 53  53  53  52  54  62  77  92 102 106 110 113]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 5456.2619\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 4708.8363\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4165.6406\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3648.3669\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3222.4696\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2847.4002\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2470.3318\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2145.4915\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1887.3452\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1611.4937\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1437.7027\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1247.3404\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1100.2233\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 958.6391\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 882.4937\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 779.1444\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 725.1793\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 683.7585\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 633.7626\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 607.0973\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 592.8230\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 571.2442\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 587.2490\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 575.4743\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 572.5209\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 593.8230\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 578.8696\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 575.9425\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 570.6925\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 578.6598\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 580.7775\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 580.9625\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 588.2663\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 578.1950\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 585.3270\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 569.6694\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 573.9035\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 574.7251\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 579.0395\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 588.7369\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 596.6691\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 580.4380\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 583.7393\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 578.7630\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 587.7049\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 565.5007\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 579.2810\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 567.0727\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 579.8281\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 574.6231\n",
            "i,mean,rmse,rmse/mean,bldg: 110 71.63531104482396 22.469936177397468 0.3136712307054475 Bull_education_Gregory\n",
            "Example prediction:\n",
            " 0 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 2 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 4 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 6 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 8 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 10 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 12 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 14 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 16 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 18 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 20 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 22 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Building Bull_office_Ivette\n",
            " Count bad values before pseudofill: 80\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_110\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_220 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_221 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_110 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_110 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_220 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_221 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [14 14 14 15 16 17 18 20 22 23 23 23]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 471.1064\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 280.9474\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 176.0880\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 110.7602\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 73.6552\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.8582\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 46.9649\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.3027\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.7706\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.9346\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.6991\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38.5305\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32.1631\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.4647\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.1692\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.4297\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24.2000\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23.0299\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22.1739\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21.1109\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21.1600\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20.7586\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19.7500\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19.5717\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18.4316\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18.3992\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 17.8143\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.8855\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15.8908\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14.8067\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14.4883\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13.3489\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13.7714\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12.1371\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12.1576\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12.4723\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11.5079\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11.5672\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11.3077\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11.0817\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10.6675\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10.7728\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10.8948\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10.3606\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.9554\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.5472\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.9925\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.8002\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.7795\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 9.2852\n",
            "i,mean,rmse,rmse/mean,bldg: 111 23.30570740202744 4.949874835130181 0.2123889547630541 Bull_office_Ivette\n",
            "Example prediction:\n",
            " 0 [21 20 20 19 18 18 17 17 18 18 18 19]\n",
            "Example prediction:\n",
            " 2 [20 20 19 18 17 17 17 18 18 19 20 21]\n",
            "Example prediction:\n",
            " 4 [21 20 19 18 18 18 18 18 19 19 20 21]\n",
            "Example prediction:\n",
            " 6 [19 18 18 17 17 17 17 18 18 19 20 21]\n",
            "Example prediction:\n",
            " 8 [17 17 17 18 18 19 20 21 22 23 23 24]\n",
            "Example prediction:\n",
            " 10 [17 17 17 18 19 20 21 22 23 23 24 24]\n",
            "Example prediction:\n",
            " 12 [19 20 21 21 22 23 24 24 24 24 24 24]\n",
            "Example prediction:\n",
            " 14 [20 20 20 20 20 20 20 20 20 20 20 20]\n",
            "Example prediction:\n",
            " 16 [20 20 20 19 19 18 18 18 18 18 18 18]\n",
            "Example prediction:\n",
            " 18 [17 17 17 16 16 17 17 17 18 18 19 20]\n",
            "Example prediction:\n",
            " 20 [25 25 24 24 23 22 21 20 19 19 18 18]\n",
            "Example prediction:\n",
            " 22 [23 23 22 21 20 20 19 19 18 18 18 18]\n",
            "Building Bull_education_Kendra\n",
            " Count bad values before pseudofill: 81\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_111\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_222 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_223 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_111 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_111 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_222 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_223 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [338 337 334 352 400 458 522 568 614 649 680 693]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 223287.8265\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 219487.0309\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 215183.0713\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 211172.4064\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 206461.4195\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 202637.7982\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 198639.5177\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 194424.2322\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 192671.3261\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 187867.9370\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 186492.8259\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 181384.6065\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 176645.3719\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 173365.5467\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 168149.4751\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 167030.6681\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 162107.5912\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 159211.7228\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 155511.3541\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 152854.5923\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 149626.3532\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 145813.6374\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 142788.0592\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 141182.6907\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 137342.4381\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 133874.7423\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 131583.8237\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 127111.8718\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 126732.7185\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 123597.0243\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 118150.1189\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 116868.3149\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 114256.4627\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 111930.5752\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 108509.2146\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105401.5962\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 103722.6203\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 101259.2899\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 98139.5349\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 96703.5707\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 92288.6033\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 91286.3521\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88595.3842\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 86474.2976\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83005.0235\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 81707.5801\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 80330.1917\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 78049.2939\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 75888.6257\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 73271.5181\n",
            "i,mean,rmse,rmse/mean,bldg: 112 459.71572834117217 272.01293609764036 0.5916981284916348 Bull_education_Kendra\n",
            "Example prediction:\n",
            " 0 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 2 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 4 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 6 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 8 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 10 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 12 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 14 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 16 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 18 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 20 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Example prediction:\n",
            " 22 [225 223 223 226 224 224 223 222 222 225 225 223]\n",
            "Building Bull_office_Nicolas\n",
            " Count bad values before pseudofill: 213\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_112\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_224 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_225 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_112 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_112 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_224 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_225 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [25 25 25 25 25 24 23 23 24 25 23 23]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 405.7268\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 237.7916\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 141.8007\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 84.6220\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.5936\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.5473\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.2953\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.8335\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.9530\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.8409\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.2443\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.3898\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.7496\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.8832\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.0146\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.8210\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.5978\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.6326\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24.9645\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.4865\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.5687\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.3747\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.9536\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24.6896\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.7096\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.0897\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.6688\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.2196\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.5337\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.9510\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.5352\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.6275\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25.6658\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24.8313\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.8857\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26.0155\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.3892\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.7342\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25.9289\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25.6606\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23.0373\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20.4927\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19.1759\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18.5661\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17.7276\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16.0934\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15.0391\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14.9624\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14.4169\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13.8080\n",
            "i,mean,rmse,rmse/mean,bldg: 113 19.988099990993636 4.393484494900649 0.2198050088242651 Bull_office_Nicolas\n",
            "Example prediction:\n",
            " 0 [22 22 23 24 24 24 25 25 25 24 24 23]\n",
            "Example prediction:\n",
            " 2 [24 24 24 24 23 23 22 22 21 21 20 21]\n",
            "Example prediction:\n",
            " 4 [23 23 23 23 23 23 23 22 22 22 21 21]\n",
            "Example prediction:\n",
            " 6 [24 24 23 23 23 22 22 21 20 20 20 20]\n",
            "Example prediction:\n",
            " 8 [23 23 23 23 22 22 21 20 20 20 20 20]\n",
            "Example prediction:\n",
            " 10 [23 22 22 22 21 20 20 19 19 19 19 20]\n",
            "Example prediction:\n",
            " 12 [22 21 21 21 20 20 19 19 19 19 19 20]\n",
            "Example prediction:\n",
            " 14 [20 20 19 19 18 18 18 18 18 19 19 20]\n",
            "Example prediction:\n",
            " 16 [20 19 19 18 18 18 18 18 18 19 19 20]\n",
            "Example prediction:\n",
            " 18 [17 17 17 18 19 20 21 22 23 23 23 23]\n",
            "Example prediction:\n",
            " 20 [18 18 18 19 20 21 22 23 24 24 24 23]\n",
            "Example prediction:\n",
            " 22 [19 20 20 21 21 22 23 24 24 24 24 23]\n",
            "Building Bull_lodging_Hugo\n",
            " Count bad values before pseudofill: 47\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_113\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_226 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_227 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_113 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_113 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_226 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_227 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [64 60 58 57 57 59 61 62 63 64 64 65]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 5264.0566\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4522.5133\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3956.3368\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3467.0247\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3024.0173\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2604.1604\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2279.8648\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1953.2232\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1697.8540\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1439.0210\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1224.8987\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1079.9017\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 917.0019\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 779.9507\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 716.9183\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 625.1964\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 564.9596\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 500.4777\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 486.4509\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 432.6342\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 433.1756\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 441.8480\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 399.7080\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 409.4968\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 408.5846\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 399.0592\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 427.1359\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 416.2642\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 388.8313\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 428.6354\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 415.6092\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 419.9886\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 461.0348\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 434.9003\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 415.8910\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 433.3885\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 401.4617\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 402.0576\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 450.9771\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 417.4627\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 409.4237\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 421.4084\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 391.8590\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 399.7004\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 407.2740\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 413.5054\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 447.1793\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 450.2103\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 415.9610\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 396.2572\n",
            "i,mean,rmse,rmse/mean,bldg: 114 71.05058881698461 25.383868582858543 0.35726471807634813 Bull_lodging_Hugo\n",
            "Example prediction:\n",
            " 0 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 2 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 4 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 6 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 8 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 10 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 12 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 14 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 16 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 18 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 20 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Example prediction:\n",
            " 22 [71 71 71 71 71 71 71 71 71 71 71 71]\n",
            "Building Bull_office_Yvonne\n",
            " Count bad values before pseudofill: 90\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_114\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_228 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_229 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_114 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_114 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_228 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_229 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [126 126 126 126 134 151 183 215 237 244 250 254]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 29472.2336\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27504.6617\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25976.6284\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24457.4772\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23424.7626\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21886.2346\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20923.4905\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19728.7905\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18616.2223\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17425.4735\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16472.8451\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15458.9483\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14504.1620\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13676.4667\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12782.9527\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11955.2988\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11155.6412\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10289.7695\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9688.9459\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9094.1513\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8398.8174\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7759.6879\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7442.3830\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6725.0802\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6254.2076\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5790.7782\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5345.7523\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5026.4512\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4650.6692\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4373.9712\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3973.2769\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3845.6698\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3506.9056\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3307.8394\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3074.0648\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2921.5647\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2885.3240\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2673.5769\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2640.1374\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2584.5866\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2460.1564\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2391.1984\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2391.7540\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2326.3284\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2304.5858\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2341.1833\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2328.6267\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2276.5808\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2218.8984\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2247.0244\n",
            "i,mean,rmse,rmse/mean,bldg: 115 162.78935706499917 49.056640360271786 0.3013504153142165 Bull_office_Yvonne\n",
            "Example prediction:\n",
            " 0 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 2 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 4 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 6 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 8 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 10 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 12 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 14 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 16 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 18 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 20 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Example prediction:\n",
            " 22 [164 164 164 164 164 164 164 164 164 164 164 164]\n",
            "Building Bull_education_Brandi\n",
            " Count bad values before pseudofill: 87\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_115\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_230 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_231 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_115 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_115 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_230 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_231 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [17 17 16 16 18 22 30 39 45 48 48 49]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 705.7982\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 475.2114\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 338.8623\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 248.2501\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 187.6720\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 153.0682\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 134.8034\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125.8058\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 120.1059\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.7228\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 118.6022\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.7211\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 120.4875\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119.5449\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.1360\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 118.0670\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 116.7942\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 118.3296\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 120.8212\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 118.9610\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.2490\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 118.0277\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105.8702\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 93.1955\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 89.0210\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 85.8654\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 79.3150\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 72.2814\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67.8096\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.5764\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 58.3923\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.8272\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51.6098\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49.0601\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47.0374\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44.5745\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.3498\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39.2532\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37.6770\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 37.6142\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36.7435\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 34.0558\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 34.4752\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33.1361\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32.6266\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 31.3708\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.9501\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 30.0362\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.7094\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.0251\n",
            "i,mean,rmse,rmse/mean,bldg: 116 27.409927050422635 9.35053350276843 0.3411367525921326 Bull_education_Brandi\n",
            "Example prediction:\n",
            " 0 [18 17 16 16 15 15 15 16 16 18 19 22]\n",
            "Example prediction:\n",
            " 2 [20 19 18 17 17 17 17 18 19 21 23 25]\n",
            "Example prediction:\n",
            " 4 [21 20 18 18 18 17 18 18 19 21 24 26]\n",
            "Example prediction:\n",
            " 6 [16 16 16 17 18 21 23 27 30 33 35 36]\n",
            "Example prediction:\n",
            " 8 [17 19 21 24 28 32 35 38 40 41 41 41]\n",
            "Example prediction:\n",
            " 10 [23 26 30 34 37 40 42 43 44 43 42 39]\n",
            "Example prediction:\n",
            " 12 [39 42 44 45 45 44 41 39 35 32 28 25]\n",
            "Example prediction:\n",
            " 14 [44 44 43 40 36 32 28 25 22 21 19 19]\n",
            "Example prediction:\n",
            " 16 [30 30 29 27 26 25 25 24 24 24 24 25]\n",
            "Example prediction:\n",
            " 18 [26 24 23 21 20 19 18 18 18 19 20 21]\n",
            "Example prediction:\n",
            " 20 [24 22 21 20 19 18 18 19 19 21 23 25]\n",
            "Example prediction:\n",
            " 22 [26 24 23 21 20 18 18 18 18 19 20 21]\n",
            "Building Bull_education_Brenda\n",
            " Count bad values before pseudofill: 86\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_116\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_232 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_233 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_116 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_116 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_232 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_233 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [119 118 118 119 121 126 137 153 168 177 179 180]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 22017.6562\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20551.8372\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19053.3960\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17927.1817\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16820.0104\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15677.0690\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14609.3578\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13611.7199\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 12826.4310\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11816.6054\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11046.1021\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10274.6164\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9424.7928\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8760.7852\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8072.8267\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7440.0698\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6891.1454\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6218.2101\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5683.6105\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5228.5656\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4814.4384\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4449.9546\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3959.0932\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3523.3683\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3262.0340\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2929.0523\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2639.3933\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2456.2086\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2208.8350\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1949.7048\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1829.8360\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1707.0051\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1579.2550\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1419.2362\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1388.0544\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1318.6705\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1297.8456\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1254.1208\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1212.8717\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1252.0248\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1137.5909\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1168.7224\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1152.4385\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1167.7044\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1124.8469\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1159.7712\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1112.3575\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1128.7127\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1158.0520\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1188.4772\n",
            "i,mean,rmse,rmse/mean,bldg: 117 144.42071818236082 31.903921283247417 0.2209095875216613 Bull_education_Brenda\n",
            "Example prediction:\n",
            " 0 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 2 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 4 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 6 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 8 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 10 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 12 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 14 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 16 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 18 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 20 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Example prediction:\n",
            " 22 [146 146 146 146 146 146 146 146 146 146 146 146]\n",
            "Building Bull_office_Hilton\n",
            " Count bad values before pseudofill: 104\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_117\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_234 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_235 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_117 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_117 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_234 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_235 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [4 4 4 4 4 4 5 5 5 5 5 5]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 27.2790\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3.8248\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.9621\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.4973\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.4011\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.3526\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2430\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.2038\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.0863\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1.0066\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9673\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.9622\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8861\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8692\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8707\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7832\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.8347\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7768\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7793\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7615\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7866\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7036\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7013\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7000\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6930\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6742\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6425\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6556\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6430\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.7044\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.6627\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5977\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5798\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5951\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5911\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5811\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5573\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5351\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5727\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5390\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5951\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5021\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5252\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5193\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5285\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5060\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5498\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5198\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.5040\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.4815\n",
            "i,mean,rmse,rmse/mean,bldg: 118 6.377929545600329 1.4492300994899523 0.22722579312430174 Bull_office_Hilton\n",
            "Example prediction:\n",
            " 0 [5 5 5 5 5 5 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 2 [6 6 6 6 6 6 6 6 6 7 6 7]\n",
            "Example prediction:\n",
            " 4 [6 6 6 6 6 6 6 6 7 7 7 7]\n",
            "Example prediction:\n",
            " 6 [6 6 6 6 6 6 6 6 7 7 7 7]\n",
            "Example prediction:\n",
            " 8 [6 6 6 6 6 6 6 6 6 6 7 7]\n",
            "Example prediction:\n",
            " 10 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 12 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 14 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 16 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 18 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 20 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Example prediction:\n",
            " 22 [6 6 6 6 6 6 6 6 6 6 6 6]\n",
            "Building Bull_education_Bernice\n",
            " Count bad values before pseudofill: 63\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_118\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_236 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_237 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_118 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_118 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_236 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_237 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [329 330 329 329 329 331 336 343 349 355 357 359]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 112701.1917\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 112066.8680\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 107648.5305\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105561.7489\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 102324.3426\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 98429.7590\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97219.0376\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 93479.0513\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 91427.3694\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 90756.2698\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 85170.2214\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 86981.9230\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 80304.4458\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 78236.7095\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 77634.3491\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 75363.5038\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 75492.1960\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71428.6962\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 68068.3825\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66635.9750\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65784.6633\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62745.0459\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61049.2544\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59765.8037\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57000.5920\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54315.8243\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51846.1435\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51061.5355\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50142.0350\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48807.7275\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 46245.1359\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 45103.5660\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44904.1850\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42414.0169\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40203.0625\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38886.7085\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 36879.2660\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37142.3160\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34826.6975\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35285.6839\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30942.8006\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33956.8697\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32035.9788\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29542.7277\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29785.9241\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24902.0960\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26483.5386\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25643.4583\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24417.2632\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23995.1222\n",
            "i,mean,rmse,rmse/mean,bldg: 119 298.3239559733666 178.34863082415472 0.5978354310911496 Bull_education_Bernice\n",
            "Example prediction:\n",
            " 0 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 2 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 4 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 6 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 8 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 10 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 12 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 14 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 16 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 18 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 20 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Example prediction:\n",
            " 22 [219 217 216 216 216 216 216 217 216 216 216 216]\n",
            "Building Bull_assembly_Maren\n",
            " Count bad values before pseudofill: 212\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_119\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_238 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_239 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_119 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_119 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_238 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_239 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [323 302 313 338 363 376 373 368 361 352 352 352]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 114927.6852\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 109408.7561\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 106787.4804\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105672.3609\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 102554.5079\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 100103.8701\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97347.6659\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 95297.7695\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 93048.8450\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88253.8858\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87691.4730\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 85053.6630\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 85622.8084\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 82677.7229\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 76832.9933\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 76472.1507\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 75014.9276\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71982.6283\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 68920.4437\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66009.5838\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67097.7350\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61811.5103\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64254.9598\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 60380.0423\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59914.5260\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56935.7705\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57538.9101\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 54559.8178\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47994.7293\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51183.4980\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49260.1520\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49077.6001\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 46835.0851\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44569.0661\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42848.5714\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40829.3044\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40143.9560\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36843.7391\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35977.3664\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36545.5685\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35091.1873\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35170.3135\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32236.6607\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32461.0810\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29544.1575\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29564.1083\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27801.5973\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27643.6488\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27056.9514\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25613.6635\n",
            "i,mean,rmse,rmse/mean,bldg: 120 294.33646088184764 209.34541006571166 0.7112452512288207 Bull_assembly_Maren\n",
            "Example prediction:\n",
            " 0 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 2 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 4 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 6 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 8 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 10 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 12 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 14 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 16 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 18 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 20 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Example prediction:\n",
            " 22 [217 217 214 216 216 216 216 217 217 215 216 215]\n",
            "Building Bull_education_Miquel\n",
            " Count bad values before pseudofill: 77\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_120\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_240 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_241 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_120 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_120 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_240 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_241 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [187 187 197 222 255 287 317 348 377 396 405 406]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 70719.6599\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67861.6841\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65636.8916\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63846.1594\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61510.6007\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 58955.7007\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56749.9810\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55261.1463\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52790.3230\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50975.8486\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49172.1503\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47534.0088\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 45503.5829\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43515.0001\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41833.0373\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40372.2701\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 38578.7154\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37476.1836\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35927.5609\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34213.2420\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32845.0922\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31677.9138\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30295.5552\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28995.2289\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27251.8069\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26270.9086\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 25421.7609\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24152.9458\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23025.5579\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22029.1368\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 20779.9537\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19893.6997\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18929.3593\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18083.6753\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16827.0386\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15878.7616\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15293.4318\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14691.3394\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13521.8308\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13135.8085\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12366.9861\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11917.1202\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11239.3388\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10503.1879\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10153.8138\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9734.6272\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9051.7195\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8768.0769\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8303.2986\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8018.4904\n",
            "i,mean,rmse,rmse/mean,bldg: 121 253.60691157471825 78.78267850685198 0.3106487832593665 Bull_education_Miquel\n",
            "Example prediction:\n",
            " 0 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 2 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 4 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 6 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 8 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 10 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 12 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 14 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 16 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 18 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 20 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Example prediction:\n",
            " 22 [210 209 209 209 208 209 209 209 209 209 209 209]\n",
            "Building Bull_office_Marco\n",
            " Count bad values before pseudofill: 60\n",
            " Count bad values after pseudofill: 0\n",
            "make_DNN\n",
            "input shape: (12, 12, 1)\n",
            "Model: \"sequential_121\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_242 (Conv2D)          (None, 10, 10, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_243 (Conv2D)          (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_121 (MaxPoolin (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_121 (TimeDi (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "gru_242 (GRU)                (None, 4, 16)             3936      \n",
            "_________________________________________________________________\n",
            "gru_243 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 8,252\n",
            "Trainable params: 8,252\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 2707.8241\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2246.3185\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1946.7830\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1655.9877\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1404.1764\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1184.1303\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1005.9516\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 865.3738\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 717.8019\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 651.0059\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 580.6121\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 527.5143\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 512.9880\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 455.7250\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 436.7363\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 438.5118\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 381.9150\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 378.2934\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 347.0843\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 343.2500\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 312.8161\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 282.2490\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 281.9735\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 292.7371\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 261.9245\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 267.8372\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 270.6838\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 263.6711\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 251.6715\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 241.9498\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 236.9375\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 230.3558\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 219.2704\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 223.6531\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 229.7787\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 225.9244\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 192.8050\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 193.0117\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 208.8633\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 197.7828\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 172.2705\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 162.2204\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 180.1189\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 193.6726\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 164.2457\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 155.8634\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 171.5708\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 162.3582\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 175.4396\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 153.2297\n",
            "i,mean,rmse,rmse/mean,bldg: 122 50.19311180353842 20.11318961100687 0.4007161319212922 Bull_office_Marco\n",
            "Example prediction:\n",
            " 0 [40 38 37 35 34 33 33 34 36 39 42 44]\n",
            "Example prediction:\n",
            " 2 [37 35 34 32 31 30 31 33 35 38 41 44]\n",
            "Example prediction:\n",
            " 4 [38 36 34 32 30 29 30 32 34 37 41 43]\n",
            "Example prediction:\n",
            " 6 [37 35 33 31 30 29 29 31 33 36 39 42]\n",
            "Example prediction:\n",
            " 8 [35 33 31 29 27 27 27 28 31 33 36 39]\n",
            "Example prediction:\n",
            " 10 [29 27 24 22 21 20 20 22 25 27 30 33]\n",
            "Example prediction:\n",
            " 12 [39 38 37 35 35 35 36 38 40 41 43 45]\n",
            "Example prediction:\n",
            " 14 [47 45 43 42 40 39 40 41 43 45 48 50]\n",
            "Example prediction:\n",
            " 16 [54 52 50 48 45 43 42 41 41 42 43 45]\n",
            "Example prediction:\n",
            " 18 [52 50 48 46 44 42 41 41 41 42 44 46]\n",
            "Example prediction:\n",
            " 20 [42 40 38 37 36 35 36 38 39 41 44 47]\n",
            "Example prediction:\n",
            " 22 [43 40 38 36 34 33 32 33 35 36 39 41]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBCpOQHFnzmP",
        "outputId": "a59f23e4-6d4e-4221-952b-ffae93de7304"
      },
      "source": [
        "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))  \n",
        "overall = overall/cnt\n",
        "print (\"overall = \",overall)  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "History 24 Future 12\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      3.21       3.63  1.13   Bull_office_Mai\n",
            "      3.53       2.63  0.75   Bull_assembly_Newton\n",
            "      6.33       1.56  0.25   Bull_lodging_Elena\n",
            "      6.38       1.45  0.23   Bull_office_Hilton\n",
            "      9.19       3.24  0.35   Bull_lodging_Xavier\n",
            "      9.22       3.53  0.38   Bull_lodging_Allen\n",
            "     10.46       3.63  0.35   Bull_lodging_Charlotte\n",
            "     10.52       2.94  0.28   Bull_assembly_Lesa\n",
            "     11.83       2.81  0.24   Bull_lodging_Graciela\n",
            "     13.10       2.36  0.18   Bull_lodging_Abby\n",
            "     18.02       5.30  0.29   Bull_education_Patrina\n",
            "     19.23       6.48  0.34   Bull_assembly_Dorethea\n",
            "     19.99       4.39  0.22   Bull_office_Nicolas\n",
            "     20.38       3.55  0.17   Bull_office_Anne\n",
            "     21.55       8.46  0.39   Bull_lodging_Anibal\n",
            "     23.31       4.95  0.21   Bull_office_Ivette\n",
            "     23.85      15.04  0.63   Bull_lodging_Terence\n",
            "     24.98       6.04  0.24   Bull_education_Lyn\n",
            "     25.97      15.29  0.59   Bull_lodging_Danielle\n",
            "     27.41       9.35  0.34   Bull_education_Brandi\n",
            "     29.00       9.54  0.33   Bull_office_Debbie\n",
            "     29.94      13.88  0.46   Bull_education_Geneva\n",
            "     30.69       8.12  0.26   Bull_education_Summer\n",
            "     31.16       7.30  0.23   Bull_education_Clarice\n",
            "     31.92       8.84  0.28   Bull_education_Reynaldo\n",
            "     32.85       4.12  0.13   Bull_office_Claudia\n",
            "     33.18      12.13  0.37   Bull_education_Brady\n",
            "     33.77       5.74  0.17   Bull_lodging_Leonard\n",
            "     33.82       5.91  0.17   Bull_lodging_Lettie\n",
            "     34.87      10.54  0.30   Bull_education_Bryon\n",
            "     35.10       6.92  0.20   Bull_education_Lenny\n",
            "     37.55       7.18  0.19   Bull_education_Venita\n",
            "     37.63      10.54  0.28   Bull_education_Pablo\n",
            "     37.83      12.82  0.34   Bull_education_Sebastian\n",
            "     37.99      13.09  0.34   Bull_public_Jefferson\n",
            "     38.85      24.95  0.64   Bull_office_Trevor\n",
            "     40.47       8.54  0.21   Bull_education_Dania\n",
            "     44.03      19.72  0.45   Bull_services_Jeanmarie\n",
            "     46.67      10.71  0.23   Bull_services_Winford\n",
            "     47.37      12.53  0.26   Bull_education_Myra\n",
            "     50.19      20.11  0.40   Bull_office_Marco\n",
            "     50.47      29.44  0.58   Bull_office_Efren\n",
            "     52.12       8.66  0.17   Bull_lodging_Perry\n",
            "     54.26       8.26  0.15   Bull_lodging_Carie\n",
            "     54.61      23.78  0.44   Bull_education_Dakota\n",
            "     57.39      19.30  0.34   Bull_assembly_Goldie\n",
            "     58.14       9.75  0.17   Bull_education_Elva\n",
            "     58.15      16.52  0.28   Bull_lodging_Jeremiah\n",
            "     62.38      18.36  0.29   Bull_office_Myron\n",
            "     65.70      14.14  0.22   Bull_education_Nolan\n",
            "     66.29      34.69  0.52   Bull_education_Olive\n",
            "     70.52      30.12  0.43   Bull_education_Magaret\n",
            "     71.05      25.38  0.36   Bull_lodging_Hugo\n",
            "     71.64      22.47  0.31   Bull_education_Gregory\n",
            "     71.89      31.77  0.44   Bull_office_Ella\n",
            "     75.79      16.18  0.21   Bull_assembly_Brandon\n",
            "     78.81      22.75  0.29   Bull_education_Reina\n",
            "     87.16      39.08  0.45   Bull_education_Matilda\n",
            "     89.12      85.97  0.96   Bull_office_Sally\n",
            "    100.43      48.46  0.48   Bull_education_Lydia\n",
            "    102.10      38.84  0.38   Bull_education_Mario\n",
            "    103.31      33.62  0.33   Bull_services_Rachelle\n",
            "    108.01     146.36  1.35   Bull_education_Annette\n",
            "    112.50      46.08  0.41   Bull_education_Dora\n",
            "    117.87      22.24  0.19   Bull_services_Nadine\n",
            "    118.40      36.28  0.31   Bull_education_Joseph\n",
            "    121.28      86.68  0.71   Bull_assembly_Katia\n",
            "    122.89      35.13  0.29   Bull_education_Genie\n",
            "    123.85      23.49  0.19   Bull_education_Stewart\n",
            "    124.05      21.01  0.17   Bull_services_Juanita\n",
            "    133.20      39.41  0.30   Bull_assembly_Vanessa\n",
            "    134.30      50.61  0.38   Bull_education_Violeta\n",
            "    139.83      42.10  0.30   Bull_lodging_Caren\n",
            "    142.74      50.80  0.36   Bull_office_Chantel\n",
            "    144.42      31.90  0.22   Bull_education_Brenda\n",
            "    147.47      58.91  0.40   Bull_assembly_Gerri\n",
            "    162.79      49.06  0.30   Bull_office_Yvonne\n",
            "    164.31      29.84  0.18   Bull_education_Miranda\n",
            "    166.66      27.61  0.17   Bull_education_Hayley\n",
            "    173.94      30.57  0.18   Bull_education_Dan\n",
            "    175.22      41.91  0.24   Bull_education_Pamela\n",
            "    183.04      53.01  0.29   Bull_education_Fabiola\n",
            "    183.68      38.78  0.21   Bull_education_Mervin\n",
            "    186.27      55.69  0.30   Bull_education_Tracey\n",
            "    186.28      26.65  0.14   Bull_education_Shona\n",
            "    191.87      69.87  0.36   Bull_education_Linnie\n",
            "    196.62      51.91  0.26   Bull_education_Nichole\n",
            "    210.60      62.63  0.30   Bull_assembly_Nathanial\n",
            "    215.07      75.57  0.35   Bull_education_Juan\n",
            "    220.56      72.45  0.33   Bull_assembly_Freddie\n",
            "    229.60      76.22  0.33   Bull_education_Roland\n",
            "    245.83      56.19  0.23   Bull_education_Roseann\n",
            "    246.32      62.04  0.25   Bull_education_Nina\n",
            "    247.75      69.17  0.28   Bull_lodging_Melissa\n",
            "    253.61      78.78  0.31   Bull_education_Miquel\n",
            "    269.84      98.22  0.36   Bull_office_Rob\n",
            "    282.28      88.45  0.31   Bull_education_Krista\n",
            "    294.34     209.35  0.71   Bull_assembly_Maren\n",
            "    295.18     106.74  0.36   Bull_education_Luke\n",
            "    298.32     178.35  0.60   Bull_education_Bernice\n",
            "    322.21     128.27  0.40   Bull_lodging_Dave\n",
            "    328.48     141.61  0.43   Bull_education_Clarita\n",
            "    353.60     181.86  0.51   Bull_assembly_Gigi\n",
            "    369.06     140.87  0.38   Bull_education_Brain\n",
            "    399.15     188.63  0.47   Bull_education_Arthur\n",
            "    401.04     187.55  0.47   Bull_education_Kristal\n",
            "    438.36     230.14  0.52   Bull_public_Hyun\n",
            "    444.14     297.56  0.67   Bull_assembly_Lance\n",
            "    459.68     251.45  0.55   Bull_office_Lilla\n",
            "    459.72     272.01  0.59   Bull_education_Kendra\n",
            "    473.17     277.03  0.59   Bull_education_Antonia\n",
            "    515.09     421.83  0.82   Bull_education_Jeffery\n",
            "    516.60     360.62  0.70   Bull_assembly_Beau\n",
            "    522.73     298.25  0.57   Bull_education_Jae\n",
            "    555.10     314.78  0.57   Bull_assembly_Amalia\n",
            "    574.63     361.83  0.63   Bull_education_Racheal\n",
            "    586.80    1001.39  1.71   Bull_education_Delia\n",
            "    618.82     414.66  0.67   Bull_education_Carl\n",
            "    824.44     617.75  0.75   Bull_lodging_Travis\n",
            "    926.21     711.83  0.77   Bull_assembly_Nick\n",
            "    980.45     780.49  0.80   Bull_education_Barry\n",
            "   1328.92    3209.52  2.42   Bull_education_Dottie\n",
            "overall =  0.41412909000535303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    }
<<<<<<< HEAD
   ],
   "source": [
    "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
    "print(\"Column 1: Mean usage.\")\n",
    "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
    "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
    "print(\"Column 4: Building.\")\n",
    "for cor in sorted(cors):\n",
    "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))  \n",
    "overall = overall/cnt\n",
    "print (\"overall = \",overall)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm8eEJdHbz9v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY4snIvJbz9z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_229.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
=======
  ]
}
>>>>>>> 8b128b67b9520be459af0ace65b7b61856bf1bda
