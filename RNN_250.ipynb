{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_250.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFNRPftWw9pK"
      },
      "source": [
        "# RNN \n",
        "Based on RNN 222 which performed well. GRU 3x16. Smooth the train set with 3 hr window. Given 12 consec hrs yesterday, predict same 12 hrs today. Predictors = steam, predicted = steam.\n",
        "\n",
        "Try the same experiment on electricity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5deM-us2w9pZ"
      },
      "source": [
        "from os import listdir\n",
        "import csv\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats  # mode\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dense\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "mycmap = colors.ListedColormap(['red','blue'])  # list color for label 0 then 1\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZgkgsP6w9pg",
        "outputId": "9a9f10bb-1ba1-42f1-c5fa-4338848a442d"
      },
      "source": [
        "# Constants\n",
        "EPOCHS=50  # use 5 for software testing, 50 for model testing\n",
        "SITE = 'Eagle'\n",
        "PREDICTORS = ['hour','month','doy','meter','cloudCoverage', 'airTemperature', 'dewTemperature', 'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
        "PREDICTORS = ['meter'] # short list for testing\n",
        "NUM_PREDICTORS=len(PREDICTORS)\n",
        "print(\"PREDICTORS=\",NUM_PREDICTORS,PREDICTORS)\n",
        "PREDICTED_VARIABLE = 'meter'  \n",
        "STEPS_HISTORY = 24\n",
        "STEPS_FORWARD = 12 \n",
        "STEPS_FUTURE =  12 \n",
        "METER_FILE='electricity.csv'\n",
        "WEATHER_FILE='weather.csv'\n",
        "EXAMPLE='Eagle_lodging_Edgardo'\n",
        "SITE_BUILDINGS = None\n",
        "SMOOTHING_WINDOW=3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTORS= 1 ['meter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgeDotTmw9pX",
        "outputId": "f2160a08-ef13-4c10-f3fa-f4cc64bc5a25"
      },
      "source": [
        "DATAPATH=''\n",
        "try:\n",
        "    # On Google Drive, set path to my drive / data directory.\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATAPATH=PATH+'My Drive/data/'  # must end in \"/\"\n",
        "except:\n",
        "    # On home computer, set path to local data directory.\n",
        "    IN_COLAB = False\n",
        "    DATAPATH='data/'  # must end in \"/\"\n",
        "\n",
        "ZIP_FILE='BuildingData.zip'\n",
        "ZIP_PATH = DATAPATH+ZIP_FILE\n",
        "MODEL_FILE='Model'  # will be used later to save models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdk510Cw9pc"
      },
      "source": [
        "def read_zip_to_panda(zip_filename,csv_filename):\n",
        "    zip_handle = ZipFile(zip_filename)\n",
        "    csv_handle = zip_handle.open(csv_filename)\n",
        "    panda = pd.read_csv(csv_handle)\n",
        "    return panda\n",
        "def fix_date_type(panda):\n",
        "    # Convert the given timestamp column to the pandas datetime data type.\n",
        "    panda['timestamp'] = pd.to_datetime(panda['timestamp'], infer_datetime_format = True)\n",
        "    indexed = panda.set_index(['timestamp'])\n",
        "    return indexed\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "6YVYM_bqw9pi",
        "outputId": "15ddad7d-cccf-487e-897b-dd2cd1e094da"
      },
      "source": [
        "def load_weather_for_site(site):\n",
        "    wet_df = read_zip_to_panda(ZIP_PATH,WEATHER_FILE)\n",
        "    wet_df = fix_date_type(wet_df)\n",
        "    site_df = wet_df.loc[wet_df['site_id'] == site]\n",
        "    # Drop the site, which is constant (we selected for one site).\n",
        "    site_df = site_df.drop(['site_id'],axis=1)\n",
        "    site_df.insert(0,'hour',0)\n",
        "    site_df.insert(1,'month',0)\n",
        "    site_df.insert(2,'doy',0)\n",
        "    L=len(site_df)\n",
        "    for i in range(0,L):\n",
        "        dt=site_df.index[i]\n",
        "        hour=dt.hour\n",
        "        month=dt.month\n",
        "        doy=dt.dayofyear\n",
        "        site_df.iat[i,0] = hour\n",
        "        site_df.iat[i,1] = month\n",
        "        site_df.iat[i,2] = doy\n",
        "    return site_df\n",
        "\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "one_site_weather.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hour</th>\n",
              "      <th>month</th>\n",
              "      <th>doy</th>\n",
              "      <th>airTemperature</th>\n",
              "      <th>cloudCoverage</th>\n",
              "      <th>dewTemperature</th>\n",
              "      <th>precipDepth1HR</th>\n",
              "      <th>precipDepth6HR</th>\n",
              "      <th>seaLvlPressure</th>\n",
              "      <th>windDirection</th>\n",
              "      <th>windSpeed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 18:00:00</th>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1026.2</td>\n",
              "      <td>330.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>21</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-21.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.2</td>\n",
              "      <td>310.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>22</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>330.0</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "      <td>-12.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-20.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1027.4</td>\n",
              "      <td>320.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     hour  month  doy  ...  seaLvlPressure  windDirection  windSpeed\n",
              "timestamp                              ...                                          \n",
              "2017-12-31 18:00:00    18     12  365  ...          1026.2          330.0        2.6\n",
              "2017-12-31 20:00:00    20     12  365  ...          1027.0          320.0        1.5\n",
              "2017-12-31 21:00:00    21     12  365  ...          1027.2          310.0        2.6\n",
              "2017-12-31 22:00:00    22     12  365  ...          1027.4          330.0        3.1\n",
              "2017-12-31 23:00:00    23     12  365  ...          1027.4          320.0        4.6\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "s-EKuCBibz9d",
        "outputId": "30f13b0f-cec9-4fb7-aae1-06699eb1ae03"
      },
      "source": [
        "def load_meter_for_building(bldg,smooth=0):\n",
        "    all_df = read_zip_to_panda(ZIP_PATH,METER_FILE)\n",
        "    all_df = fix_date_type(all_df)\n",
        "    global SITE_BUILDINGS\n",
        "    SITE_BUILDINGS = [x for x in all_df.columns if x.startswith(SITE)]\n",
        "    site_series = all_df[bldg]\n",
        "    site_df = site_series.to_frame()\n",
        "    #site_df = all_df.loc[all_df['site_id'] == site]\n",
        "    # Change column name from building name to meter.\n",
        "    site_df = site_df.rename(columns={bldg : PREDICTED_VARIABLE})\n",
        "    if smooth>0:\n",
        "        site_df = site_df.rolling(smooth).mean()\n",
        "    return site_df\n",
        "\n",
        "one_bldg_meter = load_meter_for_building(EXAMPLE)\n",
        "print(type(one_bldg_meter))\n",
        "one_bldg_meter.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>meter</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-12-31 19:00:00</th>\n",
              "      <td>37.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 20:00:00</th>\n",
              "      <td>38.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 21:00:00</th>\n",
              "      <td>38.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 22:00:00</th>\n",
              "      <td>36.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31 23:00:00</th>\n",
              "      <td>39.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     meter\n",
              "timestamp                 \n",
              "2017-12-31 19:00:00  37.63\n",
              "2017-12-31 20:00:00  38.88\n",
              "2017-12-31 21:00:00  38.66\n",
              "2017-12-31 22:00:00  36.79\n",
              "2017-12-31 23:00:00  39.31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynRgLt9w9pk",
        "outputId": "8aa94164-777c-4d71-a3bc-46b46a92531e"
      },
      "source": [
        "# TO DO: add smoothing to X\n",
        "def prepare_for_learning(wdf,mdf):\n",
        "    # Concatenate weather and meter.\n",
        "    df = pd.concat([wdf,mdf],axis=1)\n",
        "    num_samples = len(df) - STEPS_FUTURE - STEPS_HISTORY\n",
        "    X_shape = (num_samples,STEPS_FUTURE,NUM_PREDICTORS)\n",
        "    Y_shape = (num_samples,STEPS_FUTURE)\n",
        "    X=np.zeros(X_shape)\n",
        "    y=np.zeros(Y_shape)\n",
        "    predictor_series = df[PREDICTORS].values  # selected features\n",
        "    predicted_series = df[PREDICTED_VARIABLE].values  # meter\n",
        "    # TO DO: can we take predicted from mdf instead?\n",
        "    for sam in range (0,num_samples): \n",
        "        prev_val = 0\n",
        "        one_sample = predictor_series[sam:sam+STEPS_FORWARD]\n",
        "        for time in range (0,STEPS_FORWARD): \n",
        "            one_period = one_sample[time]\n",
        "            for feat in range (0,NUM_PREDICTORS):\n",
        "                val = one_period[feat]\n",
        "                if np.isnan(val):\n",
        "                    val = prev_val\n",
        "                else:\n",
        "                    prev_val = val\n",
        "                X[sam,time,feat] = val\n",
        "        for time in range (0,STEPS_FUTURE):  \n",
        "            y[sam,time]=predicted_series[sam+STEPS_HISTORY+time]\n",
        "    return X,y \n",
        "X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (17508, 12, 1)\n",
            "y shape: (17508, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1cK_1RMiVfC",
        "outputId": "1085069d-67b5-4db2-b22a-24cf19db4511"
      },
      "source": [
        "print(\"X columns:\",PREDICTORS)\n",
        "print(\"X example:\\n\",X[100].astype(int))\n",
        "print(\"y example:\\n\",y[100].astype(int))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X columns: ['meter']\n",
            "X example:\n",
            " [[41]\n",
            " [41]\n",
            " [38]\n",
            " [38]\n",
            " [41]\n",
            " [40]\n",
            " [38]\n",
            " [39]\n",
            " [42]\n",
            " [43]\n",
            " [42]\n",
            " [54]]\n",
            "y example:\n",
            " [51 42 41 38 39 38 39 40 40 42 43 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_8rzumTw9p2"
      },
      "source": [
        "def make_RNN():\n",
        "    # The GRU in Keras is optimized for speed on CoLab GPU.\n",
        "    rnn = Sequential([\n",
        "        GRU(16,return_sequences=True, \n",
        "                  input_shape=(STEPS_FORWARD,NUM_PREDICTORS)), \n",
        "        GRU(16,return_sequences=True),\n",
        "        GRU(16,return_sequences=False),\n",
        "        Dense(STEPS_FUTURE)\n",
        "    ])\n",
        "    rnn.compile(optimizer='adam',loss=MeanSquaredError())\n",
        "    return rnn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XypnRqq9w9p4",
        "scrolled": false,
        "outputId": "5bd2151b-cbda-4deb-f377-81684ecd3d5a"
      },
      "source": [
        "cors = []\n",
        "one_site_weather = load_weather_for_site(SITE)\n",
        "num_processed = 0\n",
        "for BLDG in SITE_BUILDINGS:\n",
        "    print(\"Building\",num_processed,BLDG)\n",
        "    num_processed += 1\n",
        "    one_bldg_meter = load_meter_for_building(BLDG,SMOOTHING_WINDOW)\n",
        "    count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "    MAX_BAD = 500\n",
        "    if count_bad<=MAX_BAD:\n",
        "        # Must get rid of Nan labels, else loss hits NaN during training.\n",
        "        print(\" Count bad values before pseudofill:\",count_bad)\n",
        "        pseudovalue = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        one_bldg_meter = one_bldg_meter.fillna(pseudovalue)\n",
        "        count_bad = one_bldg_meter[PREDICTED_VARIABLE].isna().sum()\n",
        "        print(\" Count bad values after pseudofill:\",count_bad)\n",
        "        # Smoothed\n",
        "        X,y = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        split = len(X)//2   # year 1 vs year 2\n",
        "        X_train = np.asarray(X[0:split])\n",
        "        y_train = np.asarray(y[0:split])\n",
        "        X_test = np.asarray(X[split:])\n",
        "        # Not smoothed\n",
        "        unsmoothed = load_meter_for_building(BLDG,0)\n",
        "        unsmoothed = unsmoothed.fillna(pseudovalue)\n",
        "        X_raw,y_raw = prepare_for_learning(one_site_weather,one_bldg_meter)\n",
        "        y_test = np.asarray(y_raw[split:])\n",
        "        #\n",
        "        model = make_RNN()\n",
        "        print(model.summary())\n",
        "        #print(\"Example X train:\\n\",X_train[example].astype(int))\n",
        "        example=411\n",
        "        print(\"Example y train:\\n\",y_train[example].astype(int))\n",
        "        model.fit(X_train,y_train,epochs=EPOCHS)\n",
        "        # Keep a table for reporting later.\n",
        "        y_pred = model.predict(X_test)\n",
        "        rmse = mean_squared_error(y_test,y_pred,squared=False)\n",
        "        mean = one_bldg_meter[PREDICTED_VARIABLE].mean()\n",
        "        cors.append([mean,rmse,rmse/mean,BLDG])\n",
        "        print(\"mean,rmse,rmse/mean,bldg:\",mean,rmse,rmse/mean,BLDG)\n",
        "        for hr in range(0,24,2):\n",
        "            print(\"Example prediction:\\n\",hr,y_pred[example+hr].astype(int))\n",
        "print()\n",
        "print(\"History\",STEPS_HISTORY,\"Future\",STEPS_FUTURE)\n",
        "print(\"Column 1: Mean usage.\")\n",
        "print(\"Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\")\n",
        "print(\"Column 3: RMSE/mean normalized to help understand RMSE.\")\n",
        "print(\"Column 4: Building.\")\n",
        "for cor in sorted(cors):\n",
        "    print(\"%10.2f %10.2f %5.2f   %s\"%(cor[0],cor[1],cor[2],cor[3]))    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building 0 Eagle_office_Lamont\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [31 30 32 35 33 27 25 26 28 30 34 38]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 20s 5ms/step - loss: 822.1761\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 574.5101\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 406.4752\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 289.6851\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 206.9877\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 146.0618\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 107.6975\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87.5907\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 74.7631\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67.6337\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63.6459\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66.4466\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61.5032\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61.9803\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.9882\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.9939\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 54.8754\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.7137\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.0510\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.8277\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.0579\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.8496\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.8214\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.5526\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.6207\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.3441\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.0632\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.6733\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.5949\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.5373\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 51.5910\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.3462\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.2883\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54.0445\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 52.8739\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.1444\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 50.3922\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.1531\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.6195\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.6700\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51.5699\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.3720\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.5815\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51.1860\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.0676\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.5866\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.2533\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.6981\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49.7158\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47.2063\n",
            "mean,rmse,rmse/mean,bldg: 30.405174953444703 5.774469407712953 0.1899173221846153 Eagle_office_Lamont\n",
            "Example prediction:\n",
            " 0 [34 33 33 33 32 32 31 31 30 30 30 30]\n",
            "Example prediction:\n",
            " 2 [32 32 32 32 32 31 31 31 30 30 30 30]\n",
            "Example prediction:\n",
            " 4 [32 32 31 31 31 30 29 29 28 29 28 29]\n",
            "Example prediction:\n",
            " 6 [32 31 31 30 30 29 29 28 28 28 27 28]\n",
            "Example prediction:\n",
            " 8 [30 29 28 28 28 27 27 27 27 27 27 28]\n",
            "Example prediction:\n",
            " 10 [28 28 28 28 28 28 28 28 29 29 29 30]\n",
            "Example prediction:\n",
            " 12 [28 29 29 30 30 31 32 32 32 32 32 32]\n",
            "Example prediction:\n",
            " 14 [28 30 31 31 32 33 34 34 34 34 33 32]\n",
            "Example prediction:\n",
            " 16 [33 34 36 37 38 38 38 38 37 36 34 32]\n",
            "Example prediction:\n",
            " 18 [33 35 36 37 38 38 38 38 37 35 34 32]\n",
            "Example prediction:\n",
            " 20 [34 34 35 35 35 35 35 34 33 33 32 31]\n",
            "Example prediction:\n",
            " 22 [34 34 34 34 33 33 32 32 31 31 30 30]\n",
            "Building 1 Eagle_health_Athena\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_3 (GRU)                  (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [393 375 355 315 293 253 253 233 233 273 335 437]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 98019.2569\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97610.1037\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 94529.3654\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 92463.0015\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 90502.9086\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88879.1857\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 86088.8064\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83301.1208\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83072.3261\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 80994.7120\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 78386.9893\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 78489.2033\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 75615.5298\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 72466.5533\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 72010.6336\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 68966.3341\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 68331.6161\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66258.8329\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64957.9554\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63020.3703\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61324.8218\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59962.0549\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56573.2256\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56472.7453\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55625.0673\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54210.3760\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51988.4884\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50605.4412\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49373.0253\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48685.1853\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47352.6601\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 46909.7774\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43906.3104\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42921.3042\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42131.2580\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39825.5142\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39357.0697\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38012.3456\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37108.9721\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35156.0006\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34162.5799\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32960.7026\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32343.2388\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31357.5386\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31221.0256\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29589.9332\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29004.5224\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28155.6558\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27355.6439\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26951.6158\n",
            "mean,rmse,rmse/mean,bldg: 296.63987630043493 178.20282737905518 0.6007379371968606 Eagle_health_Athena\n",
            "Example prediction:\n",
            " 0 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 2 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 4 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 6 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 8 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 10 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 12 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 14 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 16 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 18 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 20 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Example prediction:\n",
            " 22 [219 217 218 219 218 219 218 219 217 217 217 217]\n",
            "Building 2 Eagle_office_Ryan\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_6 (GRU)                  (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [24 24 24 24 24 24 24 24 24 26 28 34]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 688.6673\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 464.0521\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 316.7465\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 218.0790\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 150.4468\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 109.5663\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87.2940\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 73.4247\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67.9197\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65.0481\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63.8903\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63.5324\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64.7170\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64.8705\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64.5977\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.9229\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64.8704\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 66.3627\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 57.5444\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 4ms/step - loss: 56.2492\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.4019\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.6200\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51.6414\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.8242\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53.3126\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.8632\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51.9560\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 52.4174\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49.9054\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 50.9683\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.0195\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.1577\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47.6494\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44.6795\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44.5727\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 44.9008\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.6420\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.8238\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.5369\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.3651\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.0746\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.8142\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.0437\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39.1235\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.0458\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39.1640\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38.3206\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39.4181\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.4861\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.2280\n",
            "mean,rmse,rmse/mean,bldg: 26.04252650780974 5.2843076472627635 0.20291071396924887 Eagle_office_Ryan\n",
            "Example prediction:\n",
            " 0 [31 30 30 29 27 26 25 24 24 23 22 22]\n",
            "Example prediction:\n",
            " 2 [31 30 29 28 26 25 24 23 23 22 22 22]\n",
            "Example prediction:\n",
            " 4 [29 28 27 26 24 23 22 22 21 21 21 22]\n",
            "Example prediction:\n",
            " 6 [26 25 24 23 22 22 22 22 22 23 23 24]\n",
            "Example prediction:\n",
            " 8 [21 20 20 20 20 21 22 24 25 27 28 30]\n",
            "Example prediction:\n",
            " 10 [21 21 21 22 23 24 25 27 28 30 31 31]\n",
            "Example prediction:\n",
            " 12 [24 25 26 27 28 29 30 31 32 32 31 31]\n",
            "Example prediction:\n",
            " 14 [25 26 27 28 29 30 31 31 32 31 31 30]\n",
            "Example prediction:\n",
            " 16 [30 30 29 28 28 27 26 25 25 24 24 23]\n",
            "Example prediction:\n",
            " 18 [32 32 31 29 28 26 25 23 22 21 20 20]\n",
            "Example prediction:\n",
            " 20 [31 30 29 28 26 25 23 22 21 20 20 20]\n",
            "Example prediction:\n",
            " 22 [29 29 27 26 24 22 20 19 18 18 17 18]\n",
            "Building 3 Eagle_assembly_Herbert\n",
            " Count bad values before pseudofill: 17\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_9 (GRU)                  (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [87 85 81 65 51 39 39 39 40 54 70 88]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 5631.2156\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4965.3217\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4415.1351\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3871.5529\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3477.8609\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3109.4807\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2731.5201\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2425.9839\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2227.1927\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1985.4221\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1814.6315\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1654.9279\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1531.9609\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1398.9684\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1295.0385\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1215.1866\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1174.1465\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1128.3430\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1107.4730\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1081.2607\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1058.5520\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1061.1435\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1042.2838\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1038.9286\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1050.3021\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1043.3900\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1048.3945\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1045.5295\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1053.2199\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1034.6327\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1051.6898\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1039.3092\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1056.6997\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1053.4930\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1027.6994\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1041.1837\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1040.0874\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1022.0451\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1034.8983\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1045.9950\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1042.0797\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1056.1085\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1036.1648\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1044.0752\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1033.0201\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1045.6246\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1043.9374\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1030.3530\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1057.8329\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 858.0340\n",
            "mean,rmse,rmse/mean,bldg: 68.3381669994867 28.51427696441497 0.4172525868981699 Eagle_assembly_Herbert\n",
            "Example prediction:\n",
            " 0 [72 72 73 73 73 73 73 73 73 73 73 72]\n",
            "Example prediction:\n",
            " 2 [72 72 72 73 73 73 73 73 73 73 72 72]\n",
            "Example prediction:\n",
            " 4 [72 72 72 73 73 73 73 73 73 73 72 72]\n",
            "Example prediction:\n",
            " 6 [67 67 68 67 67 68 68 68 68 68 67 67]\n",
            "Example prediction:\n",
            " 8 [64 64 65 64 64 64 65 64 65 65 64 64]\n",
            "Example prediction:\n",
            " 10 [70 70 70 70 70 70 70 70 70 70 70 70]\n",
            "Example prediction:\n",
            " 12 [72 73 73 73 73 73 73 73 73 73 73 72]\n",
            "Example prediction:\n",
            " 14 [73 74 74 74 74 74 74 74 74 74 74 73]\n",
            "Example prediction:\n",
            " 16 [73 74 74 74 74 74 74 75 74 74 74 74]\n",
            "Example prediction:\n",
            " 18 [74 74 74 74 75 75 75 75 74 74 74 74]\n",
            "Example prediction:\n",
            " 20 [74 74 74 74 75 75 75 75 74 74 74 74]\n",
            "Example prediction:\n",
            " 22 [74 74 74 74 75 75 75 75 74 74 74 74]\n",
            "Building 4 Eagle_public_Alvin\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_12 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [109 104  98  90  85  81  78  79  77  80  90 107]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 10197.4848\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9110.0728\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8269.1028\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7601.3370\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6892.2808\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6297.2705\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5700.0578\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5092.9466\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4611.6729\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4151.1195\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3708.5545\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3366.1021\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3035.1807\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2667.4804\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2404.7525\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2100.1019\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1857.1291\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1614.0517\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1387.3112\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1231.4877\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1086.3450\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 983.3872\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 874.7513\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 799.4592\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 754.2690\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 685.4729\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 688.1157\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 637.1123\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 628.2170\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 607.4601\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 587.9683\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 595.9805\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 618.2416\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 590.5840\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 534.2586\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 509.4561\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 508.7944\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 490.8096\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 472.1608\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 458.9512\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 491.0987\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 460.0689\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 447.5865\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 457.0651\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 440.9942\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 435.6374\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 439.8802\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 453.6178\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 452.4790\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 453.9706\n",
            "mean,rmse,rmse/mean,bldg: 102.56918833846355 17.03193606866973 0.16605314270857632 Eagle_public_Alvin\n",
            "Example prediction:\n",
            " 0 [105 105 105 105 105 105 105 105 105 105 105 105]\n",
            "Example prediction:\n",
            " 2 [101 101 100 100 100 100 100 100 100 100 100 101]\n",
            "Example prediction:\n",
            " 4 [75 74 73 72 72 71 72 72 72 73 74 76]\n",
            "Example prediction:\n",
            " 6 [93 92 91 90 89 89 89 89 90 90 91 92]\n",
            "Example prediction:\n",
            " 8 [99 98 97 97 96 96 96 96 97 97 98 98]\n",
            "Example prediction:\n",
            " 10 [101 100 100 100  99  99  99  99 100 100 100 100]\n",
            "Example prediction:\n",
            " 12 [102 101 101 101 100 101 100 100 101 101 101 101]\n",
            "Example prediction:\n",
            " 14 [103 103 103 103 102 103 102 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 16 [113 115 116 117 118 118 118 118 117 116 114 113]\n",
            "Example prediction:\n",
            " 18 [114 115 117 118 119 119 119 119 118 117 115 113]\n",
            "Example prediction:\n",
            " 20 [113 114 115 116 117 117 117 117 116 115 114 112]\n",
            "Example prediction:\n",
            " 22 [107 107 107 107 108 108 108 108 107 107 107 106]\n",
            "Building 5 Eagle_education_Raul\n",
            " Count bad values before pseudofill: 23\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_15 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_17 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [664 629 619 603 631 642 654 647 646 655 689 709]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 4s 5ms/step - loss: 421607.0734\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 414663.7430\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 409031.1278\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 402193.0500\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 396647.4265\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 391034.0262\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 384818.7518\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 379270.3656\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 374339.1099\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 368855.6663\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 363350.6766\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 357305.6019\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 352516.7648\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 347546.7744\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 342266.9777\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 335231.6069\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 331473.3312\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 326473.0280\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 321435.5563\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 315984.2828\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 311244.9492\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 306402.8559\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 300958.2835\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 296253.2690\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 289848.3378\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 287041.5272\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 281354.4956\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 277067.3514\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 272399.3927\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 267879.6878\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 262447.2569\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 258725.6765\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 253591.4378\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 248527.1228\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 245692.4532\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240670.5078\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 236286.2245\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 231651.6601\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 227826.2980\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 224045.7525\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 220058.3642\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 215322.1192\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 210744.8302\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 206939.6003\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 202985.9490\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 199895.4852\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 195202.4974\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 191099.7305\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 187467.7605\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 183183.0352\n",
            "mean,rmse,rmse/mean,bldg: 653.3230620721913 435.7148514054135 0.6669209717217477 Eagle_education_Raul\n",
            "Example prediction:\n",
            " 0 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 2 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 4 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 6 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 8 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 10 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 12 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 14 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 16 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 18 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 20 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Example prediction:\n",
            " 22 [228 228 227 227 227 226 229 229 229 227 227 229]\n",
            "Building 6 Eagle_education_Roman\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_18 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_19 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_20 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [686 686 660 660 650 681 678 663 664 709 767 807]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 465668.2591\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 457503.1869\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 451469.2103\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 447463.9673\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 440180.6474\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 434345.5976\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 428056.5053\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 422680.6466\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 417875.9461\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 411552.8483\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 403997.1606\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 399801.2050\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 393063.0802\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 387001.4410\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 382171.2693\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 375188.0783\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 370960.3538\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 365290.2311\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 359465.7714\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 354547.6835\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 349384.4010\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 344974.8890\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 339056.2151\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 334823.9644\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 329303.1782\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 324071.0199\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 318844.4520\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 312795.8108\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 309227.1423\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 303982.7394\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 298893.5883\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 293935.5786\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 289724.9008\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 285663.9965\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 280547.6556\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 275654.5277\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 270597.5058\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 266670.3619\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 261682.7798\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 257524.5825\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 253005.6256\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 249327.4398\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 243971.2518\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239338.5167\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 235624.8244\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 230617.8503\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 226945.7108\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 222658.4165\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 217711.8134\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 214475.5619\n",
            "mean,rmse,rmse/mean,bldg: 686.8166305628407 470.8232239209911 0.6855151767876605 Eagle_education_Roman\n",
            "Example prediction:\n",
            " 0 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 2 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 4 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 6 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 8 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 10 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 12 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 14 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 16 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 18 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 20 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Example prediction:\n",
            " 22 [228 228 228 228 228 228 228 229 229 229 231 228]\n",
            "Building 7 Eagle_office_Mandi\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_21 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_22 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_23 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [16 16 15 15 15 15 15 15 16 16 17 18]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 221.0053\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 100.8968\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 48.1972\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22.6254\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13.2682\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.8247\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.8652\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.7286\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.0051\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.2263\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.1604\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.9449\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.3400\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.4084\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.0794\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.5677\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.0674\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.2671\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.0405\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.5321\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.1284\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.2499\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9.2048\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.4535\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.3490\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.6791\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.9734\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.9680\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.2051\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.0941\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.9131\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.3095\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.9584\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.4298\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.4026\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.7131\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.1750\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.8438\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.1522\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.1869\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.8869\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.4292\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.9861\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.0101\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.6081\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.1022\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.8210\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.5579\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8.2550\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7.8942\n",
            "mean,rmse,rmse/mean,bldg: 16.094371772507863 1.9095571202903305 0.11864750903494128 Eagle_office_Mandi\n",
            "Example prediction:\n",
            " 0 [17 17 16 16 16 16 16 16 16 16 16 16]\n",
            "Example prediction:\n",
            " 2 [16 16 16 16 16 16 16 16 16 16 16 16]\n",
            "Example prediction:\n",
            " 4 [16 16 16 16 16 16 16 15 15 15 15 16]\n",
            "Example prediction:\n",
            " 6 [16 16 16 16 15 15 15 15 15 15 15 15]\n",
            "Example prediction:\n",
            " 8 [15 15 15 14 15 15 15 15 16 16 17 16]\n",
            "Example prediction:\n",
            " 10 [15 15 15 15 15 15 15 16 17 17 17 17]\n",
            "Example prediction:\n",
            " 12 [15 15 15 15 16 16 16 16 17 17 17 17]\n",
            "Example prediction:\n",
            " 14 [16 16 17 17 17 18 17 18 18 18 17 17]\n",
            "Example prediction:\n",
            " 16 [17 17 18 18 18 18 18 18 18 18 17 17]\n",
            "Example prediction:\n",
            " 18 [18 18 19 19 19 19 19 18 18 18 17 17]\n",
            "Example prediction:\n",
            " 20 [18 18 18 18 18 18 18 18 17 17 17 16]\n",
            "Example prediction:\n",
            " 22 [17 17 17 18 18 17 17 17 16 16 16 16]\n",
            "Building 8 Eagle_office_Elia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_24 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_25 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_26 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [118 106  92  82  77  86  94 103 104 118 131 145]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 20888.6766\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19236.8923\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18416.4769\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17101.4195\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16144.2101\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15066.5855\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14216.8547\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13409.9835\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12487.9072\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11870.0359\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10996.1351\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10507.7874\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9880.1929\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9262.5943\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8562.2871\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8147.8484\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7499.9725\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7125.1341\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6717.1592\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6377.9864\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5939.9748\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5541.4594\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5109.0084\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4630.3809\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4311.1850\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4158.2816\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3879.8964\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3663.1234\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3528.8194\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3347.8935\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3186.0243\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2987.5654\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2898.4275\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2839.8941\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2734.1262\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2739.2331\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2713.0815\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2658.4474\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2371.1863\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2272.1557\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2226.6550\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2114.7177\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2124.4398\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2112.8432\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1991.5217\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1999.8993\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1913.1523\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1975.6116\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1899.6614\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1941.1808\n",
            "mean,rmse,rmse/mean,bldg: 134.94530336335626 37.000010276805554 0.27418523916448306 Eagle_office_Elia\n",
            "Example prediction:\n",
            " 0 [122 120 119 118 118 118 118 118 119 119 120 122]\n",
            "Example prediction:\n",
            " 2 [113 111 110 109 108 109 108 109 110 110 112 114]\n",
            "Example prediction:\n",
            " 4 [106 104 103 101 101 102 101 102 103 103 105 107]\n",
            "Example prediction:\n",
            " 6 [107 105 104 103 102 103 102 103 104 104 107 108]\n",
            "Example prediction:\n",
            " 8 [113 111 110 109 109 109 109 109 110 110 112 114]\n",
            "Example prediction:\n",
            " 10 [120 118 117 116 116 116 116 116 117 117 119 120]\n",
            "Example prediction:\n",
            " 12 [132 132 131 131 131 131 131 131 131 131 131 132]\n",
            "Example prediction:\n",
            " 14 [142 142 142 142 142 142 142 142 142 141 141 141]\n",
            "Example prediction:\n",
            " 16 [159 161 163 165 165 164 165 164 163 162 160 158]\n",
            "Example prediction:\n",
            " 18 [160 162 164 165 166 165 165 165 164 163 161 158]\n",
            "Example prediction:\n",
            " 20 [160 162 163 165 166 165 165 165 164 163 160 158]\n",
            "Example prediction:\n",
            " 22 [147 148 148 148 148 148 148 148 148 147 147 146]\n",
            "Building 9 Eagle_education_Jewell\n",
            "Building 10 Eagle_office_Henriette\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_27 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_28 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_29 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [19 23 23 19 19 19 23 23 27 27 31 32]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 766.2740\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 512.2458\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 355.9739\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 244.3512\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 166.0704\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.7735\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 87.6252\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71.3687\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.7733\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58.2986\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.3490\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.3046\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.2228\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.1569\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.1690\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56.1522\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.9460\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 55.4167\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 45.6699\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43.9748\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42.5144\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 41.6581\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40.1148\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 38.7549\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37.1654\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 35.0604\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33.2812\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33.0998\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31.6022\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31.3409\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30.4464\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.8671\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.5988\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.3780\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29.7069\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.6281\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.7786\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.6658\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.2154\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.1381\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.8056\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.3333\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.3472\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.4313\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28.1272\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.9249\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.9609\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.8686\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.2305\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27.3745\n",
            "mean,rmse,rmse/mean,bldg: 29.504431664956208 5.260408793363277 0.17829215804252588 Eagle_office_Henriette\n",
            "Example prediction:\n",
            " 0 [26 27 29 30 31 32 32 33 33 33 33 32]\n",
            "Example prediction:\n",
            " 2 [29 30 31 32 32 32 32 32 31 30 29 27]\n",
            "Example prediction:\n",
            " 4 [32 32 32 31 30 29 28 26 25 24 23 22]\n",
            "Example prediction:\n",
            " 6 [32 31 30 28 26 25 23 22 21 21 21 21]\n",
            "Example prediction:\n",
            " 8 [25 24 24 24 23 23 23 24 24 26 27 28]\n",
            "Example prediction:\n",
            " 10 [22 21 20 20 21 22 23 25 27 29 31 32]\n",
            "Example prediction:\n",
            " 12 [21 21 21 22 23 25 27 28 30 32 33 35]\n",
            "Example prediction:\n",
            " 14 [22 22 23 24 26 28 30 31 33 34 35 36]\n",
            "Example prediction:\n",
            " 16 [21 23 24 26 28 30 32 33 34 35 35 35]\n",
            "Example prediction:\n",
            " 18 [30 31 32 32 33 33 33 33 32 31 30 29]\n",
            "Example prediction:\n",
            " 20 [30 31 32 32 33 33 33 32 32 31 30 29]\n",
            "Example prediction:\n",
            " 22 [32 32 33 33 33 32 32 31 30 29 28 27]\n",
            "Building 11 Eagle_health_Margo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_30 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_31 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_32 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [53 52 51 48 48 48 49 49 51 53 57 61]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 2920.8410\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2379.1325\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1991.3328\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1653.2441\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1374.1516\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1122.4592\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 901.7600\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 743.0294\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 587.8058\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 468.6994\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 368.6327\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 298.8743\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 235.8723\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 200.0140\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 170.4008\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 154.2861\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 134.5813\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 135.5832\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 126.8016\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 128.5525\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119.8027\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125.5093\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125.6196\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 124.4296\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119.6253\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 121.8086\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125.4019\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 123.9739\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 128.8978\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 124.0193\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.4810\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125.6365\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119.3694\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.8378\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 113.3952\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.4352\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 110.7326\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 108.1674\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 109.2014\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 114.5150\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105.3283\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 126.1408\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.5609\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 112.0254\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 114.9653\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 115.6723\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 110.1257\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 108.2365\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 104.2177\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 113.5714\n",
            "mean,rmse,rmse/mean,bldg: 54.25195019572018 8.106743315864048 0.14942768484115382 Eagle_health_Margo\n",
            "Example prediction:\n",
            " 0 [51 51 51 51 50 51 51 51 51 51 51 51]\n",
            "Example prediction:\n",
            " 2 [49 49 49 49 48 48 49 48 48 49 48 49]\n",
            "Example prediction:\n",
            " 4 [44 44 43 43 43 43 43 43 43 43 43 44]\n",
            "Example prediction:\n",
            " 6 [56 55 55 54 54 54 54 54 54 54 55 55]\n",
            "Example prediction:\n",
            " 8 [55 55 54 54 54 54 54 54 54 54 54 55]\n",
            "Example prediction:\n",
            " 10 [57 58 58 58 59 59 59 59 58 58 57 57]\n",
            "Example prediction:\n",
            " 12 [58 59 60 61 61 61 62 61 61 60 59 58]\n",
            "Example prediction:\n",
            " 14 [59 60 61 62 62 63 63 62 62 61 60 59]\n",
            "Example prediction:\n",
            " 16 [59 60 61 62 63 63 63 63 62 61 60 59]\n",
            "Example prediction:\n",
            " 18 [59 60 61 62 63 63 63 63 62 61 60 59]\n",
            "Example prediction:\n",
            " 20 [59 60 61 62 63 63 63 63 62 61 60 59]\n",
            "Example prediction:\n",
            " 22 [59 60 61 62 63 63 63 63 62 61 60 59]\n",
            "Building 12 Eagle_health_Reba\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_33 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_34 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_35 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [339 321 281 287 293 332 326 323 322 325 350 375]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 139957.9661\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 135654.8818\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 132005.1846\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 129574.2417\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 126298.5203\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 122762.9806\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119397.4240\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 116808.3703\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 113581.0107\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 110610.4637\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 108219.8176\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 104834.8642\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 102534.9703\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 99691.8316\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 96593.8142\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 94248.4735\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 91302.1952\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 88800.1782\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 86783.8719\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83785.9933\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 81466.3746\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 79261.0941\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 76865.9605\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 74010.9696\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 72331.0876\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 69797.5391\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67269.7864\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65354.3559\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 63357.4637\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 61123.5983\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 58827.3897\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 56716.4030\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 54905.0568\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 53032.7705\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 51294.0175\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 49157.2802\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 47764.9384\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 45943.6289\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 43964.3280\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 42530.0886\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40882.6595\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 39034.3620\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37499.7923\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 36044.6330\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 34448.4041\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32735.4804\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 31355.8238\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30193.4332\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 28867.1380\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27178.9116\n",
            "mean,rmse,rmse/mean,bldg: 405.50266427241434 228.33823545795465 0.5630992236947631 Eagle_health_Reba\n",
            "Example prediction:\n",
            " 0 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 2 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 4 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 6 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 8 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 10 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 12 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 14 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 16 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 18 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 20 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Example prediction:\n",
            " 22 [221 221 220 221 219 221 224 220 221 219 221 222]\n",
            "Building 13 Eagle_lodging_Edgardo\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_36 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_37 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_38 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [63 66 65 59 56 50 47 44 44 44 46 48]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 2570.4595\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2041.1147\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1668.5494\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1363.8772\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1113.6789\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 917.7228\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 749.6634\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 624.0502\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 507.9779\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 424.2906\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 365.6584\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 312.5901\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 285.0729\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 270.6880\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 254.1407\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 247.3424\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 243.7555\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239.2857\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 237.6917\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239.6167\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 244.3694\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 241.1834\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239.1075\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240.6441\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 240.7464\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 242.0048\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 236.9751\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 154.1924\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 121.9581\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 117.1613\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 109.2545\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 108.0051\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 106.5209\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 103.0034\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 100.6426\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 100.9403\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 105.6580\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 103.1503\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 98.7847\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 102.5325\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 100.9208\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 101.0205\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97.9198\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 99.4719\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 99.8052\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 97.0247\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 92.9103\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 98.4274\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 98.2939\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 96.0332\n",
            "mean,rmse,rmse/mean,bldg: 49.49382871584399 7.952655812660529 0.16067974571776705 Eagle_lodging_Edgardo\n",
            "Example prediction:\n",
            " 0 [58 58 59 59 59 59 59 59 58 58 58 57]\n",
            "Example prediction:\n",
            " 2 [55 56 56 56 55 55 55 55 55 55 55 55]\n",
            "Example prediction:\n",
            " 4 [43 43 43 44 44 44 44 43 43 43 43 43]\n",
            "Example prediction:\n",
            " 6 [45 45 45 45 45 44 44 44 44 44 45 45]\n",
            "Example prediction:\n",
            " 8 [50 49 48 48 47 47 47 47 47 48 49 49]\n",
            "Example prediction:\n",
            " 10 [52 51 51 50 49 49 49 49 49 50 51 52]\n",
            "Example prediction:\n",
            " 12 [53 53 52 51 50 51 50 50 51 52 53 53]\n",
            "Example prediction:\n",
            " 14 [54 54 53 52 51 51 51 51 51 52 53 54]\n",
            "Example prediction:\n",
            " 16 [55 55 54 53 53 53 53 53 53 54 55 55]\n",
            "Example prediction:\n",
            " 18 [58 58 59 59 59 59 59 59 59 59 58 58]\n",
            "Example prediction:\n",
            " 20 [60 61 62 62 63 63 63 63 63 62 61 60]\n",
            "Example prediction:\n",
            " 22 [60 61 62 63 63 64 64 64 63 62 61 60]\n",
            "Building 14 Eagle_office_Sonya\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_39 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_40 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_41 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [166 154 138 126 122 121 121 121 135 152 170 191]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 34189.1059\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 32445.2878\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30828.9033\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 29315.2618\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 27748.7779\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 26314.9306\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24850.4833\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 23601.8265\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 22290.6518\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21148.4571\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 19895.8534\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 18870.1215\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17638.8843\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 16647.4191\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15666.3466\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14673.4051\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13798.8865\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12950.0601\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12071.2719\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11325.2921\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10425.7809\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9742.6119\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9046.1060\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8376.4464\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7770.8015\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7177.7206\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6620.9453\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6173.6891\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5600.7336\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5178.5780\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4748.1709\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4284.7487\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3909.1434\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3671.7342\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3355.1526\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3050.5767\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2833.8107\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2645.2787\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2473.6321\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2228.7953\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2216.6573\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1984.3814\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1974.0256\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1798.6117\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1796.3494\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1706.1131\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1763.8512\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1650.9905\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1730.8805\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1594.3116\n",
            "mean,rmse,rmse/mean,bldg: 180.1469615779263 32.873345628215326 0.18248071097216523 Eagle_office_Sonya\n",
            "Example prediction:\n",
            " 0 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 2 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 4 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 6 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 8 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 10 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 12 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 14 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 16 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 18 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 20 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Example prediction:\n",
            " 22 [179 179 179 179 179 178 178 179 179 178 179 179]\n",
            "Building 15 Eagle_education_Cassie\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_42 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_43 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_44 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [84 87 88 88 85 85 90 90 90 88 89 91]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 3799.1508\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3333.3007\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2988.0254\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2660.2102\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 2307.6224\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2041.3589\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1790.9012\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1605.4681\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1400.1660\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1213.9094\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1062.7909\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 935.5659\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 799.7594\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 713.3831\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 623.3086\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 552.5962\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 505.0596\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 465.3671\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 423.5677\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 391.0367\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 374.8907\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 364.0454\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 355.5729\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 348.8404\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 346.8990\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 336.2172\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 345.0391\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 352.0767\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 341.8901\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 346.1674\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 345.5507\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 344.0619\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 340.4298\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 317.8475\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 122.3753\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 92.4241\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 83.3700\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 80.6862\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71.4912\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 71.0732\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 67.8614\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 69.4746\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.4153\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.2871\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 64.6552\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.1031\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.8585\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 65.6516\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 59.6611\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 62.4770\n",
            "mean,rmse,rmse/mean,bldg: 32.56340617755592 8.834740932177086 0.27130886996294523 Eagle_education_Cassie\n",
            "Example prediction:\n",
            " 0 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 2 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 4 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 6 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 8 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 10 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 12 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 14 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 16 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 18 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 20 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Example prediction:\n",
            " 22 [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Building 16 Eagle_food_Jennifer\n",
            " Count bad values before pseudofill: 159\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_45 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_46 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_47 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [95 55 51 52 51 48 49 50 52 52 57 69]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 11891.8286\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10961.5714\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10077.8698\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9543.3186\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8813.4292\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8209.7919\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7751.5704\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7305.0651\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6742.2315\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6292.6726\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5993.3840\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5605.6868\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5337.3892\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4873.0030\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4660.5333\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4282.9201\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4135.4628\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3913.3761\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3691.9988\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3517.3516\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3251.9342\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3068.6567\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3004.8995\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2814.3079\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2728.5447\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2549.2007\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2449.6072\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2355.6728\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2259.4635\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2124.9716\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2085.2210\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1983.4865\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1955.2342\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1886.4565\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1816.8823\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1803.2600\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1725.6799\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1707.1277\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1674.6778\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1675.0664\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1613.0127\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1598.9123\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1555.2536\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1503.8895\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1456.5860\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1417.9739\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1422.9188\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1367.4867\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1333.7386\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1315.2674\n",
            "mean,rmse,rmse/mean,bldg: 86.44866787268388 41.68215827030704 0.48216079317374655 Eagle_food_Jennifer\n",
            "Example prediction:\n",
            " 0 [123 122 120 118 116 111 108 104 100  96  95  95]\n",
            "Example prediction:\n",
            " 2 [113 110 107 104 100  95  92  88  84  81  80  82]\n",
            "Example prediction:\n",
            " 4 [84 81 78 76 72 69 66 63 62 60 60 62]\n",
            "Example prediction:\n",
            " 6 [68 65 61 58 56 53 52 54 54 57 60 63]\n",
            "Example prediction:\n",
            " 8 [71 69 70 68 68 70 74 78 81 87 89 93]\n",
            "Example prediction:\n",
            " 10 [ 73  72  74  73  75  78  83  87  91  96  98 101]\n",
            "Example prediction:\n",
            " 12 [ 82  85  88  91  96 100 104 109 112 115 116 114]\n",
            "Example prediction:\n",
            " 14 [ 93  96 100 104 108 113 116 120 122 125 124 122]\n",
            "Example prediction:\n",
            " 16 [138 142 146 151 154 157 158 156 154 150 147 142]\n",
            "Example prediction:\n",
            " 18 [142 146 151 155 159 161 162 159 156 151 148 143]\n",
            "Example prediction:\n",
            " 20 [140 144 148 152 155 157 157 154 150 146 142 138]\n",
            "Example prediction:\n",
            " 22 [132 134 135 137 137 136 133 129 126 121 118 115]\n",
            "Building 17 Eagle_office_Yadira\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_48 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_49 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_50 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [124 123 105  85  66  73  80  87  86 101 117 139]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 18609.9036\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 17120.5023\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15946.9702\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14949.2813\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14094.9582\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12881.6631\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 12102.1733\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 11368.1127\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 10556.1527\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9776.0534\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 9236.8013\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8498.6021\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 8044.4237\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 7371.4077\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6772.1506\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 6271.2107\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5840.2612\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5462.1960\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 5043.8073\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4701.5764\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4277.6202\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4038.0648\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3694.1436\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3563.4538\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3240.5710\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3111.0951\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 3035.8484\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2834.0260\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2710.2495\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2572.9657\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2519.0426\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2445.4278\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2373.0877\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2366.8412\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2347.0273\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2349.4142\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2314.9468\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2260.2294\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2320.9265\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2294.6093\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2263.0107\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2244.8533\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2293.3579\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2268.7329\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2281.2138\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2060.5325\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1907.2716\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1881.9085\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1819.2077\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1769.4659\n",
            "mean,rmse,rmse/mean,bldg: 130.11094808839778 40.15491576835195 0.3086205761952528 Eagle_office_Yadira\n",
            "Example prediction:\n",
            " 0 [122 123 123 123 123 124 124 123 124 123 123 122]\n",
            "Example prediction:\n",
            " 2 [110 110 110 110 109 110 109 109 110 110 109 110]\n",
            "Example prediction:\n",
            " 4 [101 101 101 100  99 100 100 100 101 101 100 101]\n",
            "Example prediction:\n",
            " 6 [98 98 98 97 96 97 97 96 97 98 97 98]\n",
            "Example prediction:\n",
            " 8 [103 103 103 103 102 103 102 102 103 103 103 103]\n",
            "Example prediction:\n",
            " 10 [118 119 119 119 119 119 119 119 119 119 118 118]\n",
            "Example prediction:\n",
            " 12 [139 140 141 142 143 143 143 143 142 142 141 139]\n",
            "Example prediction:\n",
            " 14 [142 143 144 145 146 145 146 146 145 144 143 142]\n",
            "Example prediction:\n",
            " 16 [142 143 144 145 146 146 146 146 145 144 144 142]\n",
            "Example prediction:\n",
            " 18 [142 143 144 145 146 146 146 146 145 144 144 142]\n",
            "Example prediction:\n",
            " 20 [142 143 144 145 146 145 146 146 145 144 143 142]\n",
            "Example prediction:\n",
            " 22 [140 141 142 143 143 143 144 143 143 142 141 140]\n",
            "Building 18 Eagle_education_Peter\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_51 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_52 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_53 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [1582 1558 1547 1531 1529 1535 1543 1547 1550 1564 1624 1698]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 2517684.5845\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2495456.5255\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2477452.4382\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2466989.6327\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2447092.5536\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2433077.8027\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2417987.5418\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2408731.5327\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2387040.0245\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2369664.3345\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2353607.3264\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2347955.9400\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2340949.7109\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2329911.6945\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2305098.1264\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2282019.4055\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2279182.3782\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2266038.5736\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2255500.4736\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2242013.3591\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2229567.6991\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2213974.4745\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2196895.7409\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2184580.4445\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2175406.1145\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2156523.9682\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2139097.2568\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2135982.7755\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2121830.2927\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2107750.2809\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2093871.1577\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2076782.1932\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2070391.1718\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2057008.0827\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2032249.2095\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2021482.4255\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2015963.2600\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1997330.1918\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1994224.2109\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1980778.2845\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1952282.9295\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1949433.7886\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1937135.5864\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1924893.4418\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1914363.7686\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1899424.7564\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1889020.8514\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1874605.9159\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1857147.9741\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1845883.5136\n",
            "mean,rmse,rmse/mean,bldg: 1542.9577414756814 1294.550881626579 0.8390060510590999 Eagle_education_Peter\n",
            "Example prediction:\n",
            " 0 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 2 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 4 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 6 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 8 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 10 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 12 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 14 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 16 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 18 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 20 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Example prediction:\n",
            " 22 [232 230 233 230 232 232 231 233 233 232 233 234]\n",
            "Building 19 Eagle_health_Gregoria\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_54 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_55 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_56 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [631 622 627 608 597 577 578 581 599 617 640 656]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 290155.7509\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 281788.8306\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 276678.4165\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 271668.6638\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 268856.1013\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 264799.1297\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 260821.1718\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 255224.5357\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 249813.3035\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 248049.6737\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 242617.9373\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 239515.8981\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 234931.3209\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 231283.2703\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 227408.0441\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 223027.6349\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 219123.3619\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 217125.4483\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 212770.4653\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 209584.0683\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 205574.9981\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 201638.9028\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 199283.2625\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 193601.2964\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 189932.1714\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 188065.9172\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183785.7434\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 181069.9602\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 176251.3347\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 173474.6101\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 170517.1630\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 167440.8343\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 163801.5726\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 161031.1515\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 157924.6966\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 153536.0222\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 150879.5161\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 147527.3702\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 145523.5209\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 143061.8544\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 140075.1870\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 136842.2858\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 132978.4131\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 130262.3179\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 128600.2487\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125666.1603\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 121915.4201\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 119445.5254\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 115343.9866\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 113809.0702\n",
            "mean,rmse,rmse/mean,bldg: 431.8125411355521 287.00996065646115 0.6646633279841787 Eagle_health_Gregoria\n",
            "Example prediction:\n",
            " 0 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 2 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 4 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 6 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 8 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 10 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 12 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 14 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 16 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 18 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 20 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Example prediction:\n",
            " 22 [227 226 227 226 227 225 226 225 227 227 225 225]\n",
            "Building 20 Eagle_lodging_Dawn\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_57 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_58 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_59 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [97 96 91 89 85 79 74 70 70 70 70 72]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 5484.4993\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4763.5619\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4178.8520\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 3662.8695\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3219.1879\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2821.6763\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 2435.7597\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2093.6776\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1807.7542\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1552.7359\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1335.0237\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1143.5636\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 980.4689\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 850.1309\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 742.6910\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 647.7986\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 586.4048\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 525.4148\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 483.2847\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 459.7637\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 414.8084\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 404.1315\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 400.8371\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 386.5447\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 393.1321\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 402.6529\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 398.1461\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 412.4584\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 418.3954\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 363.0039\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 209.4206\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 192.6474\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 165.6487\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 161.1106\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 149.9872\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 147.6309\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 159.1124\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 148.8080\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 145.0826\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 146.3095\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154.1193\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 144.1170\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 143.7629\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 145.9727\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 151.9675\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 144.7978\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 153.5533\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 150.2899\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 153.7034\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 144.1118\n",
            "mean,rmse,rmse/mean,bldg: 73.15755016531799 9.672535344880677 0.1322151346378212 Eagle_lodging_Dawn\n",
            "Example prediction:\n",
            " 0 [86 87 88 89 89 90 90 89 89 88 87 86]\n",
            "Example prediction:\n",
            " 2 [85 85 85 86 86 86 86 86 85 85 84 84]\n",
            "Example prediction:\n",
            " 4 [61 61 60 60 60 60 60 60 61 61 61 62]\n",
            "Example prediction:\n",
            " 6 [62 62 61 61 61 61 61 61 61 62 62 62]\n",
            "Example prediction:\n",
            " 8 [72 71 71 71 71 70 70 70 71 71 72 72]\n",
            "Example prediction:\n",
            " 10 [73 72 72 71 71 71 71 71 71 72 72 73]\n",
            "Example prediction:\n",
            " 12 [72 72 71 71 71 70 70 70 71 71 72 72]\n",
            "Example prediction:\n",
            " 14 [74 73 73 73 73 72 72 72 73 73 74 74]\n",
            "Example prediction:\n",
            " 16 [79 78 78 77 77 77 77 77 77 78 78 79]\n",
            "Example prediction:\n",
            " 18 [86 87 88 88 88 89 89 88 88 87 86 85]\n",
            "Example prediction:\n",
            " 20 [87 88 89 90 90 90 91 90 89 89 87 86]\n",
            "Example prediction:\n",
            " 22 [87 88 89 90 90 91 91 90 90 89 88 86]\n",
            "Building 21 Eagle_education_Shanna\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_60 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_61 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_62 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [398 377 350 335 335 340 340 363 383 404 409 423]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 86338.9869\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 83320.7700\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 80885.9888\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78445.8479\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75276.1946\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 73121.0871\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71036.6500\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 68516.0484\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 66122.3047\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 64175.4816\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 61651.2014\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 60099.1766\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 58308.2827\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 55462.8743\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 53850.2402\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 51875.1055\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 50322.7903\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 48196.9458\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 46390.4544\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44924.1302\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 42989.7787\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 40869.0522\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 39530.1328\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 37843.2922\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 36613.7294\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 34808.7680\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 33054.8969\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 32251.9804\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 30533.5913\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 29500.3398\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 28542.5385\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27021.4811\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25279.9505\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 24363.5706\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23026.5438\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22337.1103\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 21143.1583\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 20166.0577\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19001.9104\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18210.6311\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 16693.6843\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 15829.8722\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15044.6020\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 14485.7056\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 13535.9495\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12864.9837\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12193.9832\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 11581.6817\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10926.9676\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10348.8370\n",
            "mean,rmse,rmse/mean,bldg: 294.6100026602838 102.65543327104707 0.348445172750701 Eagle_education_Shanna\n",
            "Example prediction:\n",
            " 0 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 2 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 4 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 6 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 8 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 10 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 12 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 14 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 16 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 18 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 20 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Example prediction:\n",
            " 22 [213 214 212 214 215 214 212 214 215 214 215 215]\n",
            "Building 22 Eagle_office_Nereida\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_63 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_64 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_65 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [158 157 154 151 147 146 145 144 146 152 171 190]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 25847.8750\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24272.2322\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22687.5016\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21526.4794\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20363.7830\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 19257.8315\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18055.4136\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17000.9847\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 15932.8641\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15101.5173\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14137.4310\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13212.6635\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12381.5008\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11542.8323\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10939.6658\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10122.1430\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9360.4041\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8563.8734\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7966.9540\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7339.2047\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6778.5461\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6233.1848\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5776.2092\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5277.4622\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 4765.8015\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4389.9445\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4042.2932\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3674.8378\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3378.0681\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 3112.1667\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2771.8489\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2564.5878\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 2372.4201\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2216.2361\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2041.3638\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1840.5015\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1720.9619\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1633.9482\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1593.1077\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1519.3920\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 1511.8935\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1407.3357\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1381.2421\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1365.2724\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 1354.8187\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1407.8612\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1377.2098\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1358.4808\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1348.1236\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1316.5698\n",
            "mean,rmse,rmse/mean,bldg: 156.17757723938658 29.777248214760778 0.19066276184524666 Eagle_office_Nereida\n",
            "Example prediction:\n",
            " 0 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 2 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 4 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 6 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 8 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 10 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 12 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 14 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 16 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 18 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 20 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Example prediction:\n",
            " 22 [162 162 162 162 162 162 162 162 162 162 162 162]\n",
            "Building 23 Eagle_health_Lucinda\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_66 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_67 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_68 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [456 468 479 491 494 490 492 493 489 479 474 481]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 314299.0857\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 307788.3458\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 302753.0657\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 297072.6208\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 292523.2401\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 287366.6257\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 282932.9225\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 277809.0372\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 273834.3220\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 267743.2103\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 263425.6282\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 259585.3390\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 254786.8416\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 249637.0754\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 246062.0600\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 240901.8666\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 237167.6161\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 233006.8026\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 228195.1604\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 223966.9184\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 219040.5776\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 215401.6835\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 212101.6143\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 206893.0523\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 203714.4212\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 199570.9209\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 195099.0377\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 192012.1491\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 187770.1764\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183599.6495\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 179840.0748\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 176166.4353\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 173346.5347\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 168889.1536\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 165230.9107\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 161530.7830\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 159133.3137\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155626.9323\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 151002.5580\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 147941.4705\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 144680.5977\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 142359.6568\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 138095.1314\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 135204.7668\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 131452.0302\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 128691.2753\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 125234.2734\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 121854.9715\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 119387.0207\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 116364.5286\n",
            "mean,rmse,rmse/mean,bldg: 567.8485090370059 358.74074556366105 0.631754314494964 Eagle_health_Lucinda\n",
            "Example prediction:\n",
            " 0 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 2 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 4 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 6 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 8 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 10 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 12 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 14 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 16 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 18 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 20 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Example prediction:\n",
            " 22 [226 227 228 228 227 226 225 226 224 226 226 226]\n",
            "Building 24 Eagle_assembly_Noel\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_69 (GRU)                 (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_70 (GRU)                 (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_71 (GRU)                 (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2935.8093\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3145.0042\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3099.0133\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3268.7425\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2638.8377\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2598.7322\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2175.1444\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2822.6497\n",
            "mean,rmse,rmse/mean,bldg: 144.15172322046277 101.45146239589454 0.7037825155980737 Eagle_assembly_Latrina\n",
            "Example prediction:\n",
            " 0 [115 116 116 115 115 114 115 115 115 116 115 115]\n",
            "Example prediction:\n",
            " 2 [118 119 119 118 118 117 118 118 118 119 118 118]\n",
            "Example prediction:\n",
            " 4 [109 109 109 109 108 108 109 108 108 109 108 109]\n",
            "Example prediction:\n",
            " 6 [ 99 100 100  99  99  98  99  99  99 100  99  99]\n",
            "Example prediction:\n",
            " 8 [104 104 104 104 104 103 104 103 104 104 104 104]\n",
            "Example prediction:\n",
            " 10 [104 104 105 104 104 103 104 103 104 104 104 104]\n",
            "Example prediction:\n",
            " 12 [106 106 106 105 105 104 106 105 105 106 105 106]\n",
            "Example prediction:\n",
            " 14 [106 106 106 105 105 104 105 105 105 106 105 106]\n",
            "Example prediction:\n",
            " 16 [106 106 106 105 105 105 106 105 106 106 105 106]\n",
            "Example prediction:\n",
            " 18 [107 107 107 106 106 105 106 106 106 107 106 107]\n",
            "Example prediction:\n",
            " 20 [108 109 109 108 108 107 108 108 108 109 108 108]\n",
            "Example prediction:\n",
            " 22 [109 109 109 109 108 108 109 108 109 109 108 109]\n",
            "Building 71 Eagle_public_Ola\n",
            " Count bad values before pseudofill: 394\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_68\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_204 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_205 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_206 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [140 130 108  86  72  72  72  73  73  73  91 113]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 21011.9294\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19385.8231\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18166.1551\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16930.7877\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15901.5336\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14849.8960\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13747.3077\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12854.5128\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12018.4571\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11147.7686\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10345.9708\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9515.3545\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8752.5878\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8164.5623\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7421.0482\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6846.3906\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6257.3769\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5691.0131\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5199.9628\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4726.5547\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4300.0029\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3840.8296\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3491.3070\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3188.9469\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2829.0098\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2620.2587\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2334.3323\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2094.8166\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1887.0888\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1717.7488\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1591.9870\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1431.2216\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1304.7635\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1249.1193\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1176.1431\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1127.1329\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1085.4527\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1025.8905\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1006.5793\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1014.7114\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 992.1885\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 963.5091\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 7ms/step - loss: 1005.0141\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 985.8188\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 995.4719\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 995.8424\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 993.8154\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 980.4700\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 983.9445\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1010.4078\n",
            "mean,rmse,rmse/mean,bldg: 130.38178561709088 43.005289758750884 0.3298412393664411 Eagle_public_Ola\n",
            "Example prediction:\n",
            " 0 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 2 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 4 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 6 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 8 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 10 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 12 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 14 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 16 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 18 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 20 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Example prediction:\n",
            " 22 [142 142 142 142 142 142 142 142 142 142 142 142]\n",
            "Building 72 Eagle_public_Henry\n",
            " Count bad values before pseudofill: 178\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_207 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_208 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_209 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [57 57 56 55 54 54 56 57 57 55 54 57]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 1146.7248\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 892.7634\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 714.5476\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 607.3095\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 499.4030\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 444.8273\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 406.1113\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 374.5584\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 360.7608\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 357.5211\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 362.1113\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 358.7903\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 353.0878\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 356.7775\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 349.4645\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 229.9435\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 182.6864\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 152.4175\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 139.6035\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 130.4208\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.0255\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.9477\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 115.4205\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113.4493\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113.0451\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.4198\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.8311\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108.7849\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 111.6241\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108.5299\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.3324\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 106.8186\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 107.5391\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.8430\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 104.4775\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 106.1610\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.5823\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 102.0013\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 106.9314\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 104.0112\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.9600\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.2152\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105.3877\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 101.0166\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 98.9814\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105.7574\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 101.2624\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 104.0001\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 100.7913\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 100.2768\n",
            "mean,rmse,rmse/mean,bldg: 29.40429575031662 9.756912662301456 0.3318192941994333 Eagle_public_Henry\n",
            "Example prediction:\n",
            " 0 [21 22 22 21 20 19 18 16 15 14 13 13]\n",
            "Example prediction:\n",
            " 2 [20 20 20 19 18 17 16 14 13 13 12 12]\n",
            "Example prediction:\n",
            " 4 [19 19 19 18 16 15 14 12 12 11 11 12]\n",
            "Example prediction:\n",
            " 6 [19 19 19 18 18 18 19 19 20 21 21 22]\n",
            "Example prediction:\n",
            " 8 [17 16 17 17 18 19 20 21 22 23 24 24]\n",
            "Example prediction:\n",
            " 10 [16 16 17 18 20 21 22 23 24 24 24 24]\n",
            "Example prediction:\n",
            " 12 [30 30 30 30 31 32 33 34 34 34 35 35]\n",
            "Example prediction:\n",
            " 14 [27 27 28 28 29 29 30 30 31 31 31 31]\n",
            "Example prediction:\n",
            " 16 [29 29 29 30 30 30 31 31 31 31 31 31]\n",
            "Example prediction:\n",
            " 18 [32 33 33 33 33 33 33 33 33 33 32 32]\n",
            "Example prediction:\n",
            " 20 [35 35 35 35 35 35 35 35 35 35 35 35]\n",
            "Example prediction:\n",
            " 22 [29 29 30 30 30 30 31 30 30 30 30 30]\n",
            "Building 73 Eagle_education_Norah\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_70\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_210 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_211 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_212 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [457 461 460 458 453 452 454 456 455 458 457 460]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 235032.9231\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 230277.0299\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 224979.5868\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 221105.7913\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 216027.4124\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 212918.4397\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 208829.4752\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 204524.0469\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 200734.7270\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 195874.6797\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 192174.3890\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 188318.8952\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 184203.5050\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 180727.1595\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 176417.0564\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 172856.9334\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 169633.6913\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 165145.7816\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162072.9154\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 158691.6963\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154939.5118\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 151887.3450\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 148198.7619\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 144029.9561\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 141461.8520\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 137945.1188\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 134814.6160\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 131269.6795\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 128400.4789\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 125179.5493\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 122347.4639\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119123.6600\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 116148.4298\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113240.8259\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 110062.1359\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 107134.8002\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 104335.1649\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 101856.9360\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 99041.6859\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96298.6542\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 93536.9908\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 90922.5679\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88171.0705\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85903.8232\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 83486.5369\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 80888.3449\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78288.6132\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75728.3349\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 73411.2049\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71099.4303\n",
            "mean,rmse,rmse/mean,bldg: 492.3250864591599 277.7130332568157 0.5640846686366306 Eagle_education_Norah\n",
            "Example prediction:\n",
            " 0 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 2 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 4 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 6 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 8 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 10 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 12 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 14 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 16 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 18 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 20 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Example prediction:\n",
            " 22 [225 228 224 223 224 228 224 226 224 225 224 225]\n",
            "Building 74 Eagle_education_Will\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_71\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_213 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_214 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_215 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [121 120 122 120 122 121 123 123 127 134 140 143]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 12389.4429\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11211.7816\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10247.7173\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9346.8865\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8574.2399\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7755.5276\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7023.6219\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6360.4190\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5756.5459\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5166.0272\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4635.7379\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4140.7832\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3643.4038\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3218.8451\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2825.6369\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2456.1361\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2130.4516\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1835.6902\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1558.1581\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1338.6534\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1105.4754\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 935.8759\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 785.0303\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 639.8396\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 516.9230\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 440.2535\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 345.6962\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 296.3740\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 241.5991\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 210.7974\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183.4897\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 172.8860\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 169.7175\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 172.2065\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154.8339\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157.5337\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 158.4193\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 161.7367\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 158.7508\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 148.5570\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162.7308\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157.5032\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162.4002\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 149.7466\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 159.5193\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 163.5682\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155.2898\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157.8338\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 151.6923\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 7ms/step - loss: 156.6293\n",
            "mean,rmse,rmse/mean,bldg: 112.097810492912 11.603943140492692 0.10351623363086485 Eagle_education_Will\n",
            "Example prediction:\n",
            " 0 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 2 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 4 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 6 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 8 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 10 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 12 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 14 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 16 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 18 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 20 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Example prediction:\n",
            " 22 [112 112 112 112 112 112 112 112 112 112 112 112]\n",
            "Building 75 Eagle_lodging_Stephanie\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_72\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_216 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_217 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_218 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [85 87 85 83 79 76 71 69 68 69 71 77]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 6095.8766\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5320.9845\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4657.4578\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4096.0701\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3584.5575\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3156.2771\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2721.7104\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2381.6016\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2036.4848\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1755.8280\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1506.1240\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1264.0036\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1079.8199\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 919.7553\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 767.0820\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 646.5166\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 549.7112\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 478.5148\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 424.3833\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 383.4035\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 358.8373\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 331.8312\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 314.6732\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 313.5136\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 300.8053\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 289.9644\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 303.7392\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 291.8905\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 293.6880\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 308.6277\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 300.5405\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 293.3252\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 304.0048\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 306.8491\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 221.6214\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 190.6833\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 181.8540\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.9242\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.2618\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 166.4961\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 158.8113\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162.9055\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 163.7174\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155.3353\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160.5665\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 150.1866\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157.0759\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 173.7445\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155.2183\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 168.1919\n",
            "mean,rmse,rmse/mean,bldg: 78.38750807585582 10.606711535114906 0.13531124786937684 Eagle_lodging_Stephanie\n",
            "Example prediction:\n",
            " 0 [86 86 87 87 87 87 87 87 87 86 86 86]\n",
            "Example prediction:\n",
            " 2 [82 82 81 81 81 81 81 81 81 81 81 82]\n",
            "Example prediction:\n",
            " 4 [64 63 63 62 62 62 62 62 63 63 64 65]\n",
            "Example prediction:\n",
            " 6 [67 66 66 65 65 65 65 65 65 66 67 67]\n",
            "Example prediction:\n",
            " 8 [71 70 70 69 69 69 69 69 70 70 71 71]\n",
            "Example prediction:\n",
            " 10 [72 72 71 71 70 70 70 71 71 71 72 72]\n",
            "Example prediction:\n",
            " 12 [75 74 74 73 73 73 73 73 73 74 74 75]\n",
            "Example prediction:\n",
            " 14 [76 76 75 75 75 74 75 75 75 75 76 76]\n",
            "Example prediction:\n",
            " 16 [82 82 81 81 81 81 81 81 81 81 81 82]\n",
            "Example prediction:\n",
            " 18 [90 91 92 93 93 94 93 93 92 91 90 89]\n",
            "Example prediction:\n",
            " 20 [90 91 92 93 94 94 94 93 93 91 90 89]\n",
            "Example prediction:\n",
            " 22 [90 91 92 93 93 94 93 93 92 91 90 89]\n",
            "Building 76 Eagle_assembly_Candice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_219 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_220 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_221 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [14 14 14 14 14 14 14 14 14 14 14 14]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 3010.7319\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2545.1477\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2319.6762\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2156.2455\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2010.2308\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1926.4697\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1799.4098\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1831.5638\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1758.2072\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1697.9045\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1700.2218\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1733.7015\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1680.8499\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1702.3353\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1709.4740\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1669.1402\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1692.2017\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1683.5239\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1716.8377\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1654.1425\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1686.1454\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1719.4026\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1680.3331\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1661.8099\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1663.5920\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1683.5373\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1656.3342\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1619.8735\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1613.5763\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1587.0521\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1562.8889\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1600.3093\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1598.9837\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1578.1331\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1554.8497\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1515.8754\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1573.4824\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1546.5401\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1498.1729\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1508.4786\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1519.4710\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1535.6885\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1511.6383\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1509.3282\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1511.1822\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1492.9140\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1524.1554\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1459.4863\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1464.1517\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1466.3792\n",
            "mean,rmse,rmse/mean,bldg: 39.08455422034735 42.58553344781204 1.0895744955341484 Eagle_assembly_Candice\n",
            "Example prediction:\n",
            " 0 [37 38 38 37 35 31 27 24 22 22 22 24]\n",
            "Example prediction:\n",
            " 2 [33 33 33 32 30 27 24 22 20 20 20 22]\n",
            "Example prediction:\n",
            " 4 [23 22 22 21 20 18 18 18 18 19 20 21]\n",
            "Example prediction:\n",
            " 6 [23 23 22 22 21 21 21 22 22 23 24 25]\n",
            "Example prediction:\n",
            " 8 [31 30 29 29 29 31 33 35 37 38 38 39]\n",
            "Example prediction:\n",
            " 10 [36 34 35 36 38 42 46 49 51 52 51 48]\n",
            "Example prediction:\n",
            " 12 [40 39 40 42 44 48 51 54 55 55 54 51]\n",
            "Example prediction:\n",
            " 14 [51 51 52 53 55 57 58 59 58 57 56 53]\n",
            "Example prediction:\n",
            " 16 [55 56 57 58 60 61 61 60 59 58 56 54]\n",
            "Example prediction:\n",
            " 18 [55 56 57 58 60 61 61 60 59 58 56 54]\n",
            "Example prediction:\n",
            " 20 [55 56 57 58 60 61 61 60 59 58 56 54]\n",
            "Example prediction:\n",
            " 22 [53 55 55 56 56 55 52 50 48 46 45 45]\n",
            "Building 77 Eagle_public_Missy\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_74\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_222 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_223 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_224 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [30 30 29 25 22 18 18 17 19 23 27 28]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 967.6762\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 699.6208\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 513.1373\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 378.2640\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 274.9211\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 206.5468\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 170.2831\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 137.4530\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 127.7314\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 115.9425\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.7628\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.4861\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 111.6946\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 111.1698\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113.7979\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.3022\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 107.4984\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 109.9357\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 110.1669\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 112.2022\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 112.9135\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.9782\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 110.8694\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105.6312\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96.8954\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94.8124\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.9936\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97.7535\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.3257\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94.1902\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 91.2795\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88.5598\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87.9472\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88.8768\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 86.8929\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.8469\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.9373\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78.6472\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79.0441\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 77.4215\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75.1104\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74.8462\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.2291\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.3768\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 73.2607\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.3217\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.1767\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67.9557\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.1437\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 68.0269\n",
            "mean,rmse,rmse/mean,bldg: 30.839050634667256 7.935515949265016 0.25732037095669946 Eagle_public_Missy\n",
            "Example prediction:\n",
            " 0 [32 33 34 34 34 33 32 30 28 26 24 24]\n",
            "Example prediction:\n",
            " 2 [35 36 35 35 33 31 28 25 23 21 20 20]\n",
            "Example prediction:\n",
            " 4 [33 33 31 29 27 24 21 19 18 17 18 19]\n",
            "Example prediction:\n",
            " 6 [36 34 32 29 26 23 21 20 20 21 23 26]\n",
            "Example prediction:\n",
            " 8 [31 28 26 23 21 20 20 22 23 25 28 30]\n",
            "Example prediction:\n",
            " 10 [25 23 21 20 20 21 23 25 28 30 32 33]\n",
            "Example prediction:\n",
            " 12 [22 21 19 20 21 23 26 28 30 32 33 33]\n",
            "Example prediction:\n",
            " 14 [21 20 20 21 23 26 28 30 32 34 34 34]\n",
            "Example prediction:\n",
            " 16 [24 24 25 26 28 29 31 32 33 34 33 33]\n",
            "Example prediction:\n",
            " 18 [29 29 30 30 31 31 32 32 32 32 32 32]\n",
            "Example prediction:\n",
            " 20 [28 29 29 30 31 32 32 32 33 33 32 32]\n",
            "Example prediction:\n",
            " 22 [29 29 30 31 32 32 32 32 32 32 31 30]\n",
            "Building 78 Eagle_lodging_Blake\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_75\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_225 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_226 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_227 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [18 19 18 17 15 14 13 13 12 12 12 13]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 387.6134\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 238.4848\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 163.4290\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 125.2017\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.2488\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.6678\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.8286\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87.4268\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88.0234\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88.0944\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.6759\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 88.9783\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 53.2142\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 37.8412\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 32.4481\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29.2052\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 26.1855\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24.6314\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23.8056\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22.4878\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21.5573\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21.0563\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20.7657\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20.0054\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20.3605\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20.2351\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19.6367\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18.6010\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18.1607\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.2668\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.6732\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.3680\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.0073\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.9762\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.7420\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.4222\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.1825\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.8781\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.0825\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.1918\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.7096\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.5541\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.2340\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.6999\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13.9449\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.5138\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.2102\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14.1708\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13.8655\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13.9969\n",
            "mean,rmse,rmse/mean,bldg: 13.556560449207657 10.455758345682543 0.7712692599909186 Eagle_lodging_Blake\n",
            "Example prediction:\n",
            " 0 [18 19 21 22 22 23 22 21 19 18 16 16]\n",
            "Example prediction:\n",
            " 2 [19 19 19 18 18 17 15 14 13 13 12 12]\n",
            "Example prediction:\n",
            " 4 [16 16 15 14 13 12 10 10  9 10  9 10]\n",
            "Example prediction:\n",
            " 6 [17 17 16 15 14 13 12 12 11 12 12 13]\n",
            "Example prediction:\n",
            " 8 [11 11 11 12 13 13 14 15 16 16 17 17]\n",
            "Example prediction:\n",
            " 10 [10 10 11 11 12 14 15 16 18 19 19 19]\n",
            "Example prediction:\n",
            " 12 [ 9  9 10 11 13 15 16 17 18 19 19 19]\n",
            "Example prediction:\n",
            " 14 [15 15 15 17 18 19 20 22 23 24 24 25]\n",
            "Example prediction:\n",
            " 16 [16 16 17 18 19 21 22 24 25 25 25 26]\n",
            "Example prediction:\n",
            " 18 [19 19 20 21 22 23 25 26 27 27 27 27]\n",
            "Example prediction:\n",
            " 20 [21 21 21 22 23 24 26 27 28 28 28 28]\n",
            "Example prediction:\n",
            " 22 [22 22 23 24 25 26 27 28 29 29 29 28]\n",
            "Building 79 Eagle_education_Petra\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_76\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_228 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_229 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_230 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [65 65 65 66 65 64 64 64 64 64 65 67]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 3967.1044\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3330.5347\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2839.1736\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2395.8856\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2030.0093\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1695.9147\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1408.1926\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1159.8585\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 947.4131\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 766.7568\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 609.7858\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 489.2282\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 384.8765\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 309.4745\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 250.5811\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 200.9229\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 164.5825\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 142.7443\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 130.1587\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.6533\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.6546\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.1300\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.5212\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113.5005\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.6185\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 112.7404\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108.1423\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 118.4324\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108.2491\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108.6958\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114.6332\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 125.1358\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96.9095\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94.9964\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105.1753\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 98.0788\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96.4447\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94.2954\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.4856\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.2285\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 90.1195\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87.0918\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 100.5608\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.2640\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97.4438\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92.8938\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.0411\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.5970\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.4789\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 86.5056\n",
            "mean,rmse,rmse/mean,bldg: 66.23134952304954 7.616655286243892 0.11500075630488515 Eagle_education_Petra\n",
            "Example prediction:\n",
            " 0 [63 63 63 63 63 63 63 63 63 63 63 63]\n",
            "Example prediction:\n",
            " 2 [63 63 62 62 62 62 62 62 62 62 62 63]\n",
            "Example prediction:\n",
            " 4 [34 34 34 33 35 34 34 35 37 36 37 38]\n",
            "Example prediction:\n",
            " 6 [49 50 49 49 49 49 49 50 50 50 51 51]\n",
            "Example prediction:\n",
            " 8 [62 62 61 61 61 61 61 61 61 61 62 62]\n",
            "Example prediction:\n",
            " 10 [63 62 62 62 62 62 62 62 62 62 62 62]\n",
            "Example prediction:\n",
            " 12 [63 63 63 63 62 62 62 62 62 63 63 63]\n",
            "Example prediction:\n",
            " 14 [64 63 63 63 63 63 63 63 63 63 63 63]\n",
            "Example prediction:\n",
            " 16 [65 65 65 65 65 65 65 65 65 65 65 64]\n",
            "Example prediction:\n",
            " 18 [66 66 67 67 67 67 67 67 67 66 66 66]\n",
            "Example prediction:\n",
            " 20 [65 66 66 66 66 66 66 66 66 65 65 65]\n",
            "Example prediction:\n",
            " 22 [64 64 64 64 64 64 64 64 64 64 64 64]\n",
            "Building 80 Eagle_lodging_Terri\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_231 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_232 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_233 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [195 209 212 215 205 209 227 243 254 244 236 225]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 11387.3481\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10527.3671\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9650.6952\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9027.8974\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8199.0567\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7323.4610\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6982.7798\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6431.8669\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5874.1687\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5368.3477\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5088.7832\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4543.8053\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4281.0192\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3923.2764\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3519.3605\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3195.9087\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2855.9579\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2649.6068\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2430.6724\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2319.0563\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2124.8741\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1952.7419\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1823.8773\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1813.3494\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1702.9250\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1590.4996\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1478.4099\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1400.5101\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1325.0498\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1248.6996\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1203.4248\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1062.6046\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1066.3163\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1016.4525\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1027.0209\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 979.6822\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 878.6680\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 870.9409\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 842.0843\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 797.1272\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 753.4973\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 711.8220\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 721.0289\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 696.8672\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 700.1161\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 679.9507\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 666.3028\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 636.2604\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 615.2901\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 638.6864\n",
            "mean,rmse,rmse/mean,bldg: 103.77673837266767 21.02921999046517 0.2026390530308261 Eagle_lodging_Terri\n",
            "Example prediction:\n",
            " 0 [156 157 156 155 156 155 156 155 155 155 154 155]\n",
            "Example prediction:\n",
            " 2 [156 156 156 155 156 155 155 154 155 155 154 155]\n",
            "Example prediction:\n",
            " 4 [134 135 134 134 134 134 133 133 134 133 133 133]\n",
            "Example prediction:\n",
            " 6 [127 128 127 127 127 127 126 126 127 126 126 126]\n",
            "Example prediction:\n",
            " 8 [136 136 135 135 136 135 135 134 135 134 134 134]\n",
            "Example prediction:\n",
            " 10 [136 136 135 136 136 135 135 135 135 135 134 135]\n",
            "Example prediction:\n",
            " 12 [131 131 130 130 130 130 130 129 130 130 129 129]\n",
            "Example prediction:\n",
            " 14 [121 121 120 121 121 121 120 120 120 120 120 120]\n",
            "Example prediction:\n",
            " 16 [132 132 131 132 132 131 131 131 131 131 130 131]\n",
            "Example prediction:\n",
            " 18 [139 139 138 138 139 138 138 137 138 137 137 137]\n",
            "Example prediction:\n",
            " 20 [134 134 133 134 134 133 133 133 133 133 132 133]\n",
            "Example prediction:\n",
            " 22 [129 130 129 129 129 129 128 128 129 129 128 128]\n",
            "Building 81 Eagle_lodging_Trina\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_78\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_234 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_235 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_236 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [61 57 57 56 52 49 47 46 43 42 43 44]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 2228.2513\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1789.4223\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1448.9607\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1173.5995\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 942.6397\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 773.7030\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 620.2224\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 515.4092\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 413.7177\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 350.0130\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 301.6084\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 267.6186\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 247.9713\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 234.1735\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 221.0029\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 220.1167\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 212.4527\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 217.9165\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 217.0747\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 170.5819\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 115.6676\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103.5942\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97.3548\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97.3742\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 93.2731\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.4848\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87.0870\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84.5056\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 82.9300\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 83.7633\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84.6918\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 86.3952\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.7044\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.7980\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79.7795\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84.4580\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79.4498\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 82.7816\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 82.6678\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 80.6771\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 83.0526\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.1379\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87.2061\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 86.0375\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.0799\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79.9891\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 80.2754\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84.0203\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.6811\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 80.8230\n",
            "mean,rmse,rmse/mean,bldg: 46.79853874510712 7.310360326364486 0.156209157858135 Eagle_lodging_Trina\n",
            "Example prediction:\n",
            " 0 [56 56 56 57 57 57 56 56 56 56 56 55]\n",
            "Example prediction:\n",
            " 2 [55 54 54 54 54 54 53 53 53 53 54 54]\n",
            "Example prediction:\n",
            " 4 [38 37 37 37 37 37 36 36 37 37 37 37]\n",
            "Example prediction:\n",
            " 6 [42 41 41 40 40 40 40 40 40 41 41 41]\n",
            "Example prediction:\n",
            " 8 [46 45 45 44 44 44 44 44 44 45 45 46]\n",
            "Example prediction:\n",
            " 10 [46 46 45 45 44 44 44 45 45 45 45 46]\n",
            "Example prediction:\n",
            " 12 [46 45 45 44 44 44 44 44 44 45 45 46]\n",
            "Example prediction:\n",
            " 14 [46 46 45 45 45 44 45 45 45 45 46 46]\n",
            "Example prediction:\n",
            " 16 [52 51 51 51 50 50 50 50 50 50 51 52]\n",
            "Example prediction:\n",
            " 18 [57 57 58 58 59 59 59 58 58 58 57 56]\n",
            "Example prediction:\n",
            " 20 [58 59 59 60 61 61 61 61 60 60 58 57]\n",
            "Example prediction:\n",
            " 22 [58 59 60 61 62 62 62 61 61 60 59 57]\n",
            "Building 82 Eagle_education_Maragret\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_237 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_238 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_239 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [14 12 10 10 10 10 10 10 10 10 12 14]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 399.9238\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 230.2953\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 137.7528\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.2593\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 55.3825\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 42.4712\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 35.9469\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 34.1416\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33.7439\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33.9648\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 32.8540\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33.6477\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33.9895\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33.4695\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 34.1524\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20.1390\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.6514\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18.0001\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.8180\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.6991\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.2626\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18.2891\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.2025\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.5536\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.0863\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.5189\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.6920\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.7823\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.7268\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.8597\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.5549\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18.0025\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.5264\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.8138\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.1154\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17.0159\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.7872\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.6972\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.4462\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.4180\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.5863\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.7887\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16.9662\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.5697\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.9660\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.6898\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.1792\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.9626\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.8260\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15.7233\n",
            "mean,rmse,rmse/mean,bldg: 21.787646578102155 2.7948449061790615 0.12827658536503167 Eagle_education_Maragret\n",
            "Example prediction:\n",
            " 0 [23 23 23 24 23 23 23 23 23 23 23 23]\n",
            "Example prediction:\n",
            " 2 [23 23 23 23 23 23 23 23 23 23 23 23]\n",
            "Example prediction:\n",
            " 4 [16 16 16 15 14 14 13 13 13 12 12 11]\n",
            "Example prediction:\n",
            " 6 [22 21 21 20 19 19 18 17 16 16 15 15]\n",
            "Example prediction:\n",
            " 8 [22 22 22 22 22 22 22 22 22 22 22 22]\n",
            "Example prediction:\n",
            " 10 [21 21 21 22 22 22 22 22 23 23 23 23]\n",
            "Example prediction:\n",
            " 12 [21 21 21 21 22 22 22 22 23 23 23 23]\n",
            "Example prediction:\n",
            " 14 [18 18 18 19 20 20 21 22 22 23 23 23]\n",
            "Example prediction:\n",
            " 16 [23 23 23 23 23 23 23 23 23 23 23 23]\n",
            "Example prediction:\n",
            " 18 [24 24 24 25 25 25 25 25 25 24 24 24]\n",
            "Example prediction:\n",
            " 20 [24 24 24 24 24 24 24 24 24 24 24 23]\n",
            "Example prediction:\n",
            " 22 [23 23 23 23 23 23 23 23 23 23 23 23]\n",
            "Building 83 Eagle_office_Lillian\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_80\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_240 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_241 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_242 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [135 128 118 113 113 115 115 123 130 137 139 143]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 6980.7445\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6249.0183\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5550.9611\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5050.3028\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4568.8538\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4131.0510\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3672.6184\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3379.2205\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3023.0139\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2675.1006\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2499.0944\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2267.3427\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2015.7049\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1857.7391\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1728.4631\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1570.6991\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1450.1054\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1137.0864\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 963.0046\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 847.3209\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 765.3212\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 668.1533\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 617.9492\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 562.6835\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 495.0568\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 480.6837\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 473.0707\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 473.8532\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 434.4293\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 408.4532\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 405.0108\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 384.1112\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 378.7869\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 363.8660\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 375.0899\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 314.3445\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 373.7329\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 351.7635\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 339.4118\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 335.6089\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 333.1071\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 323.8591\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 351.0612\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 338.2164\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 334.6995\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 320.2202\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 327.3194\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 312.3446\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 333.9740\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 317.4539\n",
            "mean,rmse,rmse/mean,bldg: 48.94701818682794 5.716911139263888 0.11679794502379631 Eagle_office_Lillian\n",
            "Example prediction:\n",
            " 0 [16 16 16 16 16 16 16 16 17 17 17 18]\n",
            "Example prediction:\n",
            " 2 [16 16 16 16 16 16 16 16 17 17 17 17]\n",
            "Example prediction:\n",
            " 4 [15 16 16 16 15 16 16 16 17 17 17 17]\n",
            "Example prediction:\n",
            " 6 [16 16 16 16 16 16 16 16 17 17 17 17]\n",
            "Example prediction:\n",
            " 8 [18 19 18 19 19 19 19 19 20 20 20 20]\n",
            "Example prediction:\n",
            " 10 [21 21 21 21 21 21 21 22 22 22 22 23]\n",
            "Example prediction:\n",
            " 12 [21 21 21 21 21 21 21 21 22 22 22 23]\n",
            "Example prediction:\n",
            " 14 [20 21 21 21 21 21 21 21 22 22 22 22]\n",
            "Example prediction:\n",
            " 16 [21 21 21 21 21 21 21 22 22 22 22 23]\n",
            "Example prediction:\n",
            " 18 [22 22 23 23 22 23 23 23 23 23 23 24]\n",
            "Example prediction:\n",
            " 20 [20 20 20 20 20 20 21 21 21 21 21 22]\n",
            "Example prediction:\n",
            " 22 [17 18 18 18 17 18 18 18 19 19 19 19]\n",
            "Building 84 Eagle_office_Amie\n",
            "Building 85 Eagle_office_Damian\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_81\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_243 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_244 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_245 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [187 197 167 148 128 127 128 130 137 164 187 210]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 29059.7021\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27126.4812\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25673.0950\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24560.8276\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23082.7554\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22018.2672\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20502.0438\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19615.6242\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18743.4439\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17419.5652\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16694.9145\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15633.6839\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14753.7512\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14009.6783\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12947.9850\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12294.5845\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11575.1267\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10978.4630\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10295.0459\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9648.9455\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9082.9126\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8306.1684\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7659.1596\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7053.5374\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6732.8763\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6039.6699\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5671.4624\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5245.0272\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4899.3933\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4559.4216\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4259.0923\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3990.0290\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3740.9094\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3511.9392\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3257.2619\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3145.8577\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3002.7041\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2840.8454\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2660.7260\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2586.7432\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2445.3987\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2488.6133\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2403.8365\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2326.1979\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2362.7746\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2331.4323\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2297.8713\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2302.4045\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2231.2083\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2279.8090\n",
            "mean,rmse,rmse/mean,bldg: 172.0950668205822 47.5434641397488 0.27626279484998406 Eagle_office_Damian\n",
            "Example prediction:\n",
            " 0 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 2 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 4 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 6 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 8 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 10 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 12 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 14 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 16 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 18 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 20 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Example prediction:\n",
            " 22 [170 170 170 169 169 169 169 169 169 169 169 169]\n",
            "Building 86 Eagle_health_Amy\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_82\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_246 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_247 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_248 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [53 51 45 38 32 29 28 27 35 47 64 76]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 6822.2479\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5980.3288\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5406.4498\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4849.2987\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4318.5538\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3902.5566\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3535.3997\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3161.2218\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2808.5896\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2567.0929\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2284.0687\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2071.7146\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1890.2410\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1738.5509\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1597.1472\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1475.4412\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1383.5067\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1302.6791\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1251.6606\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1224.9996\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1184.4192\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1151.4996\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1150.6927\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1133.1239\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1133.9627\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1113.2607\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1127.5989\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1120.5594\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1121.2983\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1115.0241\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1139.6054\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1141.8954\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1119.4762\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1121.8723\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1102.2795\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1126.2084\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1127.4995\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1097.4961\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 912.2608\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 877.5881\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 844.9387\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 805.1560\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 797.4889\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 795.1876\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 767.4461\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 750.6904\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 727.1330\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 688.7057\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 654.6943\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 645.3301\n",
            "mean,rmse,rmse/mean,bldg: 79.4401016550758 23.81047191470311 0.2997286184009024 Eagle_health_Amy\n",
            "Example prediction:\n",
            " 0 [78 80 80 79 77 72 69 67 64 63 61 63]\n",
            "Example prediction:\n",
            " 2 [72 73 73 71 69 64 61 58 56 55 55 56]\n",
            "Example prediction:\n",
            " 4 [62 61 59 55 53 48 45 42 42 42 45 48]\n",
            "Example prediction:\n",
            " 6 [64 62 58 54 52 48 45 43 43 45 49 52]\n",
            "Example prediction:\n",
            " 8 [61 57 53 51 49 50 50 52 55 59 63 65]\n",
            "Example prediction:\n",
            " 10 [58 54 53 52 54 59 61 65 68 71 73 73]\n",
            "Example prediction:\n",
            " 12 [60 57 56 56 58 63 66 70 73 75 77 76]\n",
            "Example prediction:\n",
            " 14 [65 65 68 70 72 76 79 83 84 84 82 80]\n",
            "Example prediction:\n",
            " 16 [86 89 91 94 96 98 99 98 98 96 94 91]\n",
            "Example prediction:\n",
            " 18 [ 91  95  97  99 101 102 102 101  99  96  93  91]\n",
            "Example prediction:\n",
            " 20 [91 93 95 97 98 97 97 95 93 90 88 86]\n",
            "Example prediction:\n",
            " 22 [89 91 92 92 92 90 89 86 84 82 79 79]\n",
            "Building 87 Eagle_health_Reuben\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_83\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_249 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_250 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_251 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [250 244 234 241 235 224 221 228 228 230 266 293]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 53598.3134\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 51342.8148\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 49411.0295\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 46819.7648\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 45198.1286\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43845.9740\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 41657.5727\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 39724.3308\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 38442.2443\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 36664.6367\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 35028.5497\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 33800.8312\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 32088.7150\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 30772.1071\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29387.0747\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27917.7199\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 26565.1615\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25213.6371\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23950.4859\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22862.5270\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21495.3957\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20363.9991\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19327.0940\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18301.4201\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17253.7453\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16266.5409\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15290.5245\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14403.4332\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13613.1055\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12904.6318\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12040.2474\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11074.4696\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10330.1804\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9760.8886\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9094.9718\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8583.9676\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7908.7445\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7416.4851\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6898.6904\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6372.7878\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5926.9540\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5511.5746\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5221.2993\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4913.9197\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4411.2625\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4170.6406\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3957.3427\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3632.6592\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3508.3915\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3302.9437\n",
            "mean,rmse,rmse/mean,bldg: 220.11435103483606 47.464558366433764 0.2156359098954064 Eagle_health_Reuben\n",
            "Example prediction:\n",
            " 0 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 2 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 4 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 6 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 8 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 10 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 12 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 14 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 16 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 18 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 20 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Example prediction:\n",
            " 22 [203 202 201 204 202 202 202 201 202 202 201 203]\n",
            "Building 88 Eagle_education_Edith\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_84\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_252 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_253 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_254 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [146 131 114 101  96 106 116 127 128 146 161 179]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 31745.3736\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 30125.9856\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 28812.8704\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27419.3732\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25551.9931\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24894.2670\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23443.1912\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22094.1770\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21291.0445\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19896.7394\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19004.1835\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18303.2552\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17372.4381\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16412.7522\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15390.5323\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14718.1072\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13968.6669\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13273.1715\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12625.4589\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11856.4027\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11291.2637\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10726.5305\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10011.1856\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9243.9050\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8871.9442\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8338.1702\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7805.9528\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7347.5639\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6981.6276\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6699.8975\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6270.6454\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5916.8511\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5767.2018\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5395.2701\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5209.4420\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5079.4184\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4816.7606\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4643.0863\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4505.5018\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4392.8379\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4417.9270\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4172.0940\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4189.6397\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4168.7699\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4054.9511\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3990.5778\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3960.9400\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4002.9851\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4134.8129\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4043.2008\n",
            "mean,rmse,rmse/mean,bldg: 169.08807245848166 57.92299366094569 0.34256108558555043 Eagle_education_Edith\n",
            "Example prediction:\n",
            " 0 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 2 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 4 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 6 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 8 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 10 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 12 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 14 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 16 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 18 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 20 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Example prediction:\n",
            " 22 [172 172 172 172 172 171 172 171 171 171 171 171]\n",
            "Building 89 Eagle_education_Teresa\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_255 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_256 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_257 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [102 100  96  88  85  88  89  90  87  90  93 103]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 10699.4414\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9598.3679\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8797.1993\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7936.1634\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7166.9575\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6527.0810\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5875.2385\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5281.4235\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4719.5496\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4218.4689\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3749.8018\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3324.1094\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2925.0829\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2564.1152\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2255.3649\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1952.4648\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1688.8676\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1452.7677\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1253.3826\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1070.4266\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 915.9800\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 790.1538\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 672.1647\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 573.8278\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 527.7487\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 464.7691\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 427.0410\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 400.1481\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 375.2559\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 359.7425\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 368.4820\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 338.4086\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 343.8059\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 346.1276\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 334.6093\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 352.5214\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 352.8460\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 339.5826\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 334.5134\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 340.8702\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 333.7352\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 359.2257\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 362.0265\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 344.7924\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 326.2557\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 337.4179\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 328.9797\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 345.8554\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 342.6299\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 343.0885\n",
            "mean,rmse,rmse/mean,bldg: 101.18325949910721 16.404807104684075 0.16212965648560493 Eagle_education_Teresa\n",
            "Example prediction:\n",
            " 0 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 2 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 4 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 6 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 8 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 10 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 12 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 14 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 16 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 18 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 20 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Example prediction:\n",
            " 22 [103 103 103 103 103 103 103 103 103 103 103 103]\n",
            "Building 90 Eagle_assembly_Josie\n",
            " Count bad values before pseudofill: 276\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_86\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_258 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_259 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_260 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [75 71 76 72 68 60 60 59 67 62 62 56]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 31617.6493\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29529.2802\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 28891.5223\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 27033.2654\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25927.1476\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24497.4945\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24209.9459\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22316.1511\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21151.6624\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20365.5678\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19543.8483\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18722.3671\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17403.9511\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17068.1427\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15811.9770\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15334.6887\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14617.2090\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13760.9021\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13113.2637\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12186.5606\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11644.4048\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11683.1127\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10864.8337\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10066.4049\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9946.0717\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9297.9493\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8943.4276\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8701.2941\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8349.7519\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7835.2919\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7581.2378\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7504.5714\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7209.3318\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7249.2860\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7058.7362\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7040.9733\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6772.2411\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6559.3875\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6498.6474\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6462.5114\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6512.4924\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6411.5961\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6337.1341\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6317.1480\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6364.5132\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6339.5460\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6330.0812\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6214.8717\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6193.5232\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6095.3248\n",
            "mean,rmse,rmse/mean,bldg: 191.83274980503282 80.29310896225763 0.4185578794228967 Eagle_assembly_Josie\n",
            "Example prediction:\n",
            " 0 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 2 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 4 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 6 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 8 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 10 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 12 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 14 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 16 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 18 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 20 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 22 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Building 91 Eagle_office_Norbert\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_261 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_262 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_263 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 89  81  73  80  81  90  90  91 100 100 118 132]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 8804.2401\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7959.7933\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7209.0522\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6485.2159\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5904.6279\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5318.0757\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4818.2611\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4335.0697\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3921.4410\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3533.0485\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3124.0296\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2775.5220\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2507.9251\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2233.7708\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1972.4041\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1770.9893\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1576.1589\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1423.8171\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1276.1784\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1179.2134\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1065.1371\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 943.0219\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 913.0706\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 857.9726\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 848.5351\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 836.9512\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 791.0431\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 776.5742\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 773.5019\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 785.9136\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 786.5903\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 782.4860\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 772.2899\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 772.9767\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 750.2032\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 723.2382\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 705.0917\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 704.7115\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 692.1743\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 672.1236\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 703.3208\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 669.6596\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 679.9245\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 651.2726\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 671.7303\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 667.5149\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 681.9689\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 644.1769\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 650.9258\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 632.3826\n",
            "mean,rmse,rmse/mean,bldg: 90.95752126135446 23.78605654167944 0.261507308156885 Eagle_office_Norbert\n",
            "Example prediction:\n",
            " 0 [94 93 93 92 92 92 92 92 92 93 93 94]\n",
            "Example prediction:\n",
            " 2 [95 95 94 93 93 93 93 93 93 94 94 95]\n",
            "Example prediction:\n",
            " 4 [94 93 93 92 91 92 91 91 92 92 93 93]\n",
            "Example prediction:\n",
            " 6 [89 88 87 86 86 86 86 86 86 87 87 88]\n",
            "Example prediction:\n",
            " 8 [88 87 86 85 85 85 85 85 86 86 87 88]\n",
            "Example prediction:\n",
            " 10 [94 93 93 92 92 92 92 92 92 93 93 94]\n",
            "Example prediction:\n",
            " 12 [ 99  99 100 100 100 100 100  99  99  99  99  99]\n",
            "Example prediction:\n",
            " 14 [105 107 108 110 111 111 111 111 110 108 106 104]\n",
            "Example prediction:\n",
            " 16 [106 108 110 112 113 113 113 113 111 109 108 105]\n",
            "Example prediction:\n",
            " 18 [106 108 110 111 113 113 113 112 111 109 107 105]\n",
            "Example prediction:\n",
            " 20 [104 106 107 108 109 109 110 109 108 107 105 103]\n",
            "Example prediction:\n",
            " 22 [96 96 96 95 95 95 95 95 95 96 96 96]\n",
            "Building 92 Eagle_assembly_Portia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_264 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_265 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_266 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 96  88  79  76  75  75  76  75  82  89  99 101]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 19382.8684\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17894.4648\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16848.1279\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15803.7474\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14979.7285\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13892.4084\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13150.9759\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11953.2328\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11508.4547\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10712.6536\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10082.4958\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9464.9065\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8768.2470\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8026.9242\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7633.0887\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7091.5115\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6554.1968\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6069.9520\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5707.1948\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5419.0882\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4938.8771\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4828.5242\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4614.8286\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4414.4197\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3879.4169\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3923.9782\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3693.2116\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3507.3769\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3371.3722\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3286.3456\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3205.0401\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3130.6195\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3176.0950\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3062.3045\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3034.6960\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3036.5573\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2995.3682\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3018.8834\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2969.6398\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2956.7792\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2994.1733\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2939.0750\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2957.8631\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3059.2052\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2912.4661\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2991.5049\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2987.6176\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3019.6982\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2911.0583\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2907.4790\n",
            "mean,rmse,rmse/mean,bldg: 129.48665269258794 52.72674517359804 0.4071983025059402 Eagle_assembly_Portia\n",
            "Example prediction:\n",
            " 0 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 2 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 4 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 6 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 8 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 10 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 12 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 14 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 16 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 18 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 20 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Example prediction:\n",
            " 22 [129 129 129 129 129 129 129 129 129 129 129 129]\n",
            "Building 93 Eagle_office_Michele\n",
            "Building 94 Eagle_lodging_Casey\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_89\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_267 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_268 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_269 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [95 97 97 95 88 80 74 70 70 69 70 72]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 6056.6057\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5310.2200\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4694.2125\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4175.4052\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3626.3271\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3183.1255\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2787.9148\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2415.0555\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2105.9696\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1841.1057\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1583.1806\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1362.0315\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1173.5395\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1015.7710\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 885.4401\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 787.3522\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 688.7644\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 624.9861\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 557.2465\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 514.4897\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 498.6951\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 458.2669\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 464.9858\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 437.0176\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 457.8758\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 458.2731\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 450.0037\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 467.8332\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 453.8261\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 332.3172\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 236.8972\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 202.8421\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 194.3173\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183.6032\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 189.1003\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 178.0315\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 175.6147\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 185.1138\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 166.1391\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.9168\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160.6323\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 174.1084\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 161.7484\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.4417\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 159.2493\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 175.3252\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 168.6703\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.5002\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160.4507\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162.7117\n",
            "mean,rmse,rmse/mean,bldg: 77.14309156310537 10.20373826100624 0.13227027922078122 Eagle_lodging_Casey\n",
            "Example prediction:\n",
            " 0 [92 93 94 95 95 96 95 95 95 93 92 91]\n",
            "Example prediction:\n",
            " 2 [89 90 91 91 91 91 91 91 91 90 89 89]\n",
            "Example prediction:\n",
            " 4 [73 73 73 72 73 72 72 72 72 72 73 73]\n",
            "Example prediction:\n",
            " 6 [77 76 76 75 75 75 75 75 75 75 76 76]\n",
            "Example prediction:\n",
            " 8 [76 75 75 74 74 73 73 74 74 74 75 75]\n",
            "Example prediction:\n",
            " 10 [72 71 71 70 70 70 70 70 70 71 72 72]\n",
            "Example prediction:\n",
            " 12 [71 71 70 70 69 69 70 70 70 70 71 71]\n",
            "Example prediction:\n",
            " 14 [72 72 71 71 70 70 70 71 71 71 72 72]\n",
            "Example prediction:\n",
            " 16 [82 82 81 81 81 80 80 80 80 81 81 81]\n",
            "Example prediction:\n",
            " 18 [89 90 90 91 91 91 91 90 90 89 89 88]\n",
            "Example prediction:\n",
            " 20 [91 91 92 93 93 94 93 93 93 92 91 90]\n",
            "Example prediction:\n",
            " 22 [92 93 94 95 96 96 96 95 95 94 92 91]\n",
            "Building 95 Eagle_office_Elvis\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_270 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_271 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_272 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [54 53 51 52 52 52 52 53 54 55 53 62]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 2296.2874\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1845.0159\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1521.4070\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1256.3875\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1032.5080\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 864.3204\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 713.8439\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 603.0705\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 509.3090\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 442.3891\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 398.3863\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 369.8015\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 356.6839\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 328.9739\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 324.1558\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 316.8436\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 318.6379\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 323.0013\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 324.1805\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 317.5295\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 320.0726\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 321.2516\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 314.2912\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 268.6542\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 253.9099\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 245.6478\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 238.6871\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 231.6205\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 228.9712\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 224.3521\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 225.2023\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 222.1456\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 210.4222\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 203.1180\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 193.6188\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 191.8679\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 186.8194\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 175.8992\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 173.3024\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171.8106\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 165.5064\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 162.5433\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 161.0287\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 166.5296\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154.1599\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155.5912\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154.5917\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 156.0835\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 155.3421\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 151.8779\n",
            "mean,rmse,rmse/mean,bldg: 47.95189767565624 11.783790823737151 0.245741907931194 Eagle_office_Elvis\n",
            "Example prediction:\n",
            " 0 [62 64 64 63 61 59 56 52 48 45 41 39]\n",
            "Example prediction:\n",
            " 2 [60 60 59 57 54 51 48 44 40 37 35 34]\n",
            "Example prediction:\n",
            " 4 [56 55 53 51 49 46 43 40 38 36 35 35]\n",
            "Example prediction:\n",
            " 6 [49 48 46 45 43 41 40 40 39 39 40 41]\n",
            "Example prediction:\n",
            " 8 [38 38 40 42 46 49 52 54 57 59 60 59]\n",
            "Example prediction:\n",
            " 10 [34 35 37 40 44 48 52 55 58 60 61 60]\n",
            "Example prediction:\n",
            " 12 [38 39 42 45 49 53 56 59 61 63 63 62]\n",
            "Example prediction:\n",
            " 14 [40 42 45 48 52 56 59 62 64 64 64 63]\n",
            "Example prediction:\n",
            " 16 [49 52 54 57 60 61 63 64 64 62 61 59]\n",
            "Example prediction:\n",
            " 18 [60 63 65 68 68 68 67 64 61 58 54 51]\n",
            "Example prediction:\n",
            " 20 [60 62 64 66 66 65 63 60 57 54 51 47]\n",
            "Example prediction:\n",
            " 22 [62 64 65 66 64 63 60 56 52 49 45 42]\n",
            "Building 96 Eagle_office_Tia\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_273 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_274 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_275 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [168 171 173 168 156 146 144 154 162 174 182 194]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 31702.1800\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 29530.4781\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 28132.7385\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 26804.8909\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25369.2532\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24192.4107\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22946.9899\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21613.9285\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20397.4811\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19548.4961\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18547.7479\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17359.7457\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16533.4984\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15568.1879\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14455.9283\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13643.6025\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12881.1008\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12137.4556\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11344.2143\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10606.9068\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9915.0936\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9280.3772\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8588.5621\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7971.9422\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7404.7865\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6864.9974\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6342.3705\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6043.3720\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5553.0721\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5143.9622\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4816.5159\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4490.5429\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4066.7513\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3856.8219\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3569.6878\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3352.8034\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3278.7807\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3039.5781\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2874.2333\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2470.5333\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2316.9570\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2225.8811\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2019.5842\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1950.4172\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1886.7649\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1906.9087\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1761.5530\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1705.0888\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1753.3212\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1599.8405\n",
            "mean,rmse,rmse/mean,bldg: 175.5524133166155 39.94501184245641 0.2275389502644663 Eagle_office_Tia\n",
            "Example prediction:\n",
            " 0 [179 180 179 179 179 179 179 179 179 179 180 179]\n",
            "Example prediction:\n",
            " 2 [176 177 176 176 176 176 176 176 176 176 177 176]\n",
            "Example prediction:\n",
            " 4 [170 171 171 170 171 171 171 171 170 170 171 170]\n",
            "Example prediction:\n",
            " 6 [162 162 162 162 162 162 163 163 162 162 163 162]\n",
            "Example prediction:\n",
            " 8 [181 182 181 181 181 181 181 181 181 181 182 181]\n",
            "Example prediction:\n",
            " 10 [182 183 182 183 182 182 182 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 12 [182 183 182 183 183 183 183 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 14 [182 183 182 183 183 183 183 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 16 [182 183 182 183 183 183 183 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 18 [182 183 182 183 183 183 183 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 20 [182 183 182 183 183 183 183 183 182 182 183 182]\n",
            "Example prediction:\n",
            " 22 [182 183 182 183 182 182 182 182 182 182 183 182]\n",
            "Building 97 Eagle_office_Flossie\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_276 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_277 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_278 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [127 127 127 130 127 130 127 129 134 141 153 157]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 25697.6258\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 24019.3716\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22568.1752\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21194.0155\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 19869.0255\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18702.6549\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17565.4591\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16441.0579\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15445.0639\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14431.5180\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13401.9141\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12504.1797\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11652.1235\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10796.0723\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10024.0803\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9213.3088\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8498.9285\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7807.2650\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7254.5973\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6545.3759\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5972.5352\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5431.8129\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4973.0274\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4421.9403\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4051.9183\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3649.4034\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3266.0943\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2912.2675\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2606.9859\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2264.0420\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2029.6708\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1806.7757\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1607.4893\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1418.9758\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1273.5224\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1126.8179\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1014.7702\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 919.6609\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 900.7996\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 822.7987\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 804.3971\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 770.2786\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 740.3508\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 699.3796\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 746.7923\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 706.6726\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 706.3021\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 717.0348\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 692.7777\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 722.3594\n",
            "mean,rmse,rmse/mean,bldg: 145.23397180101156 64.03709491558854 0.4409236635305082 Eagle_office_Flossie\n",
            "Example prediction:\n",
            " 0 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 2 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 4 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 6 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 8 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 10 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 12 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 14 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 16 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 18 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 20 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Example prediction:\n",
            " 22 [159 159 159 159 159 159 159 159 159 159 159 159]\n",
            "Building 98 Eagle_office_Remedios\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_279 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_280 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_281 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 82  82  80  80  79  79  77  77  79  82  90 101]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 7003.9728\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6155.9834\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5506.8436\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4935.0422\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4409.3563\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3925.9756\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3474.2865\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3045.9431\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2698.1347\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2319.0382\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1998.2662\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1752.0813\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1503.9037\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1294.6716\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1112.5473\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 954.5187\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 805.3911\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 689.7191\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 599.0923\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 539.8120\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 463.6914\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 421.7894\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 401.1252\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 367.5387\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 343.9961\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 351.6082\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 348.0488\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 339.9284\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 310.7625\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 329.0808\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 327.5041\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 341.6820\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 337.1066\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 332.7954\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 337.5976\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 327.3855\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 327.6464\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 331.1375\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 328.0255\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 329.6946\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 304.7766\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 308.6353\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 291.9405\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 309.6610\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 307.0785\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 289.1506\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 310.4012\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 282.9234\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 294.3770\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 303.6488\n",
            "mean,rmse,rmse/mean,bldg: 83.20906495458543 13.348495623151715 0.16042117082360166 Eagle_office_Remedios\n",
            "Example prediction:\n",
            " 0 [83 83 82 82 82 82 82 82 82 82 83 83]\n",
            "Example prediction:\n",
            " 2 [83 82 82 81 81 81 81 81 81 82 82 83]\n",
            "Example prediction:\n",
            " 4 [84 83 83 82 82 82 82 82 82 83 83 84]\n",
            "Example prediction:\n",
            " 6 [83 83 82 82 82 81 82 82 82 82 83 83]\n",
            "Example prediction:\n",
            " 8 [80 79 78 78 77 77 78 78 78 78 80 80]\n",
            "Example prediction:\n",
            " 10 [85 85 85 85 84 84 84 84 84 84 85 85]\n",
            "Example prediction:\n",
            " 12 [91 92 93 93 94 94 94 94 93 92 91 90]\n",
            "Example prediction:\n",
            " 14 [92 93 95 96 96 97 96 96 95 94 93 91]\n",
            "Example prediction:\n",
            " 16 [92 93 95 96 96 97 96 96 95 94 93 91]\n",
            "Example prediction:\n",
            " 18 [92 93 94 95 95 95 95 95 94 94 92 91]\n",
            "Example prediction:\n",
            " 20 [88 88 88 88 88 88 88 88 88 88 87 87]\n",
            "Example prediction:\n",
            " 22 [85 85 84 84 84 83 84 83 84 84 84 85]\n",
            "Building 99 Eagle_public_Minnie\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_282 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_283 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_284 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [220 218 218 218 220 220 222 220 220 218 224 240]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 26907.4065\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 25361.3440\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 23837.7612\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 22594.7090\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 21270.2879\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 20074.5553\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18982.5658\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18217.8531\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16923.8059\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15868.8836\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14863.3836\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14316.7884\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13148.5812\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12298.9402\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11434.8399\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10614.2121\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9872.8641\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9196.8944\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8522.7741\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7937.9001\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7382.7863\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6946.8463\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6327.1990\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5786.4065\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5385.9342\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4929.8598\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4405.7422\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4199.2433\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3808.0255\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3467.6903\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3235.3935\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3001.0212\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2792.8998\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2521.5849\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2400.3291\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2222.9772\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2145.4738\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2063.1980\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1943.9483\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1917.8108\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1800.1899\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1835.8169\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1746.9775\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1743.4093\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1695.1053\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1700.6204\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1757.7640\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1676.2180\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1747.0103\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1486.4339\n",
            "mean,rmse,rmse/mean,bldg: 146.60540417284233 21.787602404481316 0.14861391043125902 Eagle_public_Minnie\n",
            "Example prediction:\n",
            " 0 [76 73 75 75 77 74 76 74 73 76 73 76]\n",
            "Example prediction:\n",
            " 2 [68 66 68 68 70 67 69 66 66 69 66 68]\n",
            "Example prediction:\n",
            " 4 [62 59 61 61 63 60 62 59 60 62 59 61]\n",
            "Example prediction:\n",
            " 6 [166 166 166 166 166 166 166 166 166 166 166 166]\n",
            "Example prediction:\n",
            " 8 [163 163 163 163 163 163 163 163 163 163 163 163]\n",
            "Example prediction:\n",
            " 10 [154 154 154 154 154 154 154 154 153 154 153 154]\n",
            "Example prediction:\n",
            " 12 [149 149 149 149 150 149 149 149 149 149 149 149]\n",
            "Example prediction:\n",
            " 14 [145 144 144 145 145 144 145 144 144 145 144 144]\n",
            "Example prediction:\n",
            " 16 [141 140 140 141 142 141 141 140 140 141 140 141]\n",
            "Example prediction:\n",
            " 18 [157 157 157 157 157 157 157 157 157 157 156 157]\n",
            "Example prediction:\n",
            " 20 [145 144 144 144 145 144 145 144 144 144 144 144]\n",
            "Example prediction:\n",
            " 22 [138 137 137 138 139 137 138 137 137 138 137 138]\n",
            "Building 100 Eagle_office_Patrice\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_285 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_286 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_287 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [ 76  74  74  72  71  67  66  65  66  70  94 112]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 7632.6699\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6774.8868\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6041.3412\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5418.1669\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4862.8413\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4310.2942\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3809.0883\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3420.0118\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3024.6921\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2656.7690\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2358.0516\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2098.3948\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1840.2085\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1643.2471\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1440.8191\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1267.3826\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1145.8415\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1008.5054\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 896.8909\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 837.2362\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 781.8300\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 732.7771\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 692.2453\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 684.9754\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 670.1688\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 655.9082\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 644.4637\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 652.7807\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 650.2313\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 631.1114\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 657.7795\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 650.3626\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 637.8337\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 640.3919\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 647.9524\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 655.0508\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 644.6336\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 640.0893\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 634.6704\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 645.0913\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 654.3979\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 628.0259\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 637.8647\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 625.7377\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 638.3569\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 634.3016\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 633.1181\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 638.5870\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 634.8738\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 636.0919\n",
            "mean,rmse,rmse/mean,bldg: 84.55908486299509 22.712741929767898 0.268602030953477 Eagle_office_Patrice\n",
            "Example prediction:\n",
            " 0 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 2 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 4 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 6 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 8 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 10 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 12 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 14 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 16 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 18 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 20 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Example prediction:\n",
            " 22 [85 85 85 85 85 85 85 85 85 85 85 85]\n",
            "Building 101 Eagle_education_Luther\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_288 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_289 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_290 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [466 459 455 451 447 443 444 449 456 459 463 466]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 213053.9284\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 207357.3023\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 203193.6090\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 199374.3411\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 195149.5790\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 190824.8091\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 186795.1359\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 183591.3716\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 179043.9901\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 175392.4709\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171575.9153\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 167844.4111\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 164344.6581\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160306.6568\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157255.1512\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 153522.5894\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 150246.8980\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 146605.3266\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 143173.1487\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 140118.4837\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 136583.0140\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 133672.6608\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 130625.7366\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 127153.5497\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 124452.7731\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 120957.4621\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 117940.3446\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 114499.5628\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 111860.9037\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 108820.0721\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 106016.4984\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 103054.1610\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 100387.1497\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97548.6985\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94787.3937\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92308.9084\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89583.5571\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87055.8725\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84428.4277\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81770.0889\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79322.8126\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 76852.2686\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74556.4171\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72258.6062\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 69865.4136\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67531.2512\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 65389.9370\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 62911.3471\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 61211.9366\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 59001.1183\n",
            "mean,rmse,rmse/mean,bldg: 470.9357794455066 259.03442593771103 0.5500419319226618 Eagle_education_Luther\n",
            "Example prediction:\n",
            " 0 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 2 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 4 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 6 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 8 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 10 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 12 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 14 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 16 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 18 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 20 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Example prediction:\n",
            " 22 [223 225 225 225 224 223 226 223 223 225 224 224]\n",
            "Building 102 Eagle_education_Paul\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_97\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_291 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_292 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_293 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 622.5832\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 425.2089\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 308.8509\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 216.9396\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160.1078\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119.9841\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96.0922\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84.3330\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78.8638\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.4256\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.4761\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 68.7571\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 64.6829\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 60.7751\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 59.1361\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 62.0394\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 60.9524\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 57.5973\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 58.8312\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 57.5020\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 56.8482\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 58.7725\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 57.3417\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 54.2276\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 50.4151\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 50.5860\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 49.2850\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 48.2641\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47.4147\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47.2252\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 45.3828\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47.9427\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47.4382\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 46.2245\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44.0194\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43.0924\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 42.4759\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 45.3932\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 45.7803\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 42.1917\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43.9224\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44.9894\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 45.8350\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43.0305\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44.6756\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47.9444\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44.2248\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43.9576\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 44.5020\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 43.7203\n",
            "mean,rmse,rmse/mean,bldg: 24.829059177972724 5.419215272378758 0.21826099948186733 Eagle_education_Paul\n",
            "Example prediction:\n",
            " 0 [32 32 32 32 30 29 27 25 23 21 20 20]\n",
            "Example prediction:\n",
            " 2 [33 32 31 30 28 26 24 21 20 19 19 20]\n",
            "Example prediction:\n",
            " 4 [20 20 18 17 15 13 11  9  8  9 10 12]\n",
            "Example prediction:\n",
            " 6 [26 24 23 21 19 18 17 17 18 19 20 22]\n",
            "Example prediction:\n",
            " 8 [25 23 21 20 20 20 20 21 23 24 26 27]\n",
            "Example prediction:\n",
            " 10 [22 22 21 22 22 24 25 27 29 30 32 32]\n",
            "Example prediction:\n",
            " 12 [20 20 21 22 24 27 29 31 33 34 35 35]\n",
            "Example prediction:\n",
            " 14 [22 22 23 25 26 28 30 31 32 33 33 33]\n",
            "Example prediction:\n",
            " 16 [26 27 28 29 30 31 32 33 33 33 32 31]\n",
            "Example prediction:\n",
            " 18 [30 31 31 32 32 32 31 31 30 30 29 28]\n",
            "Example prediction:\n",
            " 20 [30 31 31 32 32 31 31 31 30 29 28 27]\n",
            "Example prediction:\n",
            " 22 [34 35 35 35 34 33 32 30 28 26 24 23]\n",
            "Building 103 Eagle_assembly_Margret\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_98\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_294 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_295 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_296 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [505 483 457 417 388 364 366 367 406 442 479 478]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 180001.9060\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 175996.4428\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 171048.5676\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 167365.5430\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 164128.9336\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 160232.6574\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 157424.5434\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 154698.5373\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 149814.4653\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 147592.0007\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 143970.8157\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 140930.3735\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 137733.6952\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 133760.9314\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 131168.3565\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 128144.1844\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 124710.6471\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 122231.2938\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 119193.1709\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 115691.0086\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 113473.5068\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 110744.3623\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 107962.1348\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 105436.3556\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 101864.6912\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 99801.8667\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 97086.3310\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 94218.1851\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 92080.1418\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89716.0257\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 87316.9934\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 84693.6856\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81843.5038\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 79719.9177\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 77817.0183\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75528.7278\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 73336.7081\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71192.2603\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 69300.7559\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67171.5501\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 64675.8545\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 62618.7168\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 60729.6393\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 58011.5707\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 56459.3137\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 54626.3361\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 52337.6044\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 50822.0817\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 49125.5096\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 47167.1159\n",
            "mean,rmse,rmse/mean,bldg: 381.5734830787078 152.95171140911208 0.40084470801018024 Eagle_assembly_Margret\n",
            "Example prediction:\n",
            " 0 [146 146 147 148 146 147 146 146 147 147 145 146]\n",
            "Example prediction:\n",
            " 2 [123 122 124 124 122 123 122 122 123 123 121 122]\n",
            "Example prediction:\n",
            " 4 [121 119 122 121 119 120 119 119 121 121 119 119]\n",
            "Example prediction:\n",
            " 6 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 8 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 10 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 12 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 14 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 16 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 18 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 20 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Example prediction:\n",
            " 22 [224 223 223 222 224 224 222 223 223 225 222 223]\n",
            "Building 104 Eagle_office_Marisela\n",
            " Count bad values before pseudofill: 5\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_99\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_297 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_298 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_299 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [131 131 130 128 133 133 138 129 138 144 160 173]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 19592.5036\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 18524.5268\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 17307.0163\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 16167.6412\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 15069.7947\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 14198.7243\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 13181.6271\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 12311.2255\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 11612.9516\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10799.9360\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 10010.7520\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 9330.3009\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8647.3299\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 8097.4173\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 7480.7699\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6915.1883\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 6486.7274\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5934.9597\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 5269.4941\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4692.8532\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4399.9753\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 4003.9786\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3639.3067\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3315.6483\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 3019.9863\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2705.0119\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2490.4011\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2282.0442\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 2114.8744\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1939.0430\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1824.0027\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1651.8927\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1539.3904\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1455.7593\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1410.3113\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1390.6764\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1343.1143\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1253.5993\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1226.2469\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1242.3687\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1200.2812\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1229.0611\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1236.0406\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1204.5828\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1196.4328\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1225.5415\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1227.1878\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1199.5033\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1104.6639\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1168.7918\n",
            "mean,rmse,rmse/mean,bldg: 130.5382324001738 25.294163326297348 0.1937682383254308 Eagle_office_Marisela\n",
            "Example prediction:\n",
            " 0 [145 145 145 145 145 145 145 145 145 145 145 145]\n",
            "Example prediction:\n",
            " 2 [142 142 142 142 142 142 142 142 142 141 141 141]\n",
            "Example prediction:\n",
            " 4 [135 135 135 135 135 135 135 135 135 135 135 135]\n",
            "Example prediction:\n",
            " 6 [131 130 131 131 131 131 130 131 131 131 131 131]\n",
            "Example prediction:\n",
            " 8 [133 133 133 134 133 133 133 133 133 133 133 134]\n",
            "Example prediction:\n",
            " 10 [137 137 137 137 137 137 137 137 137 137 137 137]\n",
            "Example prediction:\n",
            " 12 [141 141 141 141 141 141 141 141 140 140 140 140]\n",
            "Example prediction:\n",
            " 14 [145 145 145 145 145 145 145 145 145 145 145 145]\n",
            "Example prediction:\n",
            " 16 [150 150 150 149 150 150 150 149 149 149 149 149]\n",
            "Example prediction:\n",
            " 18 [150 150 150 149 150 150 150 149 149 149 149 149]\n",
            "Example prediction:\n",
            " 20 [149 149 149 149 149 149 149 149 149 149 149 148]\n",
            "Example prediction:\n",
            " 22 [148 148 148 148 148 148 148 148 147 147 147 147]\n",
            "Building 105 Eagle_education_Shana\n",
            " Count bad values before pseudofill: 2\n",
            " Count bad values after pseudofill: 0\n",
            "Model: \"sequential_100\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_300 (GRU)                (None, 12, 16)            912       \n",
            "_________________________________________________________________\n",
            "gru_301 (GRU)                (None, 12, 16)            1632      \n",
            "_________________________________________________________________\n",
            "gru_302 (GRU)                (None, 16)                1632      \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 12)                204       \n",
            "=================================================================\n",
            "Total params: 4,380\n",
            "Trainable params: 4,380\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Example y train:\n",
            " [47 47 45 45 45 45 45 45 46 46 49 53]\n",
            "Epoch 1/50\n",
            "274/274 [==============================] - 5s 6ms/step - loss: 2098.6328\n",
            "Epoch 2/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1653.1278\n",
            "Epoch 3/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1334.3586\n",
            "Epoch 4/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 1074.5166\n",
            "Epoch 5/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 853.6021\n",
            "Epoch 6/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 657.7158\n",
            "Epoch 7/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 511.8709\n",
            "Epoch 8/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 392.0242\n",
            "Epoch 9/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 300.5857\n",
            "Epoch 10/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 227.3655\n",
            "Epoch 11/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 176.9058\n",
            "Epoch 12/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 142.3573\n",
            "Epoch 13/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 110.9706\n",
            "Epoch 14/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 96.5245\n",
            "Epoch 15/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 89.1111\n",
            "Epoch 16/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.9743\n",
            "Epoch 17/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74.6843\n",
            "Epoch 18/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78.6308\n",
            "Epoch 19/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 76.2937\n",
            "Epoch 20/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 70.1071\n",
            "Epoch 21/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 77.5025\n",
            "Epoch 22/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 77.5335\n",
            "Epoch 23/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 85.8092\n",
            "Epoch 24/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 73.6113\n",
            "Epoch 25/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75.0572\n",
            "Epoch 26/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 76.4204\n",
            "Epoch 27/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 75.2605\n",
            "Epoch 28/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 78.4334\n",
            "Epoch 29/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 81.6660\n",
            "Epoch 30/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 76.8622\n",
            "Epoch 31/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74.2159\n",
            "Epoch 32/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.3062\n",
            "Epoch 33/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.0228\n",
            "Epoch 34/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74.2636\n",
            "Epoch 35/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.2721\n",
            "Epoch 36/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 70.3946\n",
            "Epoch 37/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67.9902\n",
            "Epoch 38/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 74.1301\n",
            "Epoch 39/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 69.7156\n",
            "Epoch 40/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.2613\n",
            "Epoch 41/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67.9887\n",
            "Epoch 42/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 69.2661\n",
            "Epoch 43/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67.2405\n",
            "Epoch 44/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 68.4795\n",
            "Epoch 45/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 66.6162\n",
            "Epoch 46/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 71.8192\n",
            "Epoch 47/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 67.9814\n",
            "Epoch 48/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 72.3303\n",
            "Epoch 49/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 66.9079\n",
            "Epoch 50/50\n",
            "274/274 [==============================] - 2s 6ms/step - loss: 70.4255\n",
            "mean,rmse,rmse/mean,bldg: 46.266861946566706 5.775282643675415 0.12482546688265245 Eagle_education_Shana\n",
            "Example prediction:\n",
            " 0 [47 47 47 47 47 47 47 47 47 47 47 47]\n",
            "Example prediction:\n",
            " 2 [47 47 47 47 46 46 46 46 47 47 47 47]\n",
            "Example prediction:\n",
            " 4 [45 44 44 44 43 43 43 43 44 44 44 45]\n",
            "Example prediction:\n",
            " 6 [46 45 45 45 45 45 45 45 45 45 45 46]\n",
            "Example prediction:\n",
            " 8 [47 47 47 47 46 46 46 46 47 47 47 47]\n",
            "Example prediction:\n",
            " 10 [47 47 47 47 47 47 47 47 47 47 47 47]\n",
            "Example prediction:\n",
            " 12 [48 48 48 48 47 47 47 47 47 47 47 47]\n",
            "Example prediction:\n",
            " 14 [48 48 48 48 48 48 48 48 48 48 48 48]\n",
            "Example prediction:\n",
            " 16 [50 50 51 51 52 52 52 52 51 51 50 50]\n",
            "Example prediction:\n",
            " 18 [50 51 51 51 52 52 52 52 52 51 50 50]\n",
            "Example prediction:\n",
            " 20 [50 50 51 51 52 52 52 52 51 51 50 50]\n",
            "Example prediction:\n",
            " 22 [49 49 49 49 50 50 50 50 49 49 49 49]\n",
            "\n",
            "History 24 Future 12\n",
            "Column 1: Mean usage.\n",
            "Column 2: RMSE of LinearRegression(X=Weather, y=Usage).\n",
            "Column 3: RMSE/mean normalized to help understand RMSE.\n",
            "Column 4: Building.\n",
            "      0.00       0.00   nan   Eagle_assembly_Noel\n",
            "     11.42       3.27  0.29   Eagle_assembly_Estelle\n",
            "     13.56      10.46  0.77   Eagle_lodging_Blake\n",
            "     15.88       4.54  0.29   Eagle_office_Mable\n",
            "     16.09       1.91  0.12   Eagle_office_Mandi\n",
            "     21.79       2.79  0.13   Eagle_education_Maragret\n",
            "     22.54       6.74  0.30   Eagle_office_Isidro\n",
            "     24.83       5.42  0.22   Eagle_education_Paul\n",
            "     26.04       5.28  0.20   Eagle_office_Ryan\n",
            "     26.49       5.66  0.21   Eagle_education_Sheena\n",
            "     29.40       9.76  0.33   Eagle_public_Henry\n",
            "     29.50       5.26  0.18   Eagle_office_Henriette\n",
            "     30.41       5.77  0.19   Eagle_office_Lamont\n",
            "     30.84       7.94  0.26   Eagle_public_Missy\n",
            "     30.89       7.22  0.23   Eagle_office_Chantelle\n",
            "     32.56       8.83  0.27   Eagle_education_Cassie\n",
            "     38.72      10.20  0.26   Eagle_education_April\n",
            "     39.08      42.59  1.09   Eagle_assembly_Candice\n",
            "     39.10       4.76  0.12   Eagle_office_Phyllis\n",
            "     40.04       7.75  0.19   Eagle_education_Eileen\n",
            "     41.31      12.48  0.30   Eagle_office_Sheree\n",
            "     44.21       9.45  0.21   Eagle_office_Jeff\n",
            "     46.05       7.91  0.17   Eagle_office_Lane\n",
            "     46.27       5.78  0.12   Eagle_education_Shana\n",
            "     46.80       7.31  0.16   Eagle_lodging_Trina\n",
            "     47.80       8.34  0.17   Eagle_office_Efrain\n",
            "     47.95      11.78  0.25   Eagle_office_Elvis\n",
            "     48.95       5.72  0.12   Eagle_office_Lillian\n",
            "     49.24      13.37  0.27   Eagle_assembly_Benny\n",
            "     49.49       7.95  0.16   Eagle_lodging_Edgardo\n",
            "     51.91      12.18  0.23   Eagle_office_Dallas\n",
            "     53.25      11.36  0.21   Eagle_health_Vincenza\n",
            "     54.25       8.11  0.15   Eagle_health_Margo\n",
            "     54.75       8.92  0.16   Eagle_lodging_Andy\n",
            "     60.09      11.94  0.20   Eagle_office_Freida\n",
            "     64.47       8.21  0.13   Eagle_lodging_Tressa\n",
            "     65.37      16.38  0.25   Eagle_office_Donovan\n",
            "     66.23       7.62  0.12   Eagle_education_Petra\n",
            "     68.34      28.51  0.42   Eagle_assembly_Herbert\n",
            "     69.35      15.95  0.23   Eagle_office_Jackie\n",
            "     73.16       9.67  0.13   Eagle_lodging_Dawn\n",
            "     77.14      10.20  0.13   Eagle_lodging_Casey\n",
            "     78.39      10.61  0.14   Eagle_lodging_Stephanie\n",
            "     79.44      23.81  0.30   Eagle_health_Amy\n",
            "     81.19      22.34  0.28   Eagle_office_Elias\n",
            "     83.21      13.35  0.16   Eagle_office_Remedios\n",
            "     84.56      22.71  0.27   Eagle_office_Patrice\n",
            "     86.45      41.68  0.48   Eagle_food_Jennifer\n",
            "     90.96      23.79  0.26   Eagle_office_Norbert\n",
            "    101.18      16.40  0.16   Eagle_education_Teresa\n",
            "    102.57      17.03  0.17   Eagle_public_Alvin\n",
            "    103.78      21.03  0.20   Eagle_lodging_Terri\n",
            "    103.86      43.57  0.42   Eagle_office_Katheleen\n",
            "    109.59      29.79  0.27   Eagle_office_Randolph\n",
            "    112.10      11.60  0.10   Eagle_education_Will\n",
            "    115.97      22.94  0.20   Eagle_assembly_Lacy\n",
            "    126.85      43.39  0.34   Eagle_education_Wesley\n",
            "    129.49      52.73  0.41   Eagle_assembly_Portia\n",
            "    130.11      40.15  0.31   Eagle_office_Yadira\n",
            "    130.38      43.01  0.33   Eagle_public_Ola\n",
            "    130.54      25.29  0.19   Eagle_office_Marisela\n",
            "    134.95      37.00  0.27   Eagle_office_Elia\n",
            "    144.15     101.45  0.70   Eagle_assembly_Latrina\n",
            "    145.23      64.04  0.44   Eagle_office_Flossie\n",
            "    146.61      21.79  0.15   Eagle_public_Minnie\n",
            "    152.19      56.26  0.37   Eagle_office_Demetra\n",
            "    156.18      29.78  0.19   Eagle_office_Nereida\n",
            "    169.09      57.92  0.34   Eagle_education_Edith\n",
            "    172.10      47.54  0.28   Eagle_office_Damian\n",
            "    175.55      39.95  0.23   Eagle_office_Tia\n",
            "    178.30      41.10  0.23   Eagle_health_Jodi\n",
            "    180.15      32.87  0.18   Eagle_office_Sonya\n",
            "    184.56      82.19  0.45   Eagle_food_Kay\n",
            "    187.16      46.35  0.25   Eagle_education_Lino\n",
            "    191.83      80.29  0.42   Eagle_assembly_Josie\n",
            "    194.12      37.19  0.19   Eagle_office_Bridgett\n",
            "    220.11      47.46  0.22   Eagle_health_Reuben\n",
            "    267.31      54.44  0.20   Eagle_education_Samantha\n",
            "    276.96     104.84  0.38   Eagle_office_Amanda\n",
            "    294.61     102.66  0.35   Eagle_education_Shanna\n",
            "    296.64     178.20  0.60   Eagle_health_Athena\n",
            "    304.14      90.65  0.30   Eagle_health_Trisha\n",
            "    316.87     115.90  0.37   Eagle_public_Preston\n",
            "    340.04     171.32  0.50   Eagle_public_Pearle\n",
            "    348.21     141.96  0.41   Eagle_office_Francis\n",
            "    381.57     152.95  0.40   Eagle_assembly_Margret\n",
            "    401.14     231.94  0.58   Eagle_education_Brianne\n",
            "    405.50     228.34  0.56   Eagle_health_Reba\n",
            "    431.81     287.01  0.66   Eagle_health_Gregoria\n",
            "    434.18     244.34  0.56   Eagle_education_Alberto\n",
            "    444.26     283.78  0.64   Eagle_assembly_Ian\n",
            "    470.94     259.03  0.55   Eagle_education_Luther\n",
            "    492.33     277.71  0.56   Eagle_education_Norah\n",
            "    517.61     332.10  0.64   Eagle_office_Chauncey\n",
            "    567.85     358.74  0.63   Eagle_health_Lucinda\n",
            "    598.43     405.70  0.68   Eagle_education_Shante\n",
            "    653.32     435.71  0.67   Eagle_education_Raul\n",
            "    686.82     470.82  0.69   Eagle_education_Roman\n",
            "    691.83     464.48  0.67   Eagle_education_Sherrill\n",
            "    907.88     682.68  0.75   Eagle_education_Brooke\n",
            "   1542.96    1294.55  0.84   Eagle_education_Peter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm8eEJdHbz9v"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY4snIvJbz9z"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}